{
    "Programming language": {
        "title": "Programming language",
        "extract": "A programming language is a system of notation for writing computer programs.\nProgramming languages are described in terms of their syntax (form) and semantics (meaning), usually defined by a formal language. Languages usually provide features such as a type system, variables, and mechanisms for error handling. An implementation of a programming language is required in order to execute programs, namely an interpreter or a compiler. An interpreter directly executes the source code, while a compiler produces an executable program.\nComputer architecture has strongly influenced the design of programming languages, with the most common type (imperative languages—which implement operations in a specified order) developed to perform well on the popular von Neumann architecture. While early programming languages were closely tied to the hardware, over time they have developed more abstraction to hide implementation details for greater simplicity.\nThousands of programming languages—often classified as imperative, functional, logic, or object-oriented—have been developed for a wide variety of uses. Many aspects of programming language design involve tradeoffs—for example, exception handling simplifies error handling, but at a performance cost. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.\n\n",
        "pageid": 23015
    },
    "ABAP": {
        "title": "ABAP",
        "extract": "ABAP (Advanced Business Application Programming, originally Allgemeiner Berichts-Aufbereitungs-Prozessor, German for \"general report preparation processor\") is a high-level programming language created by the German software company SAP SE. It is currently positioned, alongside Java, as the language for programming the SAP NetWeaver Application Server, which is part of the SAP NetWeaver platform for building business applications.\n\n",
        "pageid": 271832
    },
    "Ada (programming language)": {
        "title": "Ada (programming language)",
        "extract": "Ada is a structured, statically typed, imperative, and object-oriented high-level programming language, inspired by Pascal and other languages. It has built-in language support for design by contract (DbC), extremely strong typing, explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international technical standard, jointly defined by the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). As of May 2023, the standard, ISO/IEC 8652:2023, is called Ada 2022 informally.\nAda was originally designed by a team led by French computer scientist Jean Ichbiah of Honeywell under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages then used by the DoD. Ada was named after Ada Lovelace (1815–1852), who has been credited as the first computer programmer.\n\n",
        "pageid": 1242
    },
    "Address (programming language)": {
        "title": "Address (programming language)",
        "extract": "The Address programming language (Ukrainian: Адресна мова програмування, Russian: Адресный язык программирования) is one of the world's first high-level programming languages. It was created in 1955 by Kateryna Yushchenko. In particular, the Address programming language made possible indirect addressing and addresses of the highest rank –  analogous to pointers.\nUnlike Fortran and ALGOL 60, APL (Address Programming Language) supported indirect addressing and addressing of higher ranks. Indirect addressing is a mechanism that appeared in other programming languages much later (1964 –  in PL/1).\nThe Address language was implemented on all the computers of the first and second generation produced in the Soviet Union. The Address language influenced the architecture of the Kyiv, M-20, Dnipro, Ural, Promin and Minsk computers. The Address programming language was used exclusively for the solution of economical problems, including aviation, space exploration, machine building, and military complex –  in particular, to calculate the trajectories of ballistic missiles in flight –  in the 1950–60s. Implementations of the Address programming language were used for nearly 20 years. A book about APL was published in Ukraine in 1963 and it was translated and published in France in 1974.\nThe Address language affected not only the Soviet Union's and other socialist countries economical development, but information technology and programming worldwide. APL's proposed and implemented ideas and tools can be found in many programming-related fields, such as abstract data types, object-oriented programming, functional programming, logical programming, databases and artificial intelligence.\n\n",
        "pageid": 34146907
    },
    "Agda (programming language)": {
        "title": "Agda (programming language)",
        "extract": "Agda is a dependently typed functional programming language originally developed by Ulf Norell at Chalmers University of Technology with implementation described in his PhD thesis. The original Agda system was developed at Chalmers by Catarina Coquand in 1999. The current version, originally named Agda 2, is a full rewrite, which should be considered a new language that shares a name and tradition.\nAgda is also a proof assistant based on the propositions-as-types paradigm (Curry–Howard correspondence), but unlike Coq, has no separate tactics language, and proofs are written in a functional programming style. The language has ordinary programming constructs such as data types, pattern matching, records, let expressions and modules, and a Haskell-like syntax. The system has Emacs, Atom, and VS Code interfaces but can also be run in batch processing mode from a command-line interface.\nAgda is based on Zhaohui Luo's unified theory of dependent types (UTT), a type theory similar to Martin-Löf type theory.\nAgda is named after the Swedish song \"Hönan Agda\", written by Cornelis Vreeswijk, which is about a hen named Agda. This alludes to the name of the theorem prover Coq, which was named after Thierry Coquand.",
        "pageid": 4426773
    },
    "ALGOL 68": {
        "title": "ALGOL 68",
        "extract": "ALGOL 68 (short for Algorithmic Language 1968) is an imperative programming language member of the ALGOL family that was conceived as a successor to the ALGOL 60 language, designed with the goal of a much wider scope of application and more rigorously defined syntax and semantics.\nThe complexity of the language's definition, which runs to several hundred pages filled with non-standard terminology, made compiler implementation difficult and it was said it had \"no implementations and no users\". This was only partly true; ALGOL 68 did find use in several niche markets, notably in the United Kingdom where it was popular on International Computers Limited (ICL) machines, and in teaching roles. Outside these fields, use was relatively limited.\nNevertheless, the contributions of ALGOL 68 to the field of computer science have been deep, wide-ranging and enduring, although many of these contributions were only publicly identified when they had reappeared in subsequently developed programming languages. Many languages were developed specifically as a response to the perceived complexity of the language, the most notable being Pascal, or were reimplementations for specific roles, like Ada.\nMany languages of the 1970s trace their design specifically to ALGOL 68, selecting some features while abandoning others that were considered too complex or out-of-scope for given roles. Among these is the language C, which was directly influenced by ALGOL 68, especially by its strong typing and structures. Most modern languages trace at least some of their syntax to either C or Pascal, and thus directly or indirectly to ALGOL 68.",
        "pageid": 692880
    },
    "APL (programming language)": {
        "title": "APL (programming language)",
        "extract": "APL (named after the book A Programming Language) is a programming language developed in the 1960s by Kenneth E. Iverson. Its central datatype is the multidimensional array. It uses a large range of special graphic symbols to represent most functions and operators, leading to very concise code. It has been an important influence on the development of concept modeling, spreadsheets, functional programming, and computer math packages. It has also inspired several other programming languages.\n\n",
        "pageid": 1451
    },
    "Apple (programming language)": {
        "title": "Apple (programming language)",
        "extract": "Apple is a PL/I dialect programming language created by General Motors Research Laboratories for their Control Data Corporation STAR-100 supercomputer.\nApple is a subset of full PL/I, but has been extended to integrate with the Associative Programming Language (APL – not to be confused with APL (programming language) ): p.9 : p.92 ",
        "pageid": 77252732
    },
    "AppleScript": {
        "title": "AppleScript",
        "extract": "AppleScript is a scripting language created by Apple Inc. that facilitates automated control of Mac applications. First introduced in System 7, it is currently included in macOS in a package of automation tools. The term AppleScript may refer to the scripting language, to a script written in the language, or to the macOS Open Scripting Architecture that underlies the language.\nAppleScript is primarily a mechanism for driving Apple events – an inter-application communication (IAC) technology that exchanges data between and controls applications. Additionally, AppleScript supports basic calculations and text processing, and is extensible via scripting additions that add functions to the language. \nAppleScript is tightly bound to the Mac environment, similar to how Windows Script Host is bound to the Windows environment. In other words, AppleScript is not a general purpose scripting language like Python. One way that AppleScript is bound to the unique aspects of its environment is that it relies on applications to publish dictionaries of addressable objects and operations.\nAs is typical of a command language, AppleScript is not designed to directly perform intensive processing. For example, a script cannot efficiently perform intensive math operations or complicated text processing. However, AppleScript can be used in combination with other tools and technologies which allows it to leverage more efficient programming contexts.\nThe language has aspects of structured, procedural, object-oriented and natural language programming, but does not strictly conform to any of these paradigms.: xxvi \n\n",
        "pageid": 88392
    },
    "AspectJ": {
        "title": "AspectJ",
        "extract": "AspectJ is an aspect-oriented programming (AOP) extension for the Java programming language, created at PARC. It is available in Eclipse Foundation open-source projects, both stand-alone and integrated into Eclipse. AspectJ has become a widely used de facto standard for AOP by emphasizing simplicity and usability for end users. It uses Java-like syntax, and included IDE integrations for displaying crosscutting structure since its initial public release in 2001.",
        "pageid": 237214
    },
    "B (programming language)": {
        "title": "B (programming language)",
        "extract": "B is a programming language developed at Bell Labs circa 1969 by Ken Thompson and Dennis Ritchie.\nB was derived from BCPL, and its name may possibly be a contraction of BCPL.  Thompson's coworker Dennis Ritchie speculated that the name might be based on Bon, an earlier, but unrelated, programming language that Thompson designed for use on Multics.\nB was designed for recursive, non-numeric, machine-independent applications, such as system and language software. It was a typeless language, with the only data type being the underlying machine's natural memory word format, whatever that might be. Depending on the context, the word was treated either as an integer or a memory address.\nAs machines with ASCII processing became common, notably the DEC PDP-11 that arrived at Bell Labs, support for character data stuffed in memory words became important. The typeless nature of the language was seen as a disadvantage, which led Thompson and Ritchie to develop an expanded version of the language supporting new internal and user-defined types, which became the C programming language.",
        "pageid": 4475
    },
    "BASIC": {
        "title": "BASIC",
        "extract": "BASIC (Beginners' All-purpose Symbolic Instruction Code) is a family of general-purpose, high-level programming languages designed for ease of use. The original version was created by John G. Kemeny and Thomas E. Kurtz at Dartmouth College in 1963. They wanted to enable students in non-scientific fields to use computers. At the time, nearly all computers required writing custom software, which only scientists and mathematicians tended to learn.\nIn addition to the programming language, Kemeny and Kurtz developed the Dartmouth Time-Sharing System (DTSS), which allowed multiple users to edit and run BASIC programs simultaneously on remote terminals. This general model became popular on minicomputer systems like the PDP-11 and Data General Nova in the late 1960s and early 1970s. Hewlett-Packard produced an entire computer line for this method of operation, introducing the HP2000 series in the late 1960s and continuing sales into the 1980s. Many early video games trace their history to one of these versions of BASIC.\nThe emergence of microcomputers in the mid-1970s led to the development of multiple BASIC dialects, including Microsoft BASIC in 1975. Due to the tiny main memory available on these machines, often 4 KB, a variety of Tiny BASIC dialects were also created. BASIC was available for almost any system of the era and became the de facto programming language for home computer systems that emerged in the late 1970s. These PCs almost always had a BASIC interpreter installed by default, often in the machine's firmware or sometimes on a ROM cartridge.\nBASIC declined in popularity in the 1990s, as more powerful microcomputers came to market and programming languages with advanced features (such as Pascal and C) became tenable on such computers. By then, most nontechnical personal computer users relied on pre-written applications rather than writing their own programs. In 1991, Microsoft released Visual Basic, combining an updated version of BASIC with a visual forms builder. This reignited use of the language and \"VB\" remains a major programming language in the form of VB.NET, while a hobbyist scene for BASIC more broadly continues to exist.",
        "pageid": 4015
    },
    "Behavioral Description Language": {
        "title": "Behavioral Description Language",
        "extract": "Behavioral Description Language (BDL) is a programming language based on ANSI C with extensions for hardware description, developed to describe hardware at levels ranging from the algorithm level to the functional level.\nAlthough the term Behavioral Description Language is a generic term and can refer to multiple high-level description languages, NEC Corporation has developed a C-subset called BDL for High-Level Synthesis. This C-subset includes its own data types (called var-class), special constants for hardware design e.g. high impedance, timing descriptors and control statements.\nAs BDL is meant for Hardware synthesis, the complete ANSI-C syntax is not supported. The principal unsupported operations are: (i) Floating point data types (ii) Sizeof operator (iii) unions and (iv) Recursive functions.\nBDL is sometimes also known as Cyber C because it is synthesized using NEC's High-Level Synthesis tool called CyberWorkBench [1].",
        "pageid": 28806128
    },
    "BLISS": {
        "title": "BLISS",
        "extract": "BLISS is a system programming language developed at Carnegie Mellon University (CMU) by W. A. Wulf, D. B. Russell, and A. N. Habermann around 1970. It was perhaps the best known system language until C debuted a few years later. Since then, C became popular and common, and BLISS faded into obscurity. When C was in its infancy, a few projects within Bell Labs debated the merits of BLISS vs. C.\nBLISS is a typeless block-structured programming language based on expressions rather than statements, and includes constructs for exception handling, coroutines, and macros. It does not include a goto statement.\nThe name is variously said to be short for Basic Language for Implementation of System Software or System Software Implementation Language, Backwards. However, in his 2015 oral history for the Babbage Institute's Computer Security History Project, Wulf claimed that the acronym was originally based on the name \"Bill's Language for Implementing System Software.\"\nThe original Carnegie Mellon compiler was notable for its extensive use of optimizations, and formed the basis of the classic book The Design of an Optimizing Compiler.\nDigital Equipment Corporation (DEC) developed and maintained BLISS compilers for the PDP-10, PDP-11, VAX, DEC PRISM, MIPS, DEC Alpha, and Intel IA-32, The language did not become popular among customers and few had the compiler, but DEC used it heavily in-house into the 1980s; most of the utility programs for the OpenVMS operating system were written in BLISS-32. The DEC BLISS compiler has been ported to the IA-64 and x86-64 architectures as part of the ports of OpenVMS to these platforms. The x86-64 BLISS compiler uses LLVM as its backend code generator, replacing the proprietary GEM backend used for Alpha and IA-64.",
        "pageid": 390261
    },
    "Boo (programming language)": {
        "title": "Boo (programming language)",
        "extract": "Boo is an object-oriented, statically typed, general-purpose programming language that seeks to make use of the Common Language Infrastructure's support for Unicode, internationalization, and web applications, while using a Python-inspired syntax and a special focus on language and compiler extensibility. Some features of note include type inference, generators, multimethods, optional duck typing, macros, true closures, currying, and first-class functions.\nBoo was one of the three scripting languages for the Unity game engine (Unity Technologies employed De Oliveira, its designer), until official support was dropped in 2014 due to the small userbase. The Boo Compiler was removed from the engine in 2017. Boo has since been abandoned by De Oliveira, with development being taken over by Mason Wheeler.\nBoo is free software released under the BSD 3-Clause license. It is compatible with the Microsoft .NET and Mono frameworks.",
        "pageid": 1147624
    },
    "Boomerang (programming language)": {
        "title": "Boomerang (programming language)",
        "extract": "Boomerang is a programming language for writing lenses—well-behaved bidirectional transformations —that operate on ad-hoc, textual data formats.\nBoomerang grew out of the Harmony generic data synchronizer, which grew out of the Unison file synchronization project.",
        "pageid": 21008097
    },
    "Bs (programming language)": {
        "title": "Bs (programming language)",
        "extract": "bs is a programming language and a compiler/interpreter for modest-sized programs on UNIX systems. The bs command can be invoked either for interactive programming or with a file containing a program, optionally taking arguments, via a Unix shell, e.g., using a Shebang (Unix) #!/usr/bin/bs.\nAn early man page states, \"[bs] is a remote descendant of Basic [sic] and SNOBOL4, with a little C thrown in.\"",
        "pageid": 43779466
    },
    "Cameleon (programming language)": {
        "title": "Cameleon (programming language)",
        "extract": "Cameleon is a free and open source graphical language for functional programming, released under an MIT License.\nCameleon language is a graphical data flow language following a two-scale paradigm. It allows an easy up-scale, that is, the integration of any library writing in C++ into the data flow language. Cameleon language aims to democratize macro-programming by an intuitive interaction between the human and the computer where building an application based on a data-process and a GUI is a simple task to learn and to do. Cameleon language allows conditional execution and repetition to solve complex macro-problems.\nCameleon is built on an extension of the petri net model for the description of how the Cameleon language executes a composition.",
        "pageid": 42573172
    },
    "C Sharp (programming language)": {
        "title": "C Sharp (programming language)",
        "extract": "C# ( see SHARP) is a general-purpose high-level programming language supporting multiple paradigms. C# encompasses static typing,: 4  strong typing, lexically scoped, imperative, declarative, functional, generic,: 22  object-oriented (class-based), and component-oriented programming disciplines.\nThe principal inventors of the C# programming language were Anders Hejlsberg, Scott Wiltamuth, and Peter Golde from Microsoft. It was first widely distributed in July 2000 and was later approved as an international standard by Ecma (ECMA-334) in 2002 and ISO/IEC (ISO/IEC 23270 and 20619) in 2003. Microsoft introduced C# along with .NET Framework and Microsoft Visual Studio, both of which are technically speaking, closed-source. At the time, Microsoft had no open-source products. Four years later, in 2004, a free and open-source project called Microsoft Mono began, providing a cross-platform compiler and runtime environment for the C# programming language. A decade later, Microsoft released Visual Studio Code (code editor), Roslyn (compiler), and the unified .NET platform (software framework), all of which support C# and are free, open-source, and cross-platform. Mono also joined Microsoft but was not merged into .NET.\nAs of January 2025, the most recent stable version of the language is C# 13.0, which was released in 2024 in .NET 9.0",
        "pageid": 2356196
    },
    "Caml": {
        "title": "Caml",
        "extract": "Caml (originally an acronym for Categorical Abstract Machine Language) is a multi-paradigm, general-purpose, high-level, functional programming language which is a dialect of the ML programming language family. Caml was developed in France at French Institute for Research in Computer Science and Automation (INRIA) and École normale supérieure (Paris) (ENS).\nCaml is statically typed, strictly evaluated, and uses automatic memory management. OCaml, the main descendant of Caml, adds many features to the language, including an object-oriented programming (object) layer.",
        "pageid": 2362118
    },
    "Carbon (programming language)": {
        "title": "Carbon (programming language)",
        "extract": "Carbon is an experimental programming language designed for connectiveness with C++. The project is open-source and was started at Google. Google engineer Chandler Carruth first introduced Carbon at the CppNorth conference in Toronto in July 2022. He stated that Carbon was created to be a C++ successor. The language is expected to have an experimental MVP version 0.1 in 2025 and a production-ready version 1.0 after 2027.\nThe language intends to fix several perceived shortcomings of C++ but otherwise provides a similar feature set.\nThe main goals of the language are readability and \"bi-directional interoperability\" (which allows the user to include C++ code in the Carbon file), as opposed to using a new language like Rust, that, whilst being influenced by C++, is not two-way compatible with C++ programs. Changes to the language will be decided by the Carbon leads.\nCarbon's documents, design, implementation, and related tools are hosted on GitHub under the Apache-2.0 license with LLVM Exceptions.\n\n",
        "pageid": 71357638
    },
    "Cedar (programming language)": {
        "title": "Mesa (programming language)",
        "extract": "Mesa is a programming language developed in the mid 1970s at the Xerox Palo Alto Research Center in Palo Alto, California, United States. The language name was a pun based upon the programming language catchphrases of the time, because Mesa is a \"high level\" programming language.\nMesa is an ALGOL-like language with strong support for modular programming. Every library module has at least two source files: a definitions file specifying the library's interface plus one or more program files specifying the implementation of the procedures in the interface. To use a library, a program or higher-level library must \"import\" the definitions. The Mesa compiler type-checks all uses of imported entities; this combination of separate compilation with type-checking was unusual at the time.\nMesa introduced several other innovations in language design and implementation, notably in the handling of software exceptions, thread synchronization, and incremental compilation.\nMesa was developed on the Xerox Alto, one of the first personal computers with a graphical user interface, however, most of the Alto's system software was written in BCPL. Mesa was the system programming language of the later Xerox Star workstations, and for the GlobalView desktop environment.  Xerox PARC later developed Cedar, which was a superset of Mesa.\nMesa and Cedar had a major influence on the design of other important languages, such as Modula-2 and Java, and was an important vehicle for the development and dissemination of the fundamentals of GUIs, networked environments, and the other advances Xerox contributed to the field of computer science.",
        "pageid": 19962
    },
    "Céu (programming language)": {
        "title": "Céu (programming language)",
        "extract": "Céu is \"Structured Synchronous Reactive Programming\" \nAccording to its web page, Céu supports synchronous concurrency with shared memory and deterministic execution and has a small memory footprint.",
        "pageid": 52350190
    },
    "Charm (programming language)": {
        "title": "Charm (programming language)",
        "extract": "Charm is a computer programming language devised in the early 1990s with similarities to the RTL/2, Pascal and C languages in addition to containing some unique features of its own. The Charm language is defined by a context-free grammar amenable to being processed by recursive descent parser as described in seminal books on compiler design.\nA set of Charm tools including a compiler, assembler and linker was made available for Acorn's RISC OS platform. Charm reworked for RISC OS platforms has subsequently been reviewed in Archive magazine.\nCharm is further described in the e-book Programming in Charm on the Raspberry Pi.\n\n",
        "pageid": 31915190
    },
    "Clascal": {
        "title": "Clascal",
        "extract": "Clascal is an object-oriented programming language (and associated  discontinued compiler) developed in 1983 by the Personal Office Systems (POS) division (later renamed The Lisa Division, then later The 32-Bit Systems Division) of  Apple Computer. Clascal was used to program applications for the Lisa Office System, the operating environment of the Lisa.\nDeveloped as an extension of Lisa Pascal, which in turn harked back to the UCSD Pascal model originally implemented on the Apple II, the language was strongly influenced by the Xerox Palo Alto Research Center (PARC) release of Smalltalk-80, v1 (which had been formerly ported to the Lisa), and by Modula.  According to Larry Tesler, Clascal was developed as a replacement for Apple's version of Smalltalk, which was \"too slow\" and because the experience offered by the Smalltalk syntax was too unfamiliar for most people.\nClascal was the basis for Object Pascal on the Apple Macintosh in 1985. With the demise of the Lisa in 1986, Pascal and Object Pascal continued to be used in the Macintosh Programmer's Workshop for systems and application development for several more years, until it was finally supplanted by the languages C and C++. The MacApp application framework was based on Toolkit originally written in Clascal.\nObject Pascal, in turn, served as the basis for Borland's Delphi.",
        "pageid": 10580097
    },
    "Clipper (programming language)": {
        "title": "Clipper (programming language)",
        "extract": "Clipper is an xBase compiler that implements a variant of the xBase computer programming language. It is used to create \nor extend software programs that originally operated primarily under MS-DOS. Although it is a powerful general-purpose programming\nlanguage, it was primarily used to create database/business programs.\nOne major dBase feature  not implemented in Clipper is the dot-prompt (. prompt) interactive command set, which was an important part of the original dBase implementation.\nClipper, from Nantucket Corp and later Computer Associates, started out as a native code compiler for dBase III databases, and later evolved.",
        "pageid": 246367
    },
    "Clojure": {
        "title": "Clojure",
        "extract": "Clojure (, like closure) is a dynamic and functional dialect of the programming language Lisp on the Java platform.\nLike most other Lisps, Clojure's syntax is built on S-expressions that are first parsed into data structures by a Lisp reader before being compiled. Clojure's reader supports literal syntax for maps, sets, and vectors along with lists, and these are compiled to the mentioned structures directly. Clojure treats code as data and has a Lisp macro system. Clojure is a Lisp-1 and is not intended to be code-compatible with other dialects of Lisp, since it uses its own set of data structures incompatible with other Lisps. \nClojure advocates immutability and immutable data structures and encourages programmers to be explicit about managing identity and its states. This focus on programming with immutable values and explicit progression-of-time constructs is intended to facilitate developing more robust, especially concurrent, programs that are simple and fast. While its type system is entirely dynamic, recent efforts have also sought the implementation of a dependent type system.\nThe language was created by Rich Hickey in the mid-2000s, originally for the Java platform; the language has since been ported to other platforms, such as the Common Language Runtime (.NET). Hickey continues to lead development of the language as its benevolent dictator for life.",
        "pageid": 16561990
    },
    "COMAL": {
        "title": "COMAL",
        "extract": "COMAL (Common Algorithmic Language) is a computer programming language developed in Denmark by Børge R. Christensen and Benedict Løfstedt and originally released in 1975. It was based on the BASIC programming language, adding multi-line statements and well-defined subroutines among other additions.\nCOMAL was originally written for minicomputers, but was small enough to run on early microcomputers as well. It is one of the few structured programming languages that were available for and comfortably usable on 8-bit home computers. \n\"COMAL Kernel Syntax & Semantics\" contains the formal definition of the language. Further extensions are common to many implementations.",
        "pageid": 197700
    },
    "Concordion": {
        "title": "Concordion",
        "extract": "Concordion is a specification by example framework originally developed by David Peterson, and now maintained by a team of contributors, led by Nigel Charman.\nInspired by the Fit Framework, David states the following aims were behind Concordion:\n\nImproved readability of documents\nMore \"opinionated\" (scripting is actively discouraged)\nEasier to use",
        "pageid": 32040842
    },
    "COWSEL": {
        "title": "COWSEL",
        "extract": "COWSEL (COntrolled Working SpacE Language) is a programming language designed between 1964 and 1966 by Robin Popplestone. It was based on an reverse Polish notation (RPN) form of the language Lisp, combined with some ideas from Combined Programming Language (CPL).\nCOWSEL was initially implemented on a Ferranti Pegasus computer at the University of Leeds and on a Stantec Zebra at the Bradford Institute of Technology. Later, Rod Burstall implemented it on an Elliot 4120 at the University of Edinburgh.\nCOWSEL was renamed POP-1 in 1966, during summer, and development continued under that name from then on.",
        "pageid": 94451
    },
    "Crystal (programming language)": {
        "title": "Crystal (programming language)",
        "extract": "Crystal is a high-level general-purpose, object-oriented programming language, designed and developed by Ary Borenszweig, Juan Wajnerman, Brian Cardiff and more than 400 contributors. With syntax inspired by the language Ruby, it is a compiled language with static type-checking, but specifying the types of variables or method arguments is generally unneeded. Types are resolved by an advanced global type inference algorithm. Crystal \nis currently in active development. It is released as free and open-source software under the Apache License version 2.0.",
        "pageid": 48972626
    },
    "Cuneiform (programming language)": {
        "title": "Cuneiform (programming language)",
        "extract": "Cuneiform is an open-source workflow language\nfor large-scale scientific data analysis.\nIt is a statically typed functional programming language promoting parallel computing. It features a versatile foreign function interface allowing users to integrate software from many external programming languages. At the organizational level Cuneiform provides facilities like conditional branching and general recursion making it Turing-complete. In this, Cuneiform is the attempt to close the gap between scientific workflow systems like Taverna, KNIME, or Galaxy and large-scale data analysis programming models like MapReduce or Pig Latin while offering the generality of a functional programming language.\nCuneiform is implemented in distributed Erlang. If run in distributed mode it drives a POSIX-compliant distributed file system like Gluster or Ceph (or a FUSE integration of some other file system, e.g., HDFS). Alternatively, Cuneiform scripts can be executed on top of HTCondor or Hadoop.\nCuneiform is influenced by the work of Peter Kelly who proposes functional programming as a model for scientific workflow execution.\nIn this, Cuneiform is distinct from related workflow languages based on dataflow programming like Swift.",
        "pageid": 51797637
    },
    "D (programming language)": {
        "title": "D (programming language)",
        "extract": "D, also known as dlang, is a multi-paradigm system programming language created by Walter Bright at Digital Mars and released in 2001. Andrei Alexandrescu joined the design and development effort in 2007. Though it originated as a re-engineering of C++, D is now a very different language. As it has developed, it has drawn inspiration from other high-level programming languages. Notably, it has been influenced by Java, Python, Ruby, C#, and Eiffel.\nThe D language reference describes it as follows:\n\nD is a general-purpose systems programming language with a C-like syntax that compiles to native code. It is statically typed and supports both automatic (garbage collected) and manual memory management. D programs are structured as modules that can be compiled separately and linked with external libraries to create native libraries or executables.",
        "pageid": 243881
    },
    "DARSIMCO": {
        "title": "DARSIMCO",
        "extract": "DARSIMCO, short for Dartmouth Simplified Code, was a simple programming language written by John Kemeny in 1956 that expanded simple mathematical operations into IBM 704 assembly language (Share Assembly Language, SAL). It was an attempt to simplify basic mathematical processing, a common theme in the 1950s, but found little use before the arrival of FORTRAN at MIT the next year.",
        "pageid": 57980375
    },
    "Dartmouth Oversimplified Programming Experiment": {
        "title": "Dartmouth Oversimplified Programming Experiment",
        "extract": "DOPE, short for Dartmouth Oversimplified Programming Experiment, was a simple programming language designed by John Kemény in 1962 to offer students a transition from flow-charting to programming the LGP-30. Lessons learned from implementing DOPE were subsequently applied to the invention and development of BASIC.",
        "pageid": 65434331
    },
    "Darwin (programming language)": {
        "title": "Darwin (programming language)",
        "extract": "Darwin is a closed source programming language developed by Gaston Gonnet and colleagues at ETH Zurich.  It is used to develop the OMA orthology inference software, which was also initially developed by Gonnet. The language backend consists of the kernel, responsible for performing simple mathematical calculations, for transporting and storing data and for interpreting the user's commands, and the library, a set of programs which can perform more complicated calculations. The target audience for the language is the biosciences, so the library consisted of routines such as those to compute pairwise alignments, phylogenetic trees, multiple sequence alignments, and to make secondary structure predictions.\n\n",
        "pageid": 12794721
    },
    "DataFlex": {
        "title": "DataFlex",
        "extract": "DataFlex is an object-oriented high-level programming language and a fourth generation visual tool for developing Windows, web and mobile software applications on one framework-based platform. It was introduced and developed by Data Access Corporation beginning in 1982.",
        "pageid": 742526
    },
    "Deductive language": {
        "title": "Deductive language",
        "extract": "A deductive language is a computer programming language in which the program is a collection of predicates ('facts') and rules that connect them. Such a language is used to create knowledge based systems or expert systems which can deduce answers to problem sets by applying the rules to the facts they have been given.\nAn example of a deductive language is Prolog, or its database-query cousin, Datalog.",
        "pageid": 6477222
    },
    "DIBOL": {
        "title": "DIBOL",
        "extract": "DIBOL or Digital's Business Oriented Language is a general-purpose, procedural, imperative programming language that was designed for use in Management Information Systems (MIS) software development. It was developed from 1970 to 1993.\nDIBOL has a syntax similar to FORTRAN and BASIC, along with BCD arithmetic. It shares the COBOL program structure of separate data and procedure divisions. Unlike Fortran's numeric labels (for GOTO), DIBOL's were alphanumeric; the language supported a counterpart to computed goto.",
        "pageid": 598142
    },
    "E (programming language)": {
        "title": "E (programming language)",
        "extract": "E is an object-oriented programming language for secure distributed computing, created by Mark S. Miller, Dan Bornstein, Douglas Crockford, Chip Morningstar and others at Electric Communities in 1997. E is mainly descended from the concurrent language Joule and from Original-E, a set of extensions to Java for secure distributed programming. E combines message-based computation with Java-like syntax. A concurrency model based on event loops and promises ensures that deadlock can never occur.",
        "pageid": 1377046
    },
    "ELAN (programming language)": {
        "title": "ELAN (programming language)",
        "extract": "ELAN is an interpreted educational programming language for learning and teaching systematic programming. (Note: In May 2023 design commenced on a new programming language named 'Elan' also designed for teaching and learning programming in schools, but it has no historical connection to the 'ELAN' language described here.)\nIt was developed in 1974 by C.H.A. Koster and a group at Technische Universität Berlin as an alternative to BASIC in teaching, and approved for use in secondary schools in Germany by the \"Arbeitskreis Schulsprache\". It was in use until the late 1980s in a number of schools in Germany, Belgium, the Netherlands, and Hungary for informatics teaching in secondary education, and used at the Radboud University Nijmegen in the Netherlands for teaching systematic programming to students from various disciplines and in teacher courses.\nThe language design focuses strongly on structured programming, and has a special construction for stepwise refinement, allowing students to focus on top-down design, and bottom-up coding.\nThe microkernel operating system Eumel began as a runtime system (environment) for ELAN.\n\n",
        "pageid": 1179492
    },
    "Elixir (programming language)": {
        "title": "Elixir (programming language)",
        "extract": "Elixir is a functional, concurrent, high-level general-purpose programming language that runs on the BEAM virtual machine, which is also used to implement the Erlang programming language. Elixir builds on top of Erlang and shares the same abstractions for building distributed, fault-tolerant applications. Elixir also provides tooling and an extensible design. The latter is supported by compile-time metaprogramming with macros and polymorphism via protocols.\nThe community organizes yearly events in the United States, Europe, and Japan, as well as minor local events and conferences.\n\n",
        "pageid": 38202780
    },
    "Erlang (programming language)": {
        "title": "Erlang (programming language)",
        "extract": "Erlang ( UR-lang) is a general-purpose, concurrent, functional high-level programming language, and a garbage-collected runtime system. The term Erlang is used interchangeably with Erlang/OTP, or Open Telecom Platform (OTP), which consists of the Erlang runtime system, several ready-to-use components (OTP) mainly written in Erlang, and a set of design principles for Erlang programs.\nThe Erlang runtime system is designed for systems with these traits:\n\nDistributed\nFault-tolerant\nSoft real-time\nHighly available, non-stop applications\nHot swapping, where code can be changed without stopping a system.\nThe Erlang programming language has immutable data, pattern matching, and functional programming. The sequential subset of the Erlang language supports eager evaluation, single assignment, and dynamic typing.\nA normal Erlang application is built out of hundreds of small Erlang processes.\nIt was originally proprietary software within Ericsson, developed by Joe Armstrong, Robert Virding, and Mike Williams in 1986, but was released as free and open-source software in 1998. Erlang/OTP is supported and maintained by the Open Telecom Platform (OTP) product unit at Ericsson.\n\n",
        "pageid": 9646
    },
    "EXAPT": {
        "title": "EXAPT",
        "extract": "EXAPT (a portmanteau of \"Extended Subset of APT\") is a production-oriented programming language that allows users to generate NC programs with control information for machining tools and facilitates decision-making for production-related issues that may arise during various machining processes.\nEXAPT was first developed to address industrial requirements. Through the years, the company created additional software for the manufacturing industry. Today, EXAPT offers a suite of SAAS products and services for the manufacturing industry.\nThe trade name, EXAPT, is most commonly associated with the CAD/CAM-System, production data, and tool management software of the German company EXAPT Systemtechnik GmbH based in Aachen, DE.\n\n",
        "pageid": 51684378
    },
    "F (programming language)": {
        "title": "F (programming language)",
        "extract": "F is a modular, compiled, numeric programming language, designed for scientific programming and scientific computation. F was developed as a modern Fortran, thus making it a subset of Fortran 95. It combines both numerical and data abstraction features from these languages. F is also backwards compatible with Fortran 77, allowing calls to Fortran 77 programs. F was implemented on top of compilers from NAG, Fujitsu, Salford Software and Absoft. It was later included in the g95 compiler.\n\n",
        "pageid": 1283488
    },
    "Factor (programming language)": {
        "title": "Factor (programming language)",
        "extract": "Factor is a stack-oriented programming language created by Slava Pestov. Factor is dynamically typed and has automatic memory management, as well as powerful metaprogramming features. The language has a single implementation featuring a self-hosted optimizing compiler and an interactive development environment. The Factor distribution includes a large standard library.\n\n",
        "pageid": 891398
    },
    "Flow chart language": {
        "title": "Flow chart language",
        "extract": "Flow chart language (FCL) is a simple imperative programming language designed for the purposes of explaining fundamental concepts of program analysis and specialization, in particular, partial evaluation. The language was first presented in 1989 by Carsten K. Gomard and Neil D. Jones. It later resurfaced in their book with Peter Sestoft in 1993, and in John Hatcliff's lecture notes in 1998. The below describes FCL as it appeared in John Hatcliff's lecture notes.\nFCL is an imperative programming language close to the way a Von Neumann computer executes a program. A program is executed sequentially by following a sequence of commands, while maintaining an implicit state, i.e. the global memory. FCL has no concept of procedures, but does provide conditional and unconditional jumps. FCL lives up to its name as the abstract call-graph of an FCL program is a straightforward flow chart.\nAn FCL program takes as input a finite series of named values as parameters, and produces a value as a result.",
        "pageid": 39690826
    },
    "Flowcode": {
        "title": "Flowcode",
        "extract": "Flowcode is a Microsoft Windows-based development environment commercially produced by Matrix TSL for programming embedded devices based on PIC, AVR (including Arduino), ESP32, Raspberry Pi and RP2040 and ARM technologies using graphical programming styles (such as flowcharts) and imperative programming styles (through C, State Machines and Pseudocode). It is currently in its tenth revision.\nFlowcode is dedicated to simplifying complex functionality such as Bluetooth, Mobile Phones Communications, USB communications etc. by using pre-developed dedicated open source component libraries of functions. This is achieved by dragging virtual representations of hardware onto a visual panel, providing access to associated libraries. Flowcode is therefore ideal for speeding up software development times and allowing those with little programming experience to get started and help with projects. This makes it appropriate for the formal teaching of principles of programming microcontrollers.\nFlowcode allows the user to develop and view their program using four different visual modes. These are the Flowchart view, the Blocks view (a graphical programming paradigm inspired by Blockly), the C code view and the Pseudocode view. There is also a fifth state machine way of entering code.\nFlowcode also has a mode named App Developer which is capable of creating Windows based applications via a runtime executable. This allows the software to also create applications for testing or interacting with the embedded system.\nFlowcode also has compatibility with Solidworks.",
        "pageid": 38557302
    },
    "Forth (programming language)": {
        "title": "Forth (programming language)",
        "extract": "Forth is a stack-oriented programming language and interactive integrated development environment designed by Charles H. \"Chuck\" Moore and first used by other programmers in 1970. Although not an acronym, the language's name in its early years was often spelled in all capital letters as FORTH. The FORTH-79 and FORTH-83 implementations, which were not written by Moore, became de facto standards, and an official technical standard of the language was published in 1994 as ANS Forth. A wide range of Forth derivatives existed before and after ANS Forth. The free and open-source software Gforth implementation is actively maintained, as are several commercially supported systems.\nForth typically combines a compiler with an integrated command shell, where the user interacts via subroutines called words. Words can be defined, tested, redefined, and debugged without recompiling or restarting the whole program. All syntactic elements, including variables, operators, and control flow, are defined as words. A stack is used to pass parameters between words, leading to a Reverse Polish notation style.\nFor much of Forth's existence, the standard technique was to compile to threaded code, which can be interpreted faster than bytecode. One of the early benefits of Forth was size: an entire development environment—including compiler, editor, and user programs—could fit in memory on an 8-bit or similarly limited system. No longer constrained by space, there are modern implementations that generate optimized machine code like other language compilers. The relative simplicity of creating a basic Forth system has led to many personal and proprietary variants, such as the custom Forth used to implement the bestselling 1986 video game Starflight from Electronic Arts.\nForth is used in the Open Firmware boot loader, in spaceflight applications such as the Philae spacecraft, and in other embedded systems which involve interaction with hardware. \nMoore developed a series of microprocessors for executing compiled Forth-like code directly and experimented with smaller languages based on Forth concepts, including cmForth and colorForth. Most of these languages were designed to support Moore's own projects, such as chip design.",
        "pageid": 11012
    },
    "FreeBASIC": {
        "title": "FreeBASIC",
        "extract": "FreeBASIC is a free and open source multiplatform compiler and programming language based on BASIC licensed under the GNU GPL  for Microsoft Windows, protected-mode MS-DOS (DOS extender), Linux, FreeBSD and Xbox.  The Xbox version is no longer maintained.\nAccording to its official website, FreeBASIC provides syntax compatibility with programs originally written in Microsoft QuickBASIC (QB).  Unlike QuickBASIC, however, FreeBASIC is a command line only compiler, unless users manually install an external integrated development environment (IDE) of their choice. \n\n",
        "pageid": 1443566
    },
    "Futhark (programming language)": {
        "title": "Futhark (programming language)",
        "extract": "Futhark is a multi-paradigm, high-level, functional, data parallel, array programming language. It is a dialect of the language ML, originally developed at UCPH Department of Computer Science (DIKU) as part of the HIPERFIT project. It focuses on enabling data parallel programs written in a functional style to be executed with high performance on massively parallel hardware, especially graphics processing units (GPUs). Futhark is strongly inspired by NESL, and its implementation uses a variant of the flattening transformation, but imposes constraints on how parallelism can be expressed in order to enable more aggressive compiler optimisations. In particular, irregular nested data parallelism is not supported. It is free and open-source software released under an ISC license.",
        "pageid": 62334821
    },
    "FX-87": {
        "title": "FX-87",
        "extract": "FX-87 is a polymorphic typed functional language based on a system for static program analysis in which every expression has two static properties: a type and an effect. In a study done by MIT, FX-87 yields similar performance results as functional languages on programs that do not contain side effects (Fibonacci, Factorial). FX-87 did yield a great performance increase when matching DNA sequences.\nKFX is the kernel language of FX-87. It was described in 'Polymorphic Effect Systems', J.M. Lucassen et al., Proceedings of the 15th Annual ACM Conference POPL, ACM 1988, pp. 47–57.\n\n",
        "pageid": 17139
    },
    "General-purpose programming language": {
        "title": "General-purpose programming language",
        "extract": "In computer software, a general-purpose programming language (GPL) is a programming language for building software in a wide variety of application domains. Conversely, a domain-specific programming language (DSL) is used within a specific area. For example, Python is a GPL, while SQL is a DSL for querying relational databases.\n\n",
        "pageid": 891926
    },
    "GEORGE (programming language)": {
        "title": "GEORGE (programming language)",
        "extract": "GEORGE (General Order Generator) is a programming language invented by Charles Leonard Hamblin in 1957. It was designed around a push-down pop-up stack for arithmetic operations, and employed reverse Polish notation. The language included loops, subroutines, conditionals, vectors, and matrices.\n\n",
        "pageid": 36133392
    },
    "Gleam (programming language)": {
        "title": "Gleam (programming language)",
        "extract": "Gleam is a general-purpose, concurrent, functional high-level programming language that compiles to Erlang or JavaScript source code.\nGleam is a statically-typed language, which is different from the most popular languages that run on Erlang’s virtual machine BEAM, Erlang and Elixir. Gleam has its own type-safe implementation of OTP, Erlang's actor framework. Packages are provided using the Hex package manager, and an index for finding packages written for Gleam is available.\n\n",
        "pageid": 76326711
    },
    "Go (programming language)": {
        "title": "Go (programming language)",
        "extract": "Go is a high-level general purpose programming language that is statically typed and compiled. It is known for the simplicity of its syntax and the efficiency of development that it enables by the inclusion of a large standard library supplying many needs for common projects. It was designed at Google in 2009 by Robert Griesemer, Rob Pike, and Ken Thompson. It is syntactically similar to C, but also has memory safety, garbage collection, structural typing, and CSP-style concurrency. It is often referred to as Golang to avoid ambiguity and because of its former domain name, golang.org, but its proper name is Go.\nThere are two major implementations:\n\nThe original, self-hosting compiler toolchain, initially developed inside Google;\nA frontend written in C++, called gofrontend, originally a GCC frontend, providing gccgo, a GCC-based Go compiler; later extended to also support LLVM, providing an LLVM-based Go compiler called gollvm.\nA third-party source-to-source compiler, GopherJS, transpiles Go to JavaScript for front-end web development.\n\n",
        "pageid": 25039021
    },
    "Golo (programming language)": {
        "title": "Golo (programming language)",
        "extract": "Golo is computer software, a programming language for the Java virtual machine (JVM). It is simple, with dynamic, weak typing. It was created in 2012 as part of the research activities of the DynaMid group of the Centre of Innovation in Telecommunications and Integration of service (CITI) Laboratory at Institut national des sciences appliquées de Lyon (INSA). It is distributed as free and open-source software under the Eclipse Public License 2.0.",
        "pageid": 47051765
    },
    "GOLOG": {
        "title": "GOLOG",
        "extract": "GOLOG is a high-level logic programming language for the specification and execution of complex actions in dynamical domains. It is based on the situation calculus. It is a first-order logical language for reasoning about action and change. GOLOG was developed at the University of Toronto.\n\n",
        "pageid": 62806829
    },
    "Gosu (programming language)": {
        "title": "Gosu (programming language)",
        "extract": "Gosu is a statically typed general-purpose programming language that runs on the Java Virtual Machine. Its influences include Java, C#, and ECMAScript.  Development of Gosu began in 2002 internally for Guidewire Software, and the language saw its first community release in 2010 under the Apache 2 license.\nGosu can serve as a scripting language, having free-form Program types (.gsp files) for scripting as well as statically verified Template files (.gst files).  Gosu can optionally execute these and all other types directly from source without precompilation, which also distinguishes it from other static languages.",
        "pageid": 29539307
    },
    "Haggis (programming language)": {
        "title": "Haggis (programming language)",
        "extract": "Haggis is a high-level reference programming language used primarily to examine computing science for Scottish pupils taking SQA courses on the subject. Haggis is used as a tool to bridge the gap between pseudocode and typical computer programming.\nHaggis is not based on any one language but a mixture that is intended to allow a pupil familiar with any of the many languages used in classrooms to easily understand the syntactic construct being used in an example. It has multiple programming paradigms of functional, imperative and object-oriented to suit this purpose.\nThere are three separate language definitions, one for each level at which computing is assessed by the SQA; these are proper subsets of each other, so for example any program contained by the National 5 level language is also well-defined at Higher and Advanced Higher levels. Higher includes the definition of procedures and functions and the use of record types and files, while Advanced Higher includes object-orientation.\nOnline Haggis interpreters have been developed to provide a way for examiners and teachers to check their programs are correctly defined and behave as expected.",
        "pageid": 52222071
    },
    "Haxe": {
        "title": "Haxe",
        "extract": "Haxe is a high-level cross-platform programming language and compiler that can produce applications and source code for many different computing platforms from one code-base. It is free and open-source software, released under an MIT License. The compiler is written in OCaml. It can be run in server-mode to provide code completion for integrated development environments (IDEs).\nHaxe includes a set of features and a standard library supported across all platforms, including numeric data types, strings, arrays, maps, binary, reflective programming, maths, Hypertext Transfer Protocol (HTTP), file system and common file formats. Haxe also includes platform-specific application programming interfaces (APIs) for each compiler target. Kha, OpenFL, and Heaps.io are popular Haxe frameworks that enable creating multi-platform content from one codebase.\nHaxe originated with the idea of supporting client-side and server-side programming in one language, and simplifying the communication logic between them. Code written in Haxe can be compiled into JavaScript, C++, Java, JVM, PHP, C#, Python, Lua and Node.js. Haxe can also directly compile SWF, HashLink, and NekoVM bytecode and also runs in interpreted mode.\nHaxe supports externs (definition files) that can contain data type information of extant libraries to describe target-specific interaction in a type-safe manner, like C++ header files can describe the structure of existing object files. This enables to use the values defined in the files as if they were statically typed Haxe entities. Beside externs, other solutions exist to access each platform's native capabilities.\nMany popular IDEs and source code editors have support available for Haxe development. No particular development environment or tool set is officially recommended by the Haxe Foundation, although VS Code, IntelliJ IDEA and HaxeDevelop have the most support for Haxe development. The core functionalities of syntax highlighting, code completion, refactoring, debugging, etc. are available to various degrees.",
        "pageid": 5404706
    },
    "Hermes (programming language)": {
        "title": "Hermes (programming language)",
        "extract": "Hermes\n\nis a language for distributed programming\nthat was developed at IBM's Thomas J. Watson Research Center from 1986 through 1992,\nwith an open-source compiler and run-time system.\nHermes' primary features included:\n\nLanguage support of processes and interprocess communication.\nCompile-time verification that operations use initialized data.\nRepresentation-independent data aggregates called tables.\nLack of pointers.\nIt used typestate analysis to check variables transitions errors, to rule out some semantically non meaningful transitions from one state to another (i.e. starting from a value, some sequences of operations on a variable are nonsensical), of which reading an uninitialized variable is a special case. In this role of compile-time checking of data initialization is similar to definite assignment analysis performed by Java, Cyclone and C#.\nHermes and its predecessor, NIL (Network Implementation Language), were the earliest programming languages supporting this form of initialization checking.\nTypestate was actually used more extensively, to generate compiler-inserted \"delete\" operations.",
        "pageid": 24136948
    },
    "Io (programming language)": {
        "title": "Io (programming language)",
        "extract": "Io is a pure object-oriented programming language inspired by Smalltalk, Self, Lua, Lisp, Act1, and NewtonScript. Io has a prototype-based object model similar to those in Self and NewtonScript, eliminating the distinction between instance and class. Like Smalltalk, everything is an object and it uses dynamic typing. Like Lisp, programs are just data trees. Io uses actors for concurrency.\nRemarkable features of Io are its minimal size and openness to using external code resources. Io is executed by a small, portable virtual machine.\n\n",
        "pageid": 323340
    },
    "Janus (time-reversible computing programming language)": {
        "title": "Janus (time-reversible computing programming language)",
        "extract": "Janus is a time-reversible programming language written at Caltech in 1982. The operational semantics of the language were formally specified, together with a program inverter and an invertible self-interpreter, in 2007 by Tetsuo Yokoyama and Robert Glück. A Janus inverter and interpreter is made freely available by the TOPPS research group at DIKU. Another Janus interpreter was implemented in Prolog in 2009. An optimizing compiler has been developed in the RC3 research group. The below summarises the language presented in the 2007 paper.\nJanus is a structured imperative programming language that operates on a global store without heap allocation and does not support dynamic data structures. As a reversible programming language, Janus performs deterministic computations in both forward and backward directions. An extension of Janus features procedure parameters and local variable declarations (local-delocal). Additionally, other variants of Janus support dynamic data structures such as lists.\n\n",
        "pageid": 50304039
    },
    "Java (programming language)": {
        "title": "Java (programming language)",
        "extract": "Java is a high-level, general-purpose, memory-safe, object-oriented programming language. It is intended to let programmers write once, run anywhere (WORA), meaning that compiled Java code can run on all platforms that support Java without the need to recompile. Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages.\nJava gained popularity shortly after its release, and has been a popular programming language since then. Java was the third most popular programming language in 2022 according to GitHub. Although still widely popular, there has been a gradual decline in use of Java in recent years with other languages using JVM gaining popularity.\nJava was designed by James Gosling at Sun Microsystems. It was released in May 1995 as a core component of Sun's Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GPL-2.0-only license. Oracle, which bought Sun in 2010, offers its own HotSpot Java Virtual Machine. However, the official reference implementation is the OpenJDK JVM, which is open-source software used by most developers and is the default JVM for almost all Linux distributions.\nJava 23 is the version current as of September 2024, and Java 24 has a Final Release Candidate, set for release on 18 March 2025. Java 20 and 22 are no longer maintained. Java 8, 11, 17, and 21 are long-term support versions still under maintenance.",
        "pageid": 15881
    },
    "Java technology": {
        "title": "Java (programming language)",
        "extract": "Java is a high-level, general-purpose, memory-safe, object-oriented programming language. It is intended to let programmers write once, run anywhere (WORA), meaning that compiled Java code can run on all platforms that support Java without the need to recompile. Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages.\nJava gained popularity shortly after its release, and has been a popular programming language since then. Java was the third most popular programming language in 2022 according to GitHub. Although still widely popular, there has been a gradual decline in use of Java in recent years with other languages using JVM gaining popularity.\nJava was designed by James Gosling at Sun Microsystems. It was released in May 1995 as a core component of Sun's Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GPL-2.0-only license. Oracle, which bought Sun in 2010, offers its own HotSpot Java Virtual Machine. However, the official reference implementation is the OpenJDK JVM, which is open-source software used by most developers and is the default JVM for almost all Linux distributions.\nJava 23 is the version current as of September 2024, and Java 24 has a Final Release Candidate, set for release on 18 March 2025. Java 20 and 22 are no longer maintained. Java 8, 11, 17, and 21 are long-term support versions still under maintenance.",
        "pageid": 15881
    },
    "JavaScript": {
        "title": "JavaScript",
        "extract": "JavaScript ( ), often abbreviated as JS, is a programming language and core technology of the World Wide Web, alongside HTML and CSS. Ninety-nine percent of websites use JavaScript on the client side for webpage behavior.\nWeb browsers have a dedicated JavaScript engine that executes the client code. These engines are also utilized in some servers and a variety of apps. The most popular runtime system for non-browser usage is Node.js.\nJavaScript is a high-level, often just-in-time compiled language that conforms to the ECMAScript standard. It has dynamic typing, prototype-based object-orientation, and first-class functions. It is multi-paradigm, supporting event-driven, functional, and imperative programming styles. It has application programming interfaces (APIs) for working with text, dates, regular expressions, standard data structures, and the Document Object Model (DOM).\nThe ECMAScript standard does not include any input/output (I/O), such as networking, storage, or graphics facilities. In practice, the web browser or other runtime system provides JavaScript APIs for I/O.\nAlthough Java and JavaScript are similar in name and syntax, the two languages are distinct and differ greatly in design.",
        "pageid": 9845
    },
    "Jolie (programming language)": {
        "title": "Jolie (programming language)",
        "extract": "Jolie (Java Orchestration Language Interpreter Engine) is an open-source programming language for developing distributed applications based on microservices. In the programming paradigm proposed with Jolie, each program is a service that can communicate with other programs by sending and receiving messages over a network. Jolie supports an abstraction layer that allows services to communicate using different mediums, ranging from TCP/IP sockets to local in-memory communications between processes.\nJolie is currently supported by an interpreter implemented in the Java language, which can be run in multiple operating systems including Linux-based operating systems, OS X, and Windows. The language comes with formal semantics, meaning that the execution of Jolie programs is mathematically defined. For this reason, Jolie is used in research for the investigation of language-based techniques for the development of distributed systems, and it is also used for teaching at some universities.\nThe Jolie open source project was started by Fabrizio Montesi in 2006, as part of his studies at the University of Bologna. The project initially began as an implementation of the SOCK process calculus, a formal model proposed by Claudio Guidi et al. at the University of Bologna inspired by the CCS process calculus and the WS-BPEL programming language. Jolie extends SOCK with support for, e.g., tree-like data structures (inspired by XML, but with a syntax resembling that of C and Java), message types, typed session programming, integration with Java and JavaScript, code mobility, application containment, and web programming. A complete list of the project contributors is available at.\nThe project is currently maintained by Fabrizio Montesi and its evolution is driven by Fabrizio Montesi and Claudio Guidi.\nSince it supports the orchestration of web services, Jolie is an alternative to XML-based orchestration languages such as WS-BPEL as it offers a concise (C-like) syntax for accessing XML-like data structures.\n\n",
        "pageid": 39210326
    },
    "Joy (programming language)": {
        "title": "Joy (programming language)",
        "extract": "The Joy programming language in computer science is a purely functional programming language that was produced by Manfred von Thun of La Trobe University in Melbourne, Australia. Joy is based on composition of functions rather than lambda calculus. It was inspired by the function-level programming style of John Backus's FP.\nIt has turned out to have many similarities to Forth, due not to design but to an independent evolution and convergence.\n\n",
        "pageid": 696166
    },
    "Jq (programming language)": {
        "title": "Jq (programming language)",
        "extract": "jq is a very high-level lexically scoped functional programming language in which every JSON value is a constant. jq supports backtracking and managing indefinitely long streams of JSON data. It is related to the Icon and Haskell programming languages. The language supports a namespace-based module system and has some support for closures. In particular, functions and functional expressions can be used as parameters of other functions.\nThe original implementation of jq was in Haskell before being immediately ported to C.\n\n",
        "pageid": 72526819
    },
    "JS++": {
        "title": "JS++",
        "extract": "JS++ is a BSD-licensed programming language for web development that extends JavaScript with a sound type system. It includes imperative, object-oriented, functional, and generic programming features.\n\n",
        "pageid": 59825919
    },
    "Julia (programming language)": {
        "title": "Julia (programming language)",
        "extract": "Julia is a high-level, general-purpose dynamic programming language, designed to be fast and productive, for e.g. data science, artificial intelligence, machine learning, modeling and simulation, most commonly used for numerical analysis and computational science.\nDistinctive aspects of Julia's design include a type system with parametric polymorphism and the use of multiple dispatch as a core programming paradigm, a default just-in-time (JIT) compiler (with support for ahead-of-time compilation) and an efficient garbage collection implementation. Notably Julia does not support classes with encapsulated methods and instead it relies on structs with generic methods/functions not tied to them.\nBy default, Julia is run similarly to scripting languages, using its runtime, and allows for interactions, but Julia programs/source code can also optionally be sent to users in one ready-to-install/run file, which can be made quickly, not needing anything preinstalled. Julia programs can also be (separately) compiled to binary executables, even allowing no-source-code distribution, and the executables can get much smaller with Julia 1.12. Such compilation is not needed for speed, though it can decrease constant-factor startup cost, since Julia is also compiled when running interactively, but it can help with hiding source code. Features of the language can be separately compiled, so Julia can be used, for example, with its runtime or without it (which allows for smaller executables and libraries but is limited in capabilities).\nJulia programs can reuse libraries from other languages by calling them, e.g. calling C or Rust libraries, and Julia (libraries) can also be called from other languages, e.g. Python and R, and several Julia packages have been made easily available from those languages, in the form of Python and R libraries for corresponding Julia packages. Calling in either direction has been implemented for many languages, not just those and C++.\nJulia's Visual Studio Code extension provides a fully-featured integrated development environment with \"built-in dynamic autocompletion, inline results, plot pane, integrated REPL, variable view, code navigation, and many other advanced language features\" e.g. debugging is possible, linting, and profiling.",
        "pageid": 38455554
    },
    "K (programming language)": {
        "title": "K (programming language)",
        "extract": "K is a proprietary array processing programming language developed by Arthur Whitney and commercialized by Kx Systems. The language serves as the foundation for kdb+, an in-memory, column-based database, and other related financial products. The language, originally developed in 1993, is a variant of APL and contains elements of Scheme. Advocates of the language emphasize its speed, facility in handling arrays, and expressive syntax.\n\n",
        "pageid": 890956
    },
    "Kinetic Rule Language": {
        "title": "Kinetic Rule Language",
        "extract": "Kinetic Rule Language (KRL) is a rule-based programming language for creating applications on the Live Web. KRL programs, or rulesets, comprise a number of rules that respond to particular events.  KRL has been promoted as language for building personal clouds.\nKRL is part of an open-source project called KRE, for Kinetic Rules Engine, developed by Kynetx, Inc.",
        "pageid": 7303776
    },
    "Kojo (learning environment)": {
        "title": "Kojo (learning environment)",
        "extract": "Kojo is a programming language and integrated development environment (IDE) for computer programming and learning. It has many different features that enable playing, exploring, creating, and learning in the areas of computer programming, mental skills, (interactive) math, graphics, art, music, science, animation, games, and electronics. Kojo draws ideas from the programming languages Logo and Processing.\nKojo is open-source software. It was created, and is actively developed, by Lalit Pant, a computer programmer and teacher living in Dehradun, India. Kojo provides domain-specific languages (DSLs) for its different areas of learning, and as such can be considered an educational programming language.\nKojo is written in, and its approach is based on, the programming language Scala, where users begin with a simple subset of the language and progress in steps. Its graphical user interface is based on Java Swing; a former version was based on the Java NetBeans platform.\nLalit chose Scala as the underlying language for Kojo because of its low barrier to entry and potential power.\nKojo has been used in schools and classes around the world. Some of these include:\n\nThe State of Goa, within its ICT/coding curriculum.\nHimjyoti School, Dehradun, India.\nMondrian House School, Dehradun, India.\nRishi Valley School, Madanapalle, India.\nCardinal Forest Elementary School, Springfield, Virginia, USA.\nDiablo Valley College, Pleasant Hill, California, USA.\nOur Lady's Catholic High School, Preston, England.\nA Swedish 4th grade class consisting of 10-year-old children. Kojo has been featured by Dagens Nyheter (DN) and Computer Sweden as a result of the work done by this class.\nEvents like Silicon Valley Code Camp, CoderDojo, Hack The Future, and Meetups.\nThe development of Kojo is partly sponsored by Lightbend, formerly TypeSafe, and Lund University, Computer Science Department, where Kojo is used to introduce children and teachers to computer programming. Professor Björn Regnell of Lund University has an informative presentation on the subject. Professor Regnell writes, in translation: \"Kojo is the best tool, with a low barrier of entry, I have seen for making real text based programming available for children, that is also usable all the way up to university level\".\nKojo provides rich support for programming and learning in the Turkish language as of the latest release in 2021 and beyond.",
        "pageid": 30843423
    },
    "KOMPILER": {
        "title": "KOMPILER",
        "extract": "In computing, the KOMPILER was one of the first language compilation and runtime systems for International Business Machines' IBM 701, the fastest commercial U.S. computer available in 1955.\nInformation on KOMPILER is listed on page 16 of Volume 2, Number 5 (May 1959) of the Communications of the ACM. Known versions are KOMPILER 2 for IBM 701 and KOMPILER 3 for the IBM 704. KOMPILER was eventually replaced by a Fortran compiler on the IBM 704.",
        "pageid": 17212
    },
    "Kotlin (programming language)": {
        "title": "Kotlin (programming language)",
        "extract": "Kotlin () is a cross-platform, statically typed, general-purpose high-level programming language with type inference. Kotlin is designed to interoperate fully with Java, and the JVM version of Kotlin's standard library depends on the Java Class Library,\nbut type inference allows its syntax to be more concise. Kotlin mainly targets the JVM, but also compiles to JavaScript (e.g., for frontend web applications using React) or native code via LLVM (e.g., for native iOS apps sharing business logic with Android apps). Language development costs are borne by JetBrains, while the Kotlin Foundation protects the Kotlin trademark.\nOn 7 May 2019, Google announced that the Kotlin programming language had become its preferred language for Android app developers. Since the release of Android Studio 3.0 in October 2017, Kotlin has been included as an alternative to the standard Java compiler. The Android Kotlin compiler emits Java 8 bytecode by default (which runs in any later JVM), but allows targeting Java 9 up to 20, for optimizing, or allows for more features; has bidirectional record class interoperability support for JVM, introduced in Java 16, considered stable as of Kotlin 1.5.\nKotlin has support for the web with Kotlin/JS,  through an intermediate representation-based backend which has been declared stable since version 1.8, released December 2022. Kotlin/Native (for e.g. Apple silicon support) has been declared stable since version 1.9.20, released November 2023.\n\n",
        "pageid": 41819039
    },
    "Language interoperability": {
        "title": "Language interoperability",
        "extract": "Language interoperability is the capability of two different programming languages to natively interact as part of the same system and operate on the same kind of data structures. \nThere are many ways programming languages are interoperable with one another. HTML, CSS, and JavaScript are interoperable as they are used in tandem in webpages. Some object oriented languages are interoperable thanks to their shared hosting virtual machine (e.g. .NET CLI compliant languages in the Common Language Runtime and JVM compliant languages in the Java Virtual Machine).",
        "pageid": 36509863
    },
    "LFE (programming language)": {
        "title": "LFE (programming language)",
        "extract": "Lisp Flavored Erlang (LFE) is a functional, concurrent, garbage collected, general-purpose programming language and Lisp dialect built on Core Erlang and the Erlang virtual machine (BEAM). LFE builds on Erlang to provide a Lisp syntax for writing distributed, fault-tolerant, soft real-time, non-stop applications. LFE also extends Erlang to support metaprogramming with Lisp macros and an improved developer experience with a feature-rich read–eval–print loop (REPL). LFE is actively supported on all recent releases of Erlang; the oldest version of Erlang supported is R14.",
        "pageid": 41671035
    },
    "Lightweight programming language": {
        "title": "Lightweight programming language",
        "extract": "Lightweight programming languages are designed to have small memory footprint, are easy to implement (important when porting a language to different computer systems), and/or have minimalist syntax and features.\nThese programming languages have simple syntax and semantics, so one can learn them quickly and easily. Some lightweight languages (for example Lisp, Forth, and Tcl) are so simple to implement that they have many implementations (dialects).",
        "pageid": 10001714
    },
    "Linda (coordination language)": {
        "title": "Linda (coordination language)",
        "extract": "In computer science, Linda is a coordination model that aids communication in parallel computing environments. Developed by David Gelernter, it is meant to be used alongside a full-fledged computation language like Fortran or C where Linda's role is to \"create computational activities and to support communication among them\".",
        "pageid": 957598
    },
    "Lisp (programming language)": {
        "title": "Lisp (programming language)",
        "extract": "Lisp (historically LISP, an abbreviation of \"list processing\") is a family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.\nOriginally specified in the late 1950s, it is the second-oldest high-level programming language still in common use, after Fortran. Lisp has changed since its early days, and many dialects have existed over its history. Today, the best-known general-purpose Lisp dialects are Common Lisp, Scheme, Racket, and Clojure.\nLisp was originally created as a practical mathematical notation for computer programs, influenced by (though not originally derived from) the notation of Alonzo Church's lambda calculus. It quickly became a favored programming language for artificial intelligence (AI) research. As one of the earliest programming languages, Lisp pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, the self-hosting compiler, and the read–eval–print loop.\nThe name LISP derives from \"LISt Processor\". Linked lists are one of Lisp's major data structures, and Lisp source code is made of lists. Thus, Lisp programs can manipulate source code as a data structure, giving rise to the macro systems that allow programmers to create new syntax or new domain-specific languages embedded in Lisp.\nThe interchangeability of code and data gives Lisp its instantly recognizable syntax. All program code is written as s-expressions, or parenthesized lists. A function call or syntactic form is written as a list with the function or operator's name first, and the arguments following; for instance, a function f that takes three arguments would be called as (f arg1 arg2 arg3).",
        "pageid": 18016
    },
    "Little b (programming language)": {
        "title": "Little b (programming language)",
        "extract": "Little b is a domain-specific programming language, more specifically, a modeling language, designed to build modular mathematical models of biological systems.  It was designed and authored by Aneil Mallavarapu.  Little b is being developed in the Virtual Cell Program at Harvard Medical School, headed by mathematician Jeremy Gunawardena.\nThis language is based on Lisp and is meant to allow modular programming to model biological systems.  It will allow more flexibility to facilitate rapid change that is required to accurately capture complex biological systems.\nThe language draws on techniques from artificial intelligence and symbolic mathematics, and provides  syntactic conveniences derived from object-oriented languages.  The language was originally denoted with a lowercase b (distinguishing it from B, the predecessor to the widely used C programming language), but the name was eventually changed to \"little b\" to avoid confusion and to pay homage to Smalltalk.",
        "pageid": 4740151
    },
    "LiveCode": {
        "title": "LiveCode",
        "extract": "LiveCode (formerly Revolution and MetaCard) is a cross-platform rapid application development runtime system inspired by HyperCard. It features the LiveCode Script (formerly MetaTalk) programming language which belongs to the family of xTalk scripting languages like HyperCard's HyperTalk.\nThe environment was introduced in 2001.  The \"Revolution\" development system was based on the MetaCard engine technology which Runtime Revolution later acquired from MetaCard Corporation in 2003. The platform won the Macworld Annual Editor's Choice Award for \"Best Development Software\" in 2004.  \"Revolution\" was renamed \"LiveCode\" in the fall of 2010.  \"LiveCode\" is developed and sold by Runtime Revolution Ltd., based in Edinburgh, Scotland. In March 2015, the company was renamed \"LiveCode Ltd.\", to unify the company name with the product. In April 2013, a free/open source version 'LiveCode Community Edition 6.0' was published after a successful crowdfunding campaign at Kickstarter. The code base was re-licensed and made available as free and open source software with a version in April 2013.\nLiveCode runs on iOS, Android, OS X, Windows 95 through Windows 10, Raspberry Pi and several variations of Unix, including Linux, Solaris, and BSD. It can be used for mobile, desktop and server/CGI applications. The iOS (iPhone and iPad) version was released in December 2010. The first version to deploy to the Web was released in 2009. It is the most widely used HyperCard/HyperTalk clone, and the only one that runs on all major operating systems.\nA developer release of v.8 was announced in New York on March 12, 2015. This major enhancement to the product includes a new, separate development language, known as \"LiveCode Builder\",  which is capable of creating new object classes called \"widgets\". In earlier versions, the set of object classes was fixed, and could be enhanced only via the use of ordinary procedural languages such as C. The new language, which runs in its own IDE, is a departure from the transitional x-talk paradigm in that it permits typing of variables. But the two environments are fully integrated, and apart from the ability to create new objects, development in LiveCode proceeds in the normal way, within the established IDE.\nA second crowdfunding campaign to Bring HTML5 to LiveCode reached funding goals of nearly US$400,000 on July 31, 2014. LiveCode developer release 8.0 DP4 (August 31, 2015) was the first to include a standalone deployment option to HTML5.\nOn 31 August 2021, starting with version 9.6.4, LiveCode Community edition, licensed under GPL, was discontinued.\n\n",
        "pageid": 30890362
    },
    "Logo (programming language)": {
        "title": "Logo (programming language)",
        "extract": "Logo is an educational programming language, designed in 1967 by Wally Feurzeig, Seymour Papert, and Cynthia Solomon. Logo is not an acronym: the name was coined by Feurzeig while he was at Bolt, Beranek and Newman, and derives from the Greek logos, meaning 'word' or 'thought'.\nA general-purpose language, Logo is widely known for its use of turtle graphics, in which commands for movement and drawing produced line or vector graphics, either on screen or with a small robot termed a turtle. The language was conceived to teach concepts of programming related to Lisp and only later to enable what Papert called \"body-syntonic reasoning\", where students could understand, predict, and reason about the turtle's motion by imagining what they would do if they were the turtle. There are substantial differences among the many dialects of Logo, and the situation is confused by the regular appearance of turtle graphics programs that are named Logo.\nLogo is a multi-paradigm adaptation and dialect of Lisp, a functional programming language. There is no standard Logo, but UCBLogo has the best facilities for handling lists, files, I/O, and recursion in scripts, and can be used to teach all computer science concepts, as UC Berkeley lecturer Brian Harvey did in his Computer Science Logo Style trilogy.\nLogo is usually an interpreted language, although compiled Logo dialects (such as Lhogho and Liogo) have been developed. Logo is not case-sensitive but retains the case used for formatting purposes.\n\n",
        "pageid": 18334
    },
    "Lua (programming language)": {
        "title": "Lua (programming language)",
        "extract": "Lua  is a lightweight, high-level, multi-paradigm programming language designed mainly for embedded use in applications. Lua is cross-platform software, since the interpreter of compiled bytecode is written in ANSI C, and Lua has a relatively simple C application programming interface (API) to embed it into applications.\nLua originated in 1993 as a language for extending software applications to meet the increasing demand for customization at the time. It provided the basic facilities of most procedural programming languages, but more complicated or domain-specific features were not included; rather, it included mechanisms for extending the language, allowing programmers to implement such features. As Lua was intended to be a general embeddable extension language, the designers of Lua focused on improving its speed, portability, extensibility and ease-of-use in development.\n\n",
        "pageid": 46150
    },
    "Macroprogramming": {
        "title": "Macroprogramming",
        "extract": "In computer science, macroprogramming is a programming paradigm \naimed at expressing the macroscopic, global behaviour of an entire system of agents or computing devices.\nIn macroprogramming, the local programs for the individual components of a distributed system are compiled or interpreted from a macro-program typically expressed by a system-level perspective or in terms of the intended global goal.\nThe aim of macroprogramming approaches is to support expressing the macroscopic interactive behaviour of a whole distributed system of computing devices or agents in a single program, or, similarly, to promote their collective intelligence.\nIt is not to be confused with macros, the mechanism often found in programming languages (like C or Scala) to express substitution rules for program pieces.\nMacroprogramming originated in the context of wireless sensor network programming\nand found renewed interest in the context of the Internet of Things and swarm robotics.\nMacroprogramming shares similar goals (related to  programming a system by a global perspective) with multitier programming, choreographic programming, and aggregate computing.",
        "pageid": 73047769
    },
    "MATH-MATIC": {
        "title": "MATH-MATIC",
        "extract": "MATH-MATIC is the marketing name for the AT-3 (Algebraic Translator 3) compiler, an early programming language for the UNIVAC I and UNIVAC II.\nMATH-MATIC was written beginning around 1955 by a team led by Charles Katz under the direction of Grace Hopper. A preliminary manual was produced in 1957 and a final manual the following year.\nSyntactically, MATH-MATIC was similar to Univac's contemporaneous business-oriented language, FLOW-MATIC, differing in providing algebraic-style expressions and floating-point arithmetic, and arrays rather than record structures.",
        "pageid": 202110
    },
    "Mercury (RemObjects BASIC programming language)": {
        "title": "Mercury (RemObjects BASIC programming language)",
        "extract": "Mercury (promoted as Modern Visual Basic) is a programming language developed by RemObjects Software. RemObjects extends VB.Net underlying language and plans to add more features to it.\nMercury is a commercial product and is the sixth language supported by  RemObjects Elements Compiler toolchain, next to C#, Swift, Java, Go and Oxygene. It integrates with Microsoft's Visual Studio on Windows, as well as  RemObjects Elements IDE called Water on Windows and Fire on macOS.",
        "pageid": 70564140
    },
    "MiniKanren": {
        "title": "MiniKanren",
        "extract": "miniKanren is a family of programming languages for relational programming. As relations are bidirectional, if miniKanren is given an expression and a desired output, miniKanren can run the expression \"backward\", finding all possible inputs to the expression that produce the desired output. This bidirectional behavior allows the user to constrain both the input to the program and the result of the program simultaneously. miniKanren performs an interleaved search which will eventually find any solution that exists, even if any one branch of the search tree is infinitely long and contains no solutions. If no solution exists, miniKanren may search forever if the search tree is infinite.\nAn example of miniKanren code is evalo, a relational goal that relates expressions to the values that they evaluate to. When evalo is called in miniKanren like so: (evalo q q), it will generate quines, that is, expressions q that when run will evaluate to themselves.\nThe book The Reasoned Schemer uses miniKanren to demonstrate relational programming, and provides a complete implementation in Scheme. The core of the language fits on two printed pages. The Scheme implementation of miniKanren is designed to be easily understood, modified, and extended.\nαleanTAP is a program written in αKanren, an extension of miniKanren for nominal logic. Given a theorem, it can find a proof, making it a theorem-prover. Given a proof, it can find the theorem, making it a theorem-checker. Given part of a proof and part of a theorem, it will fill in the missing parts of the proof and the theorem, making it a theorem-explorer.\nThere are implementations of miniKanren in Haskell, Racket, Ruby, Clojure, JavaScript, Scala, Swift, Dart and Python. The canonical implementation is an embedded language in Scheme. The Clojure core.logic library was inspired by miniKanren.\nThe name kanren comes from a Japanese word (関連) meaning \"relation\".",
        "pageid": 39116526
    },
    "Mocklisp": {
        "title": "Gosling Emacs",
        "extract": "Gosling Emacs (often shortened to \"Gosmacs\" or \"gmacs\") is a discontinued Emacs implementation written in 1981 by James Gosling in C.\nGosling initially allowed Gosling Emacs to be redistributed with no formal restrictions, as required by the \"Emacs commune\" since the 1970s,  only asking for a letter acknowledging his authorship. Later, wishing to move on and after a failed search for people who would maintain it under the same rights, he finally sold his version of Emacs to UniPress because they agreed to sell it under reasonable terms. The dispute between Richard Stallman and UniPress inspired the creation of the first formal license for Emacs, which later became the GPL, as Congress had introduced copyright for software in 1980.",
        "pageid": 59666
    },
    "Mojo (programming language)": {
        "title": "Mojo (programming language)",
        "extract": "Mojo is a programming language in the Python family that is currently under development. It is available both in browsers via Jupyter notebooks, and locally on Linux and macOS. Mojo aims to combine the usability of a high-level programming language, specifically Python, with the performance of a system programming language such as C++, Rust, and Zig. As of February 2025, the Mojo compiler is closed source with an open source standard library. Modular, the company behind Mojo, has stated an intent to eventually open source the Mojo language, as it matures.\nMojo builds on the Multi-Level Intermediate Representation (MLIR) compiler software framework instead of directly on the lower level LLVM compiler framework, as do many languages such as Julia, Swift, Clang, and Rust. MLIR is a newer compiler framework that allows Mojo to exploit higher level compiler passes unavailable in LLVM alone, and allows Mojo to compile down and target more than only central processing units (CPUs), including producing code that can run on graphics  processing units (GPUs), Tensor Processing Units (TPUs), application-specific integrated circuits (ASICs) and other accelerators. It can also often more effectively use certain types of CPU optimizations directly, like single instruction, multiple data (SIMD) with minor intervention by a developer, as occurs in many other languages. According to Jeremy Howard of fast.ai, Mojo can be seen as \"syntax sugar for MLIR\" and for that reason Mojo is well optimized for applications like artificial intelligence (AI).",
        "pageid": 73729965
    },
    "Multi-adjoint logic programming": {
        "title": "Multi-adjoint logic programming",
        "extract": "Multi-adjoint logic programming defines syntax and semantics of a logic programming program in such a way that the underlying maths justifying the results are a residuated lattice and/or MV-algebra.\nThe definition of a multi-adjoint logic program is given, as usual in fuzzy logic programming, as a set of weighted rules and facts of a given formal language F. Notice that we are allowed to use different implications in our rules.\nDefinition: A multi-adjoint logic program is a set P of rules of the form <(A ←i B), δ> such that:\n1. The rule (A ←i B) is a formula of F;\n2. The confidence factor δ is an element (a truth-value) of L;\n3. The head A is an atom;\n4. The body B is a formula built from atoms B1, …, Bn (n ≥ 0) by the use of conjunctors, disjunctors, and aggregators.\n5. Facts are rules with body ┬.\n6. A query (or goal) is an atom intended as a question ?A prompting the system.",
        "pageid": 2099529
    },
    "Nemerle": {
        "title": "Nemerle",
        "extract": "Nemerle is a general-purpose, high-level, statically typed programming language designed for platforms using the Common Language Infrastructure (.NET/Mono). It offers functional, object-oriented, aspect-oriented, reflective and imperative features. It has a simple C#-like syntax and a powerful metaprogramming system. \nIn June 2012, the core developers of Nemerle were hired by the Czech software development company JetBrains. The team was focusing on developing Nitra, a framework to implement extant and new programming languages. Both the Nemerle language and Nitra have seemingly been abandoned or discontinued by JetBrains; Nitra has not been updated by its original creators since 2017 and Nemerle is now maintained entirely by the Russian Software Development Network, independently from JetBrains, although no major updates have been released yet and development is progressing very slowly. Neither Nemerle, nor Nitra have been mentioned or referenced by JetBrains for years.\nNemerle is named after the Archmage Nemmerle, a character in the fantasy novel A Wizard of Earthsea by Ursula K. Le Guin.",
        "pageid": 30883042
    },
    "Nim (programming language)": {
        "title": "Nim (programming language)",
        "extract": "Nim is a general-purpose, multi-paradigm, statically typed, compiled high-level system programming language, designed and developed by a team around Andreas Rumpf. Nim is designed to be \"efficient, expressive, and elegant\", supporting metaprogramming, functional, message passing, procedural, and object-oriented programming styles by providing several features such as compile time code generation, algebraic data types, a foreign function interface (FFI) with C, C++, Objective-C, and JavaScript, and supporting compiling to those same languages as intermediate representations.",
        "pageid": 45413679
    },
    "OpenQASM": {
        "title": "OpenQASM",
        "extract": "Open Quantum Assembly Language (OpenQASM; pronounced open kazm) is a programming language designed for describing quantum circuits and algorithms for execution on quantum computers.",
        "pageid": 55836870
    },
    "Pencil Code (programming language)": {
        "title": "Pencil Code (programming language)",
        "extract": "Pencil Code is an educational programming language and website.  It allows programming using Scratch-style block coding or CoffeeScript.  Code runs directly in the web browser and can be shared with others.  The language centers on a model of a pencil programmatically drawing on a 2-dimensional screen, with the pencil cursor visually depicted as a turtle.\nA 2019 study by Deng et al. in an eight-week teaching intervention comparing text-based and block-based environments found that students learning in a mixed environment had improved confidence and computational thinking.",
        "pageid": 73001932
    },
    "Perl": {
        "title": "Perl",
        "extract": "Perl is a high-level, general-purpose, interpreted, dynamic programming language. Though Perl is not officially an acronym, there are various backronyms in use, including \"Practical Extraction and Reporting Language\".\nPerl was developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier. Since then, it has undergone many changes and revisions. Perl originally was not capitalized and the name was changed to being capitalized by the time Perl 4 was released. The latest release is Perl 5, first released in 1994. From 2000 to October 2019 a sixth version of Perl was in development; the sixth version's name was changed to Raku. Both languages continue to be developed independently by different development teams which liberally borrow ideas from each other.\nPerl borrows features from other programming languages including C, sh, AWK, and sed. It provides text processing facilities without the arbitrary data-length limits of many contemporary Unix command line tools. Perl is a highly expressive programming language: source code for a given algorithm can be short and highly compressible.\nPerl gained widespread popularity in the mid-1990s as a CGI scripting language, in part due to its powerful regular expression and string parsing abilities. In addition to CGI, Perl 5 is used for system administration, network programming, finance, bioinformatics, and other applications, such as for graphical user interfaces (GUIs). It has been nicknamed \"the Swiss Army chainsaw of scripting languages\" because of its flexibility and power. In 1998, it was also referred to as the \"duct tape that holds the Internet together\", in reference to both its ubiquitous use as a glue language and its perceived inelegance.",
        "pageid": 23939
    },
    "Pharo": {
        "title": "Pharo",
        "extract": "Pharo is a cross-platform implementation of the classic Smalltalk-80 programming language and runtime system. It is based on the OpenSmalltalk virtual machine (VM) named Cog,: 16  which evaluates a dynamic, reflective, and object-oriented programming language with a syntax closely resembling Smalltalk-80. It is free and open-source software, released under a mix of MIT, and Apache 2 licenses.\nPharo is shipped with source code compiled into a system image that contains all software needed to run Pharo.: 16  Like the original Smalltalk-80, Pharo provides several live programming features such as immediate object manipulation, live updates, and just-in-time compilation (JIT). The system image includes an integrated development environment (IDE) to modify its components.\nPharo was forked from Squeak v3.9 in March 2008.: 10 \n\n",
        "pageid": 23490878
    },
    "PHP": {
        "title": "PHP",
        "extract": "PHP is a general-purpose scripting language geared towards web development. It was originally created by Danish-Canadian programmer Rasmus Lerdorf in 1993 and released in 1995. The PHP reference implementation is now produced by the PHP Group. PHP was originally an abbreviation of Personal Home Page, but it now stands for the recursive acronym PHP: Hypertext Preprocessor.\nPHP code is usually processed on a web server by a PHP interpreter implemented as a module, a daemon or a Common Gateway Interface (CGI) executable. On a web server, the result of the interpreted and executed PHP code—which may be any type of data, such as generated HTML or binary image data—would form the whole or part of an HTTP response. Various web template systems, web content management systems, and web frameworks exist that can be employed to orchestrate or facilitate the generation of that response. Additionally, PHP can be used for many programming tasks outside the web context, such as standalone graphical applications and drone control. PHP code can also be directly executed from the command line.\nThe standard PHP interpreter, powered by the Zend Engine, is free software released under the PHP License. PHP has been widely ported and can be deployed on most web servers on a variety of operating systems and platforms.\nThe PHP language has evolved without a written formal specification or standard, with the original implementation acting as the de facto standard that other implementations aimed to follow. \nW3Techs reports that as of 27 October 2024 (about two years since  PHP 7 was discontinued and 11 months after the PHP 8.3 release), PHP 7 is still used by 50.0% of PHP websites, which is outdated and known to be insecure. In addition, the even more outdated (discontinued for 5+  years) and insecure PHP 5 is used by 13.2% and the no longer supported PHP 8.0 is also very popular, so  the majority of PHP websites do not use supported versions.",
        "pageid": 24131
    },
    "PIC (markup language)": {
        "title": "PIC (markup language)",
        "extract": "In computing, Pic is a domain-specific programming language by Brian Kernighan for specifying line diagrams.\nThe language contains predefined basic linear objects: line, move, arrow, and spline, the planar\nobjects box, circle, ellipse, arc, and definable composite elements.\nObjects are placed with respect to other objects or absolute coordinates.\nA liberal interpretation of the input invokes\ndefault parameters when objects are incompletely specified.\nAn interpreter translates this description into\nconcrete drawing commands in a variety of possible output formats.\nPic is a procedural programming language, with variable assignment, macros, conditionals, and looping. The language is an example of a little language originally intended for the comfort of non-programmers in the Unix environment (Bentley 1988).",
        "pageid": 8033525
    },
    "Pico (programming language)": {
        "title": "Pico (programming language)",
        "extract": "Pico is a programming language developed at the Software Languages Lab at Vrije Universiteit Brussel, intended to be simple, powerful, extensible, and easy to read. The language was created to introduce the essentials of programming to non-computer science students.\nPico can be seen as an effort to generate a palatable and enjoyable language for people who do not want to study hard for the elegance and power of a language. They have done it by adapting Scheme's semantics.\nWhile designing Pico, the Software Languages Lab was inspired by the Abelson and Sussman's book \"Structure and Interpretation of Computer Programs\". Furthermore, they were influenced by the teaching of programming at high school or academic level.\nPico should be interpreted as 'small', the idea was to create a small language for educational purposes.",
        "pageid": 379013
    },
    "PL/M": {
        "title": "PL/M",
        "extract": "PL/M, an acronym for Programming Language for Microcomputers, is a high-level language conceived and developed by Gary Kildall in 1973 for Hank Smith at Intel for the Intel 8008. It was later expanded for the newer Intel 8080.\nThe 8080 had enough power to run the PL/M compiler, but lacked a suitable form of mass storage. In an effort to port the language from the PDP-10 to the 8080, Kildall used PL/M to write a disk operating system that allowed a floppy disk to be used. This was the basis of CP/M.",
        "pageid": 543057
    },
    "POP-2": {
        "title": "POP-2",
        "extract": "POP-2 (also called POP2) is a programming language developed around 1970 from the earlier language POP-1 (developed by Robin Popplestone in 1968, originally named COWSEL) by Robin Popplestone and Rod Burstall at the University of Edinburgh. It drew roots from many sources: the languages Lisp and ALGOL 60, and theoretical ideas from Peter J. Landin. It used an incremental compiler, which gave it some of the flexibility of an interpreted language, including allowing new function definitions at run time and modification of function definitions while a program runs (both of which are features of dynamic compilation), without the overhead of an interpreted language.",
        "pageid": 981616
    },
    "Processing": {
        "title": "Processing",
        "extract": "Processing is a free graphics library and integrated development environment (IDE) built for the electronic arts, new media art, and visual design communities with the purpose of teaching non-programmers the fundamentals of computer programming in a visual context.\nProcessing uses the Java programming language, with additional simplifications such as additional classes and aliased mathematical functions and operations. It also provides a graphical user interface for simplifying the compilation and execution stage.\nThe Processing language and IDE have been the precursor to other projects including Arduino and Wiring.",
        "pageid": 546083
    },
    "Programming Languages: Application and Interpretation": {
        "title": "Programming Languages: Application and Interpretation",
        "extract": "Programming Languages: Application and Interpretation (PLAI) is a free programming language textbook by Shriram Krishnamurthi.  It is in use at over 30 universities, in several high-schools.\nThe book differs from most other programming language texts in its attempt to wed two different styles of programming language education: one based on language surveys and another based on interpreters.  In the former style, it can be too easy to ignore difficult technical points, which are sometimes best understood by trying to reproduce them (via implementation); in the latter, it can be too easy to miss the high-level picture in the forest of details.  PLAI therefore interleaves the two, using the survey approach to motivate ideas and interpreters to understand them.\nThe book is accompanied by supporting software that runs in the Racket programming language.\nSince PLAI is constantly under development, some of the newer material (especially assignments) is found on course pages at Brown University.\nPLAI is also an experiment in publishing methods.  The essay\nBooks as Software discusses why the book is self-published.  The current public release is version 3.2.2 (2023-02-26) is available as a free electronic edition for screen use or printing.",
        "pageid": 23749946
    },
    "PureBasic": {
        "title": "PureBasic",
        "extract": "PureBasic is a commercially distributed procedural computer programming language and integrated development environment based on BASIC and developed by Fantaisie Software for Windows, Linux, macOS and Raspberry Pi. An Amiga version is available, although it has been discontinued and some parts of it are released as open-source. The first public release of PureBasic for Windows was on 17 December 2000. It has been continually updated ever since.\nPureBasic has a \"lifetime license model\". As cited on the website, the first PureBasic user (who registered in 1998) still has free access to new updates and this is not going to change.\nPureBasic compiles directly to IA-32, x86-64, PowerPC or 680x0  instruction sets, generating small standalone executables and DLLs which need no runtime libraries beyond the standard system libraries. Programs developed without using the platform-specific application programming interfaces (APIs) can be built easily from the same source file with little or no modification.\nPureBasic supports inline assembly, allowing the developer to include FASM assembler commands within PureBasic source code, while using the variables declared in PureBasic source code, enabling experienced programmers to improve the speed of speed-critical sections of code. PureBasic supports and has integrated the OGRE 3D Environment. Other 3D environments such as the Irrlicht Engine are unofficially supported.\nSince version 6.00 (June 2022), in addition to compilation using ASM, PureBasic offers compilation with a C backend. This enables access to new platforms (e.g. Raspberry) and should make it easier to add new libraries in the future.\n\n",
        "pageid": 60643
    },
    "PV-Wave": {
        "title": "PV-Wave",
        "extract": "PV-WAVE (Precision Visuals - Workstation Analysis and Visualization Environment) is an array oriented fourth-generation programming language used by engineers, scientists, researchers, business analysts and software developers to build and deploy visual data analysis applications. In January 2019, PV-Wave parent Rogue Wave Software was acquired by Minneapolis, Minnesota-based application software developer Perforce.",
        "pageid": 17742923
    },
    "Python (programming language)": {
        "title": "Python (programming language)",
        "extract": "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\nPython is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n\n",
        "pageid": 23862
    },
    "Qore (programming language)": {
        "title": "Qore (programming language)",
        "extract": "Qore is an interpreted, high-level, general-purpose, garbage collected  dynamic programming language, featuring support for code embedding and sandboxing with optional strong typing and a focus on fundamental support for multithreading and SMP scalability.\nQore is unique because it is an interpreted scripting language with fundamental support for multithreading (meaning more than one part of the same code can run at the same time), and additionally because it features automatic memory management (meaning programmers do not have to allocate and free memory explicitly) while also supporting the RAII idiom with destructors for scope-based resource management and exception-safe programming.  This is due to Qore's unique prompt collection implementation for garbage collection.\n\n",
        "pageid": 33850384
    },
    "Quantum Computation Language": {
        "title": "Quantum Computation Language",
        "extract": "Quantum Computation Language (QCL) is one of the first implemented quantum programming languages. The most important feature of QCL is the   support for user-defined operators and functions. Its syntax resembles the syntax of the C programming language and its classical data types are similar to primitive data types in C. One can combine classical code and quantum code in the same program.\nThe language was created before there were multi-qubit quantum computers and the only implementation of QCL uses an interpreter with a built-in classically simulated quantum computer.\nQCL was created to explore programming concepts for quantum computers.\nThe QCL library provides standard quantum operators used in quantum algorithms such as:\n\nControlled-not with many target qubits,\nHadamard operation on many qubits,\nPhase and controlled phase.\nQuantum algorithms for addition, multiplication and exponentiation with binary constants (all modulus n)\nThe quantum fourier transform",
        "pageid": 55836939
    },
    "Raku (programming language)": {
        "title": "Raku (programming language)",
        "extract": "Raku is a member of the Perl family of programming languages. Formerly named Perl 6, it was renamed in October 2019. Raku introduces elements of many modern and historical languages. Compatibility with Perl was not a goal, though a compatibility mode is part of the specification. The design process for Raku began in 2000.\n\n",
        "pageid": 1146638
    },
    "Real-time Programming Language": {
        "title": "Real-time Programming Language",
        "extract": "Real-time Programming Language (RPL) is a compiled database programming language used on CMC/Microdata/McDonnell Douglas REALITY databases, derived and expanded from the PROC procedure language, with much extra functionality added.  It was originally developed under the name \"PORC\" by John Timmons and Paul Desjardins in about 1975.  \"PORC\" was then further developed by Tim Holland under the employ of George Ridgway's company Systems Management, Inc. (SMI) in Chicago.  A number of large scale manufacturing applications were developed in RPL, including that which was in use at Plessey and GEC-Plessey Telecommunications limited in Liverpool and also the Trifid suite of manufacturing software.",
        "pageid": 14524293
    },
    "Rebol": {
        "title": "Rebol",
        "extract": "Rebol ( REB-əl; historically REBOL) is a cross-platform data exchange language and a multi-paradigm dynamic programming language designed by Carl Sassenrath for network communications and distributed computing.  It introduces the concept of dialecting: small, optimized, domain-specific languages for code and data, which is also the most notable property of the language according to its designer Carl Sassenrath:\n\nAlthough it can be used for programming, writing functions, and performing processes, its greatest strength is the ability to easily create domain-specific languages or dialects\nDouglas Crockford, known for his involvement in the development of JavaScript, has described Rebol as \"a more modern language, but with some very similar ideas to  Lisp, in that it's all built upon a representation of data which is then executable as programs\" and as one of JSON's influences.\nOriginally, the language and its official implementation were proprietary and closed source, developed by REBOL Technologies. Following discussion with Lawrence Rosen, the Rebol version 3 interpreter was released under the Apache 2.0 license on December 12, 2012. Older versions are only available in binary form, and no source release for them is planned.\nRebol has been used to program Internet applications (both client- and server-side), database applications, utilities, and multimedia applications.\n\n",
        "pageid": 26384
    },
    "Red (programming language)": {
        "title": "Red (programming language)",
        "extract": "Red is a programming language designed to overcome the limitations of the programming language Rebol. Red was introduced in 2011 by Nenad Rakočević, and is both an imperative and functional programming language. Its syntax and general usage overlaps that of the interpreted Rebol language.\nThe implementation choices of Red intend to create a full stack programming language: Red can be used for extremely high-level programming (DSLs and GUIs) as well as low-level programming (operating systems and device drivers). Key to the approach is that the language has two parts: Red/System and Red.\n\nRed/System is similar to C, but packaged into a Rebol lexical structure –  for example, one would write if x > y [print \"Hello\"] instead of if (x > y) {printf(\"Hello\\n\");}.\nRed is a homoiconic language, which is capable of meta-programming with Rebol-like semantics. Red's runtime library is written in Red/System, and uses a hybrid approach: it compiles what it can deduce statically and uses an embedded interpreter otherwise. The project roadmap includes a just-in-time compiler for cases in between, but this has not yet been implemented.\nRed seeks to remain independent of any other toolchain; it does its own code generation. It is therefore possible to cross-compile Red programs from any platform it supports to any other, via a command-line switch. Both Red and Red/System are distributed as open-source software under the modified BSD license. The runtime library is distributed under the more permissive Boost Software License.\nAs of version 0.6.4 Red includes a garbage collector \"the Simple GC\".",
        "pageid": 35733875
    },
    "Refal": {
        "title": "Refal",
        "extract": "Refal (\"Recursive functions algorithmic language\"; Russian: РЕФАЛ) \"is a functional programming language oriented toward symbolic computations\", including \"string processing, language translation, [and] artificial intelligence\". It is one of the oldest members of this family, first conceived of in 1966 as a theoretical tool, with the first implementation appearing in 1968. Refal was intended to combine mathematical simplicity with practicality for writing large and sophisticated programs.\nOne of the first functional programming languages to do so, and unlike Lisp of its time, Refal is based on pattern matching. Its pattern matching works in conjunction with term rewriting.\nThe basic data structure of Lisp and Prolog is a linear list built by cons operation in a sequential manner, thus with O(n) access to list's nth element. Refal's lists are built and scanned from both ends, with pattern matching working for nested lists as well as the top-level one. In effect, the basic data structure of Refal is a tree rather than a list. This gives freedom and convenience in creating data structures while using only mathematically simple control mechanisms of pattern matching and substitution.\nRefal also includes a feature called the freezer to support efficient partial evaluation.\nRefal can be applied to the processing and transformation of tree structures, similarly to XSLT.",
        "pageid": 14926151
    },
    "Ring (programming language)": {
        "title": "Ring (programming language)",
        "extract": "Ring is a dynamically typed, general-purpose programming language. It can be embedded in C/C++ projects, extended using C/C++ code or used as a standalone language. The supported programming paradigms are imperative, procedural, object-oriented, functional, meta, declarative using nested structures, and natural programming. The language is portable (Windows, Linux, macOS, Android, WebAssembly, etc.) and can be used to create console, GUI, web, game and mobile applications.",
        "pageid": 64207904
    },
    "S-PLUS": {
        "title": "S-PLUS",
        "extract": "S-PLUS is a commercial implementation of the S programming language sold by TIBCO Software Inc.\nIt features object-oriented programming capabilities and advanced analytical algorithms. Its statistical analysis capabilities are commonly used by econometricians. The S-PLUS FinMetrics software package was developed for econometric time series analysis.\nDue to the increasing popularity of the open source S successor R, TIBCO Software released the TIBCO Enterprise Runtime for R (TERR) as an alternative R interpreter. It is available on Windows and UNIX operating systems.\n\n",
        "pageid": 3830007
    },
    "Scala (programming language)": {
        "title": "Scala (programming language)",
        "extract": "Scala ( SKAH-lah) is a strong statically typed high-level general-purpose programming language that supports both object-oriented programming and functional programming. Designed to be concise, many of Scala's design decisions are intended to address criticisms of Java.\nScala source code can be compiled to Java bytecode and run on a Java virtual machine (JVM). Scala can also be transpiled to JavaScript to run in a browser, or compiled directly to a native executable. When running on the JVM, Scala provides language interoperability with Java so that libraries written in either language may be referenced directly in Scala or Java code. Like Java, Scala is object-oriented, and uses a syntax termed curly-brace which is similar to the language C. Since Scala 3, there is also an option to use the off-side rule (indenting) to structure blocks, and its use is advised. Martin Odersky has said that this turned out to be the most productive change introduced in Scala 3.\nUnlike Java, Scala has many features of functional programming languages (like Scheme, Standard ML, and Haskell), including currying, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), anonymous types, operator overloading, optional parameters, named parameters, raw strings, and an experimental exception-only version of algebraic effects that can be seen as a more powerful version of Java's checked exceptions.\nThe name Scala is a portmanteau of scalable and language, signifying that it is designed to grow with the demands of its users.",
        "pageid": 3254510
    },
    "Scientific Vector Language": {
        "title": "Scientific Vector Language",
        "extract": "SVL or Scientific Vector Language is a programming language created by Chemical Computing Group. It was first released in 1994. SVL is the built-in command, scripting and application development language of MOE. It is a \"chemistry aware\" computer programming language with over 1,000 specific functions for analyzing and manipulating chemical structures and related molecular objects. SVL is a concise, high-level language whose programs are typically 10 times smaller than their equivalent when compared to C or Fortran. SVL source code is compiled to a \"byte code\" representation, which is then executed by the base run-time environment making SVL programs inherently portable across different computer hardware and operating systems.\n\n",
        "pageid": 12594048
    },
    "Scriptol": {
        "title": "Scriptol",
        "extract": "Scriptol is an object-oriented programming language that allows users to declare an XML document as a class. The language is universal and allows users to create dynamic web pages, as well as create scripts and binary applications.",
        "pageid": 19949822
    },
    "Self (programming language)": {
        "title": "Self (programming language)",
        "extract": "Self is a general-purpose, high-level, object-oriented programming language based on the concept of prototypes. Self began as a dialect of Smalltalk, being dynamically typed and using just-in-time compilation (JIT) with the prototype-based approach to objects: it was first used as an experimental test system for language design in the 1980s and 1990s. In 2006, Self was still being developed as part of the Klein project, which was a Self virtual machine written fully in Self. The latest version, 2024.1 was released in August 2024.\nSeveral just-in-time compilation techniques were pioneered and improved in Self research as they were required to allow a very high level object oriented language to perform at up to half the speed of optimized C. Much of the development of Self took place at Sun Microsystems, and the techniques they developed were later deployed for Java's HotSpot virtual machine.\nAt one point a version of Smalltalk was implemented in Self. Because it was able to use the JIT, this also gave extremely good performance.\n\n",
        "pageid": 60265
    },
    "SenseTalk": {
        "title": "SenseTalk",
        "extract": "SenseTalk is a high-level English-like scripting language in the XTalk family, that supports both procedural and object-oriented paradigms. SenseTalk scripts are intended to be largely readable by ordinary people, including those with little to no training in programming. \nTo this end, SenseTalk includes a number of language elements that provide functionality oriented towards human tasks rather than the underlying machine behavior. For example, to check whether a quantity is divisible by 3, the script could use the expression if quantity is divisible by 3 … or if quantity is a multiple of 3 …, with the emphasis being on readability and a focus on the human concept of divisibility.  Compare this to more traditional programming languages (C, Java, Python, etc.) where the same test would typically be written as if (quantity % 3) == 0 …, with the focus being on the machine operations needed to determine the result. \nThis shift in focus away from the underlying machine computation, towards an English-like description of the behavior in human terms leads to the description of SenseTalk as a “People Oriented Programming language”. \n\n",
        "pageid": 9096092
    },
    "Simula": {
        "title": "Simula",
        "extract": "Simula is the name of two simulation programming languages, Simula I and Simula 67, developed in the 1960s at the Norwegian Computing Center in Oslo, by Ole-Johan Dahl and Kristen Nygaard. Syntactically, it is an approximate superset of ALGOL 60,: 1.3.1  and was also influenced by the design of SIMSCRIPT.\nSimula 67 introduced objects,: 2, 5.3  classes,: 1.3.3, 2  inheritance and subclasses,: 2.2.1  virtual procedures,: 2.2.3  coroutines,: 9.2  and discrete event simulation,: 14.2  and featured garbage collection.: 9.1  Other forms of subtyping (besides inheriting subclasses) were introduced in Simula derivatives.\nSimula is considered the first object-oriented programming language. As its name suggests, the first Simula version by 1962 was designed for doing simulations; Simula 67 though was designed to be a general-purpose programming language and provided the framework for many of the features of object-oriented languages today.\nSimula has been used in a wide range of applications such as simulating very-large-scale integration (VLSI) designs, process modeling, communication protocols, algorithms, and other applications such as typesetting, computer graphics, and education. The influence of Simula is often understated, and Simula-type objects are reimplemented in C++, Object Pascal, Java, C#, and many other languages. Computer scientists such as Bjarne Stroustrup, creator of C++, and James Gosling, creator of Java, have acknowledged Simula as a major influence.",
        "pageid": 29513
    },
    "SLIP (programming language)": {
        "title": "SLIP (programming language)",
        "extract": "SLIP is a list processing computer programming language, invented by Joseph Weizenbaum in the 1960s.  The name SLIP stands for Symmetric LIst Processor.  It was first implemented as an extension to the Fortran programming language, and later embedded into MAD and ALGOL. The best known program written in the language is ELIZA, an early natural language processing computer program created by Weizenbaum at the MIT Artificial Intelligence Laboratory.",
        "pageid": 4449554
    },
    "Smalltalk": {
        "title": "Smalltalk",
        "extract": "Smalltalk is a purely object oriented programming language (OOP) that was originally created in the 1970s for educational use, specifically for constructionist learning, but later found use in business. It was created at Xerox PARC by Learning Research Group (LRG) scientists, including Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Diana Merry, and Scott Wallace.\nIn Smalltalk, executing programs are built of opaque, atomic, so-called objects, which are instances of template code stored in classes. These objects intercommunicate by passing of messages, via an intermediary virtual machine environment (VM). A relatively small number of objects, called primitives, are not amenable to live redefinition, sometimes being defined independently of the Smalltalk programming environment.\nHaving undergone significant industry development toward other uses, including business and database functions, Smalltalk is still in use today. When first publicly released, Smalltalk-80 presented numerous foundational ideas for the nascent field of object-oriented programming (OOP).\nSince inception, the language provided interactive programming via an integrated development environment. This requires reflection and late binding in the language execution of code. Later development has led to at least one instance of Smalltalk execution environment which lacks such an integrated graphical user interface or front-end.\nSmalltalk-like languages are in active development and have gathered communities of users around them. American National Standards Institute (ANSI) Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.\nSmalltalk took second place for \"most loved programming language\" in the Stack Overflow Developer Survey in 2017, but it was not among the 26 most loved programming languages of the 2018 survey.",
        "pageid": 28319
    },
    "SNOBOL": {
        "title": "SNOBOL",
        "extract": "SNOBOL (\"StriNg Oriented and symBOlic Language\") is a series of programming languages developed between 1962 and 1967 at AT&T Bell Laboratories by David J. Farber, Ralph Griswold and Ivan P. Polonsky, culminating in SNOBOL4. It was one of a number of text-string-oriented languages developed during the 1950s and 1960s; others included COMIT and TRAC. Despite the similar name, it is entirely unlike COBOL.\nSNOBOL4 stands apart from most programming languages of its era by having patterns as a first-class data type, a data type whose values can be manipulated in all ways permitted to any other data type in the programming language, and by providing operators for pattern concatenation and alternation. SNOBOL4 patterns are a type of object and admit various manipulations, much like later object-oriented languages such as JavaScript whose patterns are known as regular expressions. In addition SNOBOL4 strings generated during execution can be treated as programs and either interpreted or compiled and executed (as in the eval function of other languages).\nSNOBOL4 was quite widely taught in larger U.S. universities in the late 1960s and early 1970s and was widely used in the 1970s and 1980s as a text manipulation language in the humanities.\nIn the 1980s and 1990s, its use faded as newer languages such as AWK and Perl made string manipulation by means of regular expressions fashionable. SNOBOL4 patterns include a way to express BNF grammars, which are equivalent to context-free grammars and more powerful than regular expressions. \nThe \"regular expressions\" in current versions of AWK and Perl are in fact extensions of regular expressions in the traditional sense, but regular expressions, unlike SNOBOL4 patterns, are not recursive, which gives a distinct computational advantage to SNOBOL4 patterns. (Recursive expressions did appear in Perl 5.10, though, released in December 2007.)\nThe later SL5 (1977) and Icon (1978) languages were designed by Griswold to combine the backtracking of SNOBOL4 pattern matching with more standard ALGOL-like structuring.\n\n",
        "pageid": 29515
    },
    "Source (programming language)": {
        "title": "Source (programming language)",
        "extract": "Source is a family of sublanguages of JavaScript, developed for the textbook Structure and Interpretation of Computer Programs, JavaScript Edition (SICP JS). The JavaScript sublanguages Source §1, Source §2, Source §3 and Source §4 are designed to be just expressive enough to support all examples of the respective chapter of the textbook.",
        "pageid": 62661728
    },
    "Squeak": {
        "title": "Squeak",
        "extract": "Squeak is an object-oriented, class-based, and reflective programming language. It was derived from Smalltalk-80 by a group that included some of Smalltalk-80's original developers, initially at Apple Computer, then at Walt Disney Imagineering, where it was intended for use in internal Disney projects. The group later was supported by HP Labs, SAP, and most recently, Y Combinator.\nSqueak runs on a virtual machine (VM), allowing for a high degree of portability. The Squeak system includes code for generating a new version of the VM on which it runs, along with a VM simulator written in Squeak.",
        "pageid": 37426
    },
    "Squirrel (programming language)": {
        "title": "Squirrel (programming language)",
        "extract": "Squirrel is a high level imperative, object-oriented programming language, designed to be a lightweight scripting language that fits in the size, memory bandwidth, and real-time requirements of applications like video games.\nMirthKit, a simple toolkit for making and distributing open source, cross-platform 2D games, uses Squirrel for its platform. It is used extensively by Code::Blocks for scripting and was also used in Final Fantasy Crystal Chronicles: My Life as a King. It is also used in Left 4 Dead 2, Portal 2 and Thimbleweed Park for scripted events and in NewDark, an unofficial Thief 2: The Metal Age engine update, to facilitate additional, simplified means of scripting mission events, aside of the regular C scripting.",
        "pageid": 2819069
    },
    "StaDyn (programming language)": {
        "title": "StaDyn (programming language)",
        "extract": "StaDyn is an object-oriented general-purpose programming language for the .NET platform that supports both static and dynamic typing in the same programming language.\nThe StaDyn compiler gathers type information for the dynamically typed code. That type information is used to detect type errors at compilation time and to perform significant optimizations. For that purpose, it provides type reconstruction (inference), flow-sensitive types, union and intersection types, constraint-based typing,  alias analysis and method specialization.\nIts first prototype appeared in 2007, as a modification of C# 3.0. Type inference was supported by including var as a new type, unlike C#, which only offers var to define initialized local variables. Flow-sensitive types of var references are inferred by the compiler, providing type-safe duck typing. When a more lenient approach is required by the programmer, the dynamictype could be used instead of var. Although type inference is still performed, dynamic references behave closer to those in dynamic languages.\nStaDyn is designed by Francisco Ortin from the University of Oviedo. The language has been implemented by different members of the Computational Reflection research group, including Miguel Garcia, Jose Baltasar García Perez-Schofield and Jose Quiroga, besides Francisco Ortin.\nThe name StaDyn is a portmanteau of static and dynamic, denoting its aim to provide the benefits of both static and dynamic typing.",
        "pageid": 70802400
    },
    "Structured text": {
        "title": "Structured text",
        "extract": "Structured text, abbreviated as ST or STX, is one of the five languages supported by the IEC 61131-3 standard, designed for programmable logic controllers (PLCs). It is a high level language that is block structured and syntactically resembles Pascal, on which it is based. All of the languages share IEC61131 Common Elements. The variables and function calls are defined by the common elements so different languages within the IEC 61131-3 standard can be used in the same program.\nComplex statements and nested instructions are supported:\n\nIteration loops (REPEAT-UNTIL; WHILE-DO)\nConditional execution (IF-THEN-ELSE; CASE)\nFunctions (SQRT(), SIN())",
        "pageid": 3432584
    },
    "Swift (parallel scripting language)": {
        "title": "Swift (parallel scripting language)",
        "extract": "Swift is an implicitly parallel programming language that allows writing scripts that distribute program execution across distributed computing resources, including clusters, clouds, grids, and supercomputers. Swift implementations are open-source software under the Apache License, version 2.0.",
        "pageid": 42946796
    },
    "Swift (programming language)": {
        "title": "Swift (programming language)",
        "extract": "Swift is a high-level general-purpose, multi-paradigm, compiled programming language created by Chris Lattner in 2010 for Apple Inc. and maintained by the open-source community. Swift compiles to machine code and uses an LLVM-based compiler. Swift was first released in June 2014 and the Swift toolchain has shipped in Xcode since Xcode version 6, released in September 2014.\nApple intended Swift to support many core concepts associated with Objective-C, notably dynamic dispatch, widespread late binding, extensible programming, and similar features, but in a \"safer\" way, making it easier to catch software bugs; Swift has features addressing some common programming errors like null pointer dereferencing and provides syntactic sugar to help avoid the pyramid of doom. Swift supports the concept of protocol extensibility, an extensibility system that can be applied to types, structs and classes, which Apple promotes as a real change in programming paradigms they term \"protocol-oriented programming\" (similar to traits and type classes).\nSwift was introduced at Apple's 2014 Worldwide Developers Conference (WWDC). It underwent an upgrade to version 1.2 during 2014 and a major upgrade to Swift 2 at WWDC 2015. It was initially a proprietary language, but version 2.2 was made open-source software under the Apache License 2.0 on December 3, 2015, for Apple's platforms and Linux.\nThrough version 3.0 the syntax of Swift went through significant evolution, with the core team making source stability a focus in later versions. In the first quarter of 2018 Swift surpassed Objective-C in measured popularity.\nSwift 4.0, released in 2017, introduced several changes to some built-in classes and structures. Code written with previous versions of Swift can be updated using the migration functionality built into Xcode. Swift 5, released in March 2019, introduced a stable binary interface on Apple platforms, allowing the Swift runtime to be incorporated into Apple operating systems. It is source compatible with Swift 4.\nSwift 5.1 was officially released in September 2019. Swift 5.1 builds on the previous version of Swift 5 by extending the stable features of the language to compile-time with the introduction of module stability. The introduction of module stability makes it possible to create and share binary frameworks that will work with future releases of Swift.\nSwift 5.5, officially announced by Apple at the 2021 WWDC, significantly expands language support for concurrency and asynchronous code, notably introducing a unique version of the actor model.\nSwift 5.9, was released in September 2023 and includes a macro system, generic parameter packs, and ownership features like the new consume operator.\nSwift 5.10, was released in March 2024. This version improves the language's concurrency model, allowing for full data isolation to prevent data races. It is also the last release before Swift 6. Version 5.10 is currently available for macOS, Windows and for Linux.\nSwift 6 was released in September 2024.",
        "pageid": 42946389
    },
    "Tea (programming language)": {
        "title": "Tea (programming language)",
        "extract": "Tea is a high-level scripting language for the Java environment. It combines features of Scheme, Tcl, and Java.\n\n",
        "pageid": 3431871
    },
    "TI-BASIC 83": {
        "title": "TI-BASIC 83",
        "extract": "TI-BASIC 83,TI-BASIC Z80 or simply TI-BASIC, is the built-in programming language for the Texas Instruments programmable calculators in the TI-83 series. Calculators that implement TI-BASIC have a built in editor for writing programs. While the considerably faster Z80 assembly language: 120  is supported for the calculators, TI-BASIC's in-calculator editor and more user friendly syntax make it easier to use. TI-BASIC is interpreted.: 155 ",
        "pageid": 60386284
    },
    "TREE-META": {
        "title": "TREE-META",
        "extract": "The TREE-META (or Tree Meta, TREEMETA) Translator Writing System is a compiler-compiler system for context-free languages originally developed in the 1960s.  Parsing statements of the metalanguage resemble augmented Backus–Naur form with embedded tree-building directives. Unparsing rules include extensive tree-scanning and code-generation constructs.\n\n",
        "pageid": 20619029
    },
    "TreeDL": {
        "title": "TreeDL",
        "extract": "Tree Description Language (TreeDL) is a computer language for description of strictly-typed tree data structures and operations on them. The main use of TreeDL is in the development of language-oriented tools (compilers, translators, etc.) for the description of a structure of abstract syntax trees.\nTree description can be used as\n\na documentation of interface between parser and other subsystems;\na source for generation of data types representing a tree in target programming languages;\na source for generation of various support code: visitors, walkers, factories, etc.\nTreeDL can be used with any parser generator that allows custom actions during parsing (for example, ANTLR, JavaCC).",
        "pageid": 4388651
    },
    "Trellis-Owl": {
        "title": "Trellis-Owl",
        "extract": "Trellis/Owl, or simply Owl, is a defunct object-oriented programming language created by Digital Equipment Corporation. It was part of a programming environment, Trellis. It ran on the OpenVMS operating system.\nTrellis/Owl differed from contemporary languages in several ways. For one, it did not use dot notation for method calls on objects, and used a traditional functional style instead, which they referred to as operations. Operations were supported by the concept of a controlling object, the first parameter in the function call, which indicated which class was being referred to. Whereas most OO languages of the era might have a myStringVariableToPrint.print() method, in Trellis/Owl this would be print(myStringVariableToPrint), and the print method of the class String would be called based on a string being the first parameter. Trellis/Owl also supported properties, which they referred to as components. Trellis/Owl also included a system allowing the easy creation of iterators, using the yields keyword to replace returns in the definition of an operation. yields indicates the operator will return a series of values instead of one.",
        "pageid": 52218751
    },
    "Tritium (programming language)": {
        "title": "Tritium (programming language)",
        "extract": "Tritium is a simple scripting language for efficiently transforming structured data like HTML, XML, and JSON. It is similar in purpose to XSLT but has a syntax influenced by jQuery, Sass, and CSS versus XSLT's XML based syntax.",
        "pageid": 39932131
    },
    "TTCN-3": {
        "title": "TTCN-3",
        "extract": "TTCN-3 (Testing and Test Control Notation version 3) is a strongly typed testing language used in conformance testing of communicating systems. TTCN-3 is written by ETSI in the ES 201 873 series, and standardized by ITU-T in the Z.160 Series.\nTTCN-3 has its own data types and can be combined with ASN.1, IDL and XML type definitions.",
        "pageid": 16604682
    },
    "Tuple space": {
        "title": "Tuple space",
        "extract": "A tuple space is an implementation of the  associative memory paradigm for parallel/distributed computing. It provides a repository of tuples that can be accessed concurrently. As an illustrative example, consider that there are a group of processors that produce pieces of data and a group of processors that use the data. Producers post their data as tuples in the space, and the consumers then retrieve data from the space that match a certain pattern. This is also known as the blackboard metaphor. Tuple space may be thought as a form of distributed shared memory.\nTuple spaces were the theoretical underpinning of the Linda language developed by David Gelernter and Nicholas Carriero at Yale University in 1986.\nImplementations of tuple spaces have also been developed for Java (JavaSpaces), Lisp, Lua, Prolog, Python, Ruby, Smalltalk, Tcl, and the .NET Framework.",
        "pageid": 1966238
    },
    "TypeScript": {
        "title": "TypeScript",
        "extract": "TypeScript (abbreviated as TS) is a free and open-source high-level programming language developed by Microsoft that adds static typing with optional type annotations to JavaScript. It is designed for the development of large applications and transpiles to JavaScript.\nTypeScript may be used to develop JavaScript applications for both client-side and server-side execution (as with Node.js, Deno or Bun). Multiple options are available for transpilation. The default TypeScript Compiler can be used, or the Babel compiler can be invoked to convert TypeScript to JavaScript.\nTypeScript supports definition files that can contain type information of existing JavaScript libraries, much like C++ header files can describe the structure of existing object files. This enables other programs to use the values defined in the files as if they were statically typed TypeScript entities. There are third-party header files for popular libraries such as jQuery, MongoDB, and D3.js. TypeScript headers for the Node.js library modules are also available, allowing development of Node.js programs within TypeScript.\nThe TypeScript compiler is itself written in TypeScript and compiled to JavaScript. It is licensed under the Apache License 2.0. Anders Hejlsberg, lead architect of C# and creator of Delphi and Turbo Pascal, has worked on the development of TypeScript.",
        "pageid": 8157205
    },
    "Universal Test Specification Language": {
        "title": "Universal Test Specification Language",
        "extract": "Universal Test Specification Language (UTSL) is a programming language used to describe ASIC tests in a format that leads to an automated translation of the test specification into an executable test code. UTSL is platform independent and provided a code generation interface for a specific platform is available, UTSL code can be translated into the programming language of a specific Automatic Test Equipment (ATE).",
        "pageid": 46397207
    },
    "V (programming language)": {
        "title": "V (programming language)",
        "extract": "V, also known as vlang, is a statically typed, compiled programming language created by Alexander Medvednikov in early 2019. It was inspired by the language Go, and other influences including Oberon, Swift, and Rust. It is free and open-source software released under the MIT License, and currently in beta.\nThe goals of V include ease of use, readability, and maintainability.",
        "pageid": 72172767
    },
    "Vala (programming language)": {
        "title": "Vala (programming language)",
        "extract": "Vala is an object-oriented programming language with a self-hosting compiler that generates C code and uses the GObject system.\nVala is syntactically similar to C# and includes notable features such as anonymous functions, signals, properties, generics, assisted memory management, exception handling, type inference, and foreach statements. Its developers, Jürg Billeter and Raffaele Sandrini, wanted to bring these features to the plain C runtime with little overhead and no special runtime support by targeting the GObject object system. Rather than compiling directly to machine code or assembly language, it compiles to a lower-level intermediate language. It source-to-source compiles to C, which is then compiled with a C compiler for a given platform, such as GCC or Clang.\nUsing functionality from native code libraries requires writing vapi files, defining the library interfaces. Writing these interface definitions is well-documented for C libraries. Bindings are already available for a large number of libraries, including libraries that are not based on GObject such as the multimedia library SDL and OpenGL.",
        "pageid": 12655903
    },
    "Visual Basic (classic)": {
        "title": "Visual Basic (classic)",
        "extract": "Visual Basic (VB), sometimes referred to as Classic Visual Basic, is a third-generation programming language based on BASIC, as well as an associated integrated development environment (IDE). Visual Basic was developed by Microsoft for Windows, and is known for supporting rapid application development (RAD) of graphical user interface (GUI) applications, event-driven programming, and both consumption and development of \ncomponents via the Component Object Model (COM) technology. \nVB was first released in 1991. The final release was version 6 (VB6) in 1998. On April 8, 2008, Microsoft stopped supporting the VB6 IDE, relegating it to legacy status. The Microsoft VB team still maintains compatibility for VB6 applications through its \"It Just Works\" program on supported Windows operating systems.\nVisual Basic .NET (VB.NET) is based on Classic Visual Basic. Because VB.NET was later rebranded back to Visual Basic, the name is ambiguous: it can refer to either Classic Visual Basic or to the .NET version.\nJust as BASIC was originally intended to be easy to learn, Microsoft intended the same for VB.\nDevelopment of a VB application is exclusively supported via the VB integrated development environment (IDE), an application in the contemporary Visual Studio suite of tools. Unlike modern versions of Visual Studio, which support many languages including VB (.NET), the VB IDE only supports VB.\nIn 2014, some software developers still preferred Visual Basic 6.0 over its successor, Visual Basic .NET.  Visual Basic 6.0 was selected as the most dreaded programming language by respondents of Stack Overflow's annual developer survey in 2016, 2017, and 2018.",
        "pageid": 6097382
    },
    "XPL": {
        "title": "XPL",
        "extract": "XPL, for expert's programming language is a programming language based on PL/I, a portable one-pass compiler written in its own language, and a parser generator tool for easily implementing similar compilers for other languages.  XPL was designed in 1967 as a way to teach compiler design principles and as starting point for students to build compilers for their own languages.\nXPL was designed and implemented by William M. McKeeman, David B. Wortman, James J. Horning and others at Stanford University. XPL was first announced at the 1968 Fall Joint Computer Conference.  The methods and compiler are described in detail in the 1971 textbook A Compiler Generator.\nThey called the combined work a 'compiler generator'.  But that implies little or no language- or target-specific programming is required to build a compiler for a new language or new target.  A better label for XPL is a translator writing system.  It helps to write a compiler with less new or changed programming code.",
        "pageid": 902082
    },
    "XSLT": {
        "title": "XSLT",
        "extract": "XSLT (Extensible Stylesheet Language Transformations) is a language originally designed for transforming XML documents into other XML documents, or other formats such as HTML for web pages, plain text, or XSL Formatting Objects. These formats can be subsequently converted to formats such as PDF, PostScript, and PNG. Support for JSON and plain-text transformation was added in later updates to the XSLT 1.0 specification.\nAs of August 2022, the most recent stable version of the language is XSLT 3.0, which achieved Recommendation status in June 2017.\nXSLT 3.0 implementations support Java, .NET, C/C++, Python, PHP and NodeJS. An XSLT 3.0 JavaScript library can also be hosted within the web browser. Modern web browsers also include native support for XSLT 1.0.\nThe XSLT document transformation specifies how to transform an XML document into new document (usually XML, but other formats, such as plain text are supported). Typically, input documents are XML files, but anything from which the processor can build an XQuery and XPath Data Model can be used, such as relational database tables or geographical information systems.\nWhile XSLT was originally designed as a special-purpose language for XML transformation, the language is Turing-complete, making it theoretically capable of arbitrary computations.",
        "pageid": 34211
    },
    "Zig (programming language)": {
        "title": "Zig (programming language)",
        "extract": "Zig (Also known as Ziglang) is an imperative, general-purpose, statically typed, compiled system programming language designed by Andrew Kelley. It is free and open-source software, released under an MIT License.\nA major goal of the language is to improve on the C language, (also taking inspiration from Rust), with the intent of being even smaller and simpler to program in, while offering more functionality. The improvements in language simplicity relate to flow control, function calls, library imports, variable declaration and Unicode support. Further, the language makes no use of macros or preprocessor instructions. Features adopted from modern languages include the addition of compile time generic programming data types, allowing functions to work on a variety of data, along with a small set of new compiler directives to allow access to the information about those types using reflective programming (reflection). Like C, Zig omits garbage collection, and has manual memory management. To help eliminate the potential errors that arise in such systems, it includes option types, a simple syntax for using them, and a unit testing framework built into the language. Zig has many features for low-level programming, notably packed structs (structs without padding between fields), arbitrary-width integers and multiple pointer types.\nThe main drawback of the system is that, although Zig has a growing community, as of 2025, it remains a new language with areas for improvement in maturity, ecosystem and tooling. Also the learning curve for Zig can be steep, especially for those unfamiliar with low-level programming concepts. The availability of learning resources is limited for complex use cases, though this is gradually improving as interest and adoption increase. Other challenges mentioned by the reviewers are interoperability with other languages (extra effort to manage data marshaling and communication is required), as well as manual memory deallocation (disregarding proper memory management results directly in memory leaks).\nThe development is funded by the Zig Software Foundation (ZSF), a non-profit corporation with Andrew Kelley as president, which accepts donations and hires multiple full-time employees. Zig has very active contributor community, and is still in its early stages of development. Despite this, a Stack Overflow survey in 2024 found that Zig software developers earn salaries of $103,000 USD per year on average, making it one of the best-paying programming languages. However, only 0.83% reported they were proficient in Zig.",
        "pageid": 61049743
    },
    "Algorithm": {
        "title": "Algorithm",
        "extract": "In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning).\nIn contrast, a heuristic is an approach to solving problems that do not have well-defined correct or optimal results. For example, although social media recommender systems are commonly called \"algorithms\", they actually rely on heuristics as there is no truly \"correct\" recommendation.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.",
        "pageid": 775
    },
    "Coded exposure photography": {
        "title": "Coded exposure photography",
        "extract": "Coded exposure photography, also known as a flutter shutter, is the name given to any mathematical algorithm that reduces the effects of motion blur in photography. The key element of the coded exposure process is the mathematical formula that affects the shutter frequency. This involves the calculation of the relationship between the photon exposure of the light sensor and the randomized code. The camera is made to take a series of snapshots with random time intervals using a simple computer, this creates a blurred image that can be reconciled into a clear image using the algorithm.\nMotion de-blurring technology grew due to increasing demand for clearer images in sporting events and other digital media. The relative inexpensiveness of the coded exposure technology makes it a viable alternative to expensive cameras and equipment that are built to take millions of images per second.",
        "pageid": 60746548
    },
    "Hub labels": {
        "title": "Hub labels",
        "extract": "In computer science, hub labels or the hub-labelling algorithm is a speedup technique that consumes much fewer resources than the lookup table but is still extremely fast for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.\nThis method allows at the most with two SELECT statements and the analysis of two strings to compute the shortest path between two vertices of a graph.\nFor a graph that is oriented like a road graph, this technique requires the prior computation of two tables from structures constructed using the method of the contraction hierarchies. \nIn the end, these two computed tables will have as many rows as nodes present within the graph. For each row (each node), a label will be calculated.\nA label is a string containing the distance information between the current node (the node of the row) and all the other nodes that can be reached with an ascending search on the relative multi-level structure. The advantage of these distances is that they all represent the shortest paths. \nSo, for future queries, the search of a shortest path will start from the source on the first table and the destination on the second table, from which it will search within the labels for the common nodes with the associated distance information. Only the smallest sum of distances will be kept as the shortest path result.\n\n",
        "pageid": 55213052
    },
    "List of algorithm general topics": {
        "title": "List of algorithm general topics",
        "extract": "This is a list of algorithm general topics.\n\nAnalysis of algorithms\nAnt colony algorithm\nApproximation algorithm\nBest and worst cases\nBig O notation\nCombinatorial search\nCompetitive analysis\nComputability theory\nComputational complexity theory\nEmbarrassingly parallel problem\nEmergent algorithm\nEvolutionary algorithm\nFast Fourier transform\nGenetic algorithm\nGraph exploration algorithm\nHeuristic\nHill climbing\nImplementation\nLas Vegas algorithm\nLock-free and wait-free algorithms\nMonte Carlo algorithm\nNumerical analysis\nOnline algorithm\nPolynomial time approximation scheme\nProblem size\nPseudorandom number generator\nQuantum algorithm\nRandom-restart hill climbing\nRandomized algorithm\nRunning time\nSorting algorithm\nSearch algorithm\nStable algorithm (disambiguation)\nSuper-recursive algorithm\nTree search algorithm",
        "pageid": 632487
    },
    "List of algorithms": {
        "title": "List of algorithms",
        "extract": "An algorithm is fundamentally a set of rules or defined procedures that is typically designed and used to solve a specific problem or a broad set of problems. \nBroadly, algorithms define process(es), sets of rules, or methodologies that are to be followed in calculations, data processing, data mining, pattern recognition, automated reasoning or other problem-solving operations. With the increasing automation of services, more and more decisions are being made by algorithms. Some general examples are; risk assessments, anticipatory policing, and pattern recognition technology.\nThe following is a list of well-known algorithms along with one-line descriptions for each.\n\n",
        "pageid": 18568
    },
    "Unrestricted algorithm": {
        "title": "Unrestricted algorithm",
        "extract": "An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.\nIn the problem of developing algorithms for computing, as regards the values of a real-valued function of a real variable (e.g., g[x] in \"restricted\" algorithms), the error that can be tolerated in the result is specified in advance. An interval on the real line would also be specified for values when the values of a function are to be evaluated. Different algorithms may have to be applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of x and also the precision required in g(x) quite arbitrarily. The algorithm should then produce an acceptable result without failure.",
        "pageid": 54117020
    },
    "Adaptive algorithm": {
        "title": "Adaptive algorithm",
        "extract": "An adaptive algorithm is an algorithm that changes its behavior at the time it is run, based on information available and on a priori defined reward mechanism (or criterion). Such information could be the story of recently received data, information on the available computational resources, or other run-time acquired (or a priori known) information related to the environment in which it operates.\nAmong the most used adaptive algorithms is the Widrow-Hoff’s least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. In adaptive filtering the LMS is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square  of the error signal (difference between the desired and the actual signal).\nFor example, stable partition, using no additional memory is O(n lg n) but given O(n) memory, it can be O(n) in time. As implemented by the C++ Standard Library, stable_partition is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory.  Another example is adaptive sort, whose behavior changes upon the presortedness of its input.\nAn example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.\nIn machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters such as learning rate are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, adaptive quadrature, AdaBoost, Adagrad, Adadelta, RMSprop, and Adam.\nIn data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.\nIn signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called \"adaptive\" because the window length (the size of an audio \"chunk\") can change according to the nature of the sound being compressed, to try to achieve the best-sounding compression strategy.",
        "pageid": 8286430
    },
    "Algorism": {
        "title": "Algorism",
        "extract": "Algorism is the technique of performing basic arithmetic by writing numbers in place value form and applying a set of memorized rules and facts to the digits. One who practices algorism is known as an algorist. This positional notation system has largely superseded earlier calculation systems that used a different set of symbols for each numerical magnitude, such as Roman numerals, and in some cases required a device such as an abacus.\n\n",
        "pageid": 417534
    },
    "The Algorithm Auction": {
        "title": "The Algorithm Auction",
        "extract": "The Algorithm Auction is the world's first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.\nFive lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OkCupid's original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.\nTwo other lots were “living algorithms,” including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.\nExhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktivist Richard Jones.\nSebastian Chan, Director of Digital & Emerging Media at the Cooper–Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.\n\n",
        "pageid": 46493377
    },
    "Algorithm characterizations": {
        "title": "Algorithm characterizations",
        "extract": "Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the \"characterizations\" of the notion of \"algorithm\" in more detail.\n\n",
        "pageid": 6901703
    },
    "Algorithm engineering": {
        "title": "Algorithm engineering",
        "extract": "Algorithm engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithmics theory and practical applications of algorithms in software engineering.\nIt is a general methodology for algorithmic research.",
        "pageid": 10140499
    },
    "Algorithmic game theory": {
        "title": "Algorithmic game theory",
        "extract": "Algorithmic game theory (AGT) is an area in the intersection of game theory and computer science, with the objective of understanding and design of algorithms in strategic environments.\nTypically, in Algorithmic Game Theory problems, the input to a given algorithm is distributed among many players who have a personal interest in the output. In those situations, the agents might not report the input truthfully because of their own personal interests. We can see Algorithmic Game Theory from two perspectives:\n\nAnalysis: given the currently implemented algorithms, analyze them using Game Theory tools (e.g., calculate and prove properties on their Nash equilibria, price of anarchy, and best-response dynamics).\nDesign: design games that have both good game-theoretical and algorithmic properties. This area is called algorithmic mechanism design.\nOn top of the usual requirements in classical algorithm design (e.g., polynomial-time running time, good approximation ratio), the designer must also care about incentive constraints.\n\n",
        "pageid": 16334749
    },
    "Algorithmic logic": {
        "title": "Algorithmic logic",
        "extract": "Algorithmic logic is a calculus of programs that allows the expression of semantic properties of programs by appropriate logical formulas. It provides a framework that enables proving the formulas from the axioms of program constructs such as assignment, iteration and composition instructions and from the axioms of the data structures in question see Mirkowska & Salwicki (1987), Banachowski et al. (1977).\nThe following diagram helps to locate algorithmic logic among other logics.\n\n  \n    \n      \n        \n        \n          [\n          \n            \n              \n                \n                  \n                    P\n                    r\n                    o\n                    p\n                    o\n                    s\n                    i\n                    t\n                    i\n                    o\n                    n\n                    a\n                    l\n                     \n                    l\n                    o\n                    g\n                    i\n                    c\n                  \n                \n              \n              \n                \n                  o\n                  r\n                \n              \n              \n                \n                  \n                    S\n                    e\n                    n\n                    t\n                    e\n                    n\n                    t\n                    i\n                    a\n                    l\n                     \n                    c\n                    a\n                    l\n                    c\n                    u\n                    l\n                    u\n                    s\n                  \n                \n              \n            \n          \n          ]\n        \n        ⊂\n        \n          [\n          \n            \n              \n                \n                  \n                    P\n                    r\n                    e\n                    d\n                    i\n                    c\n                    a\n                    t\n                    e\n                     \n                    c\n                    a\n                    l\n                    c\n                    u\n                    l\n                    u\n                    s\n                  \n                \n              \n              \n                \n                  o\n                  r\n                \n              \n              \n                \n                  \n                    F\n                    i\n                    r\n                    s\n                    t\n                     \n                    o\n                    r\n                    d\n                    e\n                    r\n                     \n                    l\n                    o\n                    g\n                    i\n                    c\n                  \n                \n              \n            \n          \n          ]\n        \n        ⊂\n        \n          [\n          \n            \n              \n                \n                  \n                    C\n                    a\n                    l\n                    c\n                    u\n                    l\n                    u\n                    s\n                     \n                    o\n                    f\n                     \n                    p\n                    r\n                    o\n                    g\n                    r\n                    a\n                    m\n                    s\n                  \n                \n              \n              \n                \n                  o\n                  r\n                \n              \n              \n                \n                  \n                    \n                      Algorithmic logic\n                    \n                  \n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\qquad \\left[{\\begin{array}{l}\\mathrm {Propositional\\ logic} \\\\or\\\\\\mathrm {Sentential\\ calculus} \\end{array}}\\right]\\subset \\left[{\\begin{array}{l}\\mathrm {Predicate\\ calculus} \\\\or\\\\\\mathrm {First\\ order\\ logic} \\end{array}}\\right]\\subset \\left[{\\begin{array}{l}\\mathrm {Calculus\\ of\\ programs} \\\\or\\\\{\\mbox{Algorithmic logic}}\\end{array}}\\right]}\n  \n\nThe formalized language of algorithmic logic (and of algorithmic theories of various data structures) contains three types of well formed expressions: Terms - i.e. expressions denoting operations on elements of data structures, \nformulas - i.e. expressions denoting the relations among elements of data structures,  programs - i.e. algorithms - these expressions describe the computations.\nFor semantics of terms and formulas consult pages on first-order logic and Tarski's semantics. The meaning of a program \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n is the set of possible computations of the program.\nAlgorithmic logic is one of many logics of programs.\nAnother logic of programs is dynamic logic, see dynamic logic, Harel, Kozen & Tiuryn (2000).",
        "pageid": 42360188
    },
    "Algorithmic management": {
        "title": "Algorithmic management",
        "extract": "Algorithmic management is a term used to describe certain labor management practices in the contemporary digital economy. In scholarly uses, the term was initially coined in 2015 by Min Kyung Lee, Daniel Kusbit, Evan Metsky, and Laura Dabbish to describe the managerial role played by algorithms on the Uber and Lyft platforms, but has since been taken up by other scholars to describe more generally the managerial and organisational characteristics of platform economies. However, digital direction of labor was present in manufacturing already since the 1970s and algorithmic management is becoming increasingly widespread across a wide range of industries.\nThe concept of algorithmic management can be broadly defined as the delegation of managerial functions to algorithmic and automated systems.  Algorithmic management has been enabled by \"recent advances in digital technologies\" which allow for the real-time and \"large-scale collection of data\" which is then used to \"improve learning algorithms that carry out learning and control functions traditionally performed by managers\".\nIn the contemporary workplace, firms employ an ecology of accounting devices, such as “rankings, lists, classifications, stars and other symbols’ in order to effectively manage their operations and create value without the need for traditional forms of hierarchical control.” Many of these devices fall under the label of what is called algorithmic management, and were first developed by companies operating in the sharing economy or gig economy, functioning as effective labor and cost cutting measures. The Data&Society explainer of the term, for example, describes algorithmic management as ‘a diverse set of technological tools and techniques that structure the conditions of work and remotely manage workforces. Data&Society also provides a list of five typical features of algorithmic management:\n\nProlific data collection and surveillance of workers through technology;\nReal-time responsiveness to data that informs management decisions;\nAutomated or semi-automated decision-making;\nTransfer of performance evaluations to rating systems or other metrics; and\nThe use of “nudges” and penalties to indirectly incentivize worker behaviors.\nProponents of algorithmic management claim that it “creates new employment opportunities, better and cheaper consumer services, transparency and fairness in parts of the labour market that are characterised by inefficiency, opacity and capricious human bosses.” On the other hand, critics of algorithmic management claim that the practice leads to several issues, especially as it impacts the employment status of workers managed by its new array of tools and techniques.\n\n",
        "pageid": 67039572
    },
    "Algorithmic mechanism design": {
        "title": "Algorithmic mechanism design",
        "extract": "Algorithmic mechanism design (AMD) lies at the intersection of economic game theory, optimization, and computer science. The prototypical problem in mechanism design is to design a system for multiple self-interested participants, such that the participants' self-interested actions at equilibrium lead to good system performance. Typical objectives studied include revenue maximization and social welfare maximization. Algorithmic mechanism design differs from classical economic mechanism design in several respects. It typically employs the analytic tools of theoretical computer science, such as worst case analysis and approximation ratios, in contrast to classical mechanism design in economics which often makes distributional assumptions about the agents. It also considers computational constraints to be of central importance: mechanisms that cannot be efficiently implemented in polynomial time are not considered to be viable solutions to a mechanism design problem. This often, for example, rules out the classic economic mechanism, the Vickrey–Clarke–Groves auction.\n\n",
        "pageid": 15875500
    },
    "Algorithmic paradigm": {
        "title": "Algorithmic paradigm",
        "extract": "An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.",
        "pageid": 51411922
    },
    "Algorithmic transparency": {
        "title": "Algorithmic transparency",
        "extract": "Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\nThe phrases \"algorithmic transparency\" and \"algorithmic accountability\" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, \"algorithmic transparency\" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair.  \"Algorithmic accountability\" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.\nCurrent research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a \"right to explanation\" of decisions made by algorithms, though it is unclear what this means. Furthermore, the European Union founded The European Center for Algorithmic Transparency (ECAT).\n\n",
        "pageid": 52773150
    },
    "Algorithms and Combinatorics": {
        "title": "Algorithms and Combinatorics",
        "extract": "Algorithms and Combinatorics (ISSN 0937-5511) is a book series in mathematics, and particularly in combinatorics and the design and analysis of algorithms. It is published by Springer Science+Business Media, and was founded in 1987.",
        "pageid": 57504451
    },
    "Algorithms of Oppression": {
        "title": "Algorithms of Oppression",
        "extract": "Algorithms of Oppression: How Search Engines Reinforce Racism is a 2018 book by Safiya Umoja Noble in the fields of information science, machine learning, and human-computer interaction.\n\n",
        "pageid": 56463048
    },
    "AVT Statistical filtering algorithm": {
        "title": "AVT Statistical filtering algorithm",
        "extract": "AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources. It is most effective in cases when there is inband noise present. In those cases AVT is better at filtering data then, band-pass filter or any digital filtering based on variation of.\nConventional filtering is useful when signal/data has different frequency than noise and signal/data is separated/filtered by frequency discrimination of noise. Frequency discrimination filtering is done using Low Pass, High Pass and Band Pass filtering which refers to relative frequency filtering criteria target for such configuration. Those filters are created using passive and active components and sometimes are implemented using software algorithms based on Fast Fourier transform (FFT).\nAVT filtering is implemented in software and its inner working is based on statistical analysis of raw data.\nWhen signal frequency/(useful data distribution frequency) coincides with noise frequency/(noisy data distribution frequency) we have inband noise. In this situations frequency discrimination filtering does not work since the noise and useful signal are indistinguishable and where AVT excels. To achieve filtering in such conditions there are several methods/algorithms available which are briefly described below.",
        "pageid": 44995795
    },
    "Bartels–Stewart algorithm": {
        "title": "Bartels–Stewart algorithm",
        "extract": "In numerical linear algebra, the Bartels–Stewart algorithm is used to numerically solve the Sylvester matrix equation \n  \n    \n      \n        A\n        X\n        −\n        X\n        B\n        =\n        C\n      \n    \n    {\\displaystyle AX-XB=C}\n  \n. Developed by R.H. Bartels and G.W. Stewart in 1971, it was the first numerically stable method that could be systematically applied to solve such equations. The algorithm works by using the real Schur decompositions of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n to transform \n  \n    \n      \n        A\n        X\n        −\n        X\n        B\n        =\n        C\n      \n    \n    {\\displaystyle AX-XB=C}\n  \n into a triangular system that can then be solved using forward or backward substitution. In 1979, G. Golub, C. Van Loan and S. Nash introduced an improved version of the algorithm, known as the Hessenberg–Schur algorithm. It remains a standard approach for solving  Sylvester equations when \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is of small to moderate size.\n\n",
        "pageid": 58536963
    },
    "Behavior selection algorithm": {
        "title": "Behavior selection algorithm",
        "extract": "In artificial intelligence, a behavior selection algorithm, or action selection algorithm, is an algorithm that selects appropriate behaviors or actions for one or more intelligent agents. In game artificial intelligence, it selects behaviors or actions for one or more non-player characters. Common behavior selection algorithms include:\n\nFinite-state machines\nHierarchical finite-state machines\nDecision trees\nBehavior trees\nHierarchical task networks\nHierarchical control systems\nUtility systems\nDialogue tree (for selecting what to say)\n\n",
        "pageid": 47893974
    },
    "Berlekamp–Rabin algorithm": {
        "title": "Berlekamp–Rabin algorithm",
        "extract": "In number theory, Berlekamp's root finding algorithm, also called the Berlekamp–Rabin algorithm, is the probabilistic method of finding roots of polynomials over the field \n  \n    \n      \n        \n          \n            F\n          \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {F} _{p}}\n  \n with \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n elements. The method was discovered by Elwyn Berlekamp in 1970 as an auxiliary to the algorithm for polynomial factorization over finite fields. The algorithm was later modified by Rabin for arbitrary finite fields in 1979. The method was also independently discovered before Berlekamp by other researchers.",
        "pageid": 61379828
    },
    "Birkhoff algorithm": {
        "title": "Birkhoff algorithm",
        "extract": "Birkhoff's algorithm (also called Birkhoff-von-Neumann algorithm) is an algorithm for decomposing a bistochastic matrix into a convex combination of permutation matrices. It was published by Garrett Birkhoff in 1946.: 36  It has many applications. One such application is for the problem of fair random assignment: given a randomized allocation of items, Birkhoff's algorithm can decompose it into a lottery on deterministic allocations.\n\n",
        "pageid": 64692455
    },
    "Bisection (software engineering)": {
        "title": "Bisection (software engineering)",
        "extract": "Bisection is a method used in software development to identify change sets that result in a specific behavior change. It is mostly employed for finding the patch that introduced a bug. Another application area is finding the patch that indirectly fixed a bug.\n\n",
        "pageid": 36033877
    },
    "Block swap algorithms": {
        "title": "Block swap algorithms",
        "extract": "In computer algorithms, block swap algorithms swap two regions of elements of an array. It is simple to swap two non-overlapping regions of an array of equal size. However, it is not simple to swap two non-overlapping regions of an array in-place that are next to each other, but are of unequal sizes (such swapping is equivalent to array rotation). Three algorithms are known to accomplish this: Bentley's juggling (also known as dolphin algorithm ), Gries-Mills, and reversal algorithm. All three algorithms are linear time \nO(n), (see Time complexity).",
        "pageid": 61176336
    },
    "British Museum algorithm": {
        "title": "British Museum algorithm",
        "extract": "The British Museum algorithm is a general approach to finding a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities is enormous.\nNewell, Shaw, and Simon \ncalled this procedure the British Museum algorithm \n\n\"... since it seemed to them as sensible as placing monkeys in front of typewriters in order to reproduce all the books in the British Museum.\"",
        "pageid": 920295
    },
    "Broadcast (parallel pattern)": {
        "title": "Broadcast (parallel pattern)",
        "extract": "Broadcast is a collective communication primitive in parallel programming to distribute programming instructions or data to nodes in a cluster. It is the reverse operation of reduction. The broadcast operation is widely used in parallel algorithms, such as matrix-vector multiplication, Gaussian elimination and shortest paths.\nThe Message Passing Interface implements broadcast in MPI_Bcast.\n\n",
        "pageid": 60378307
    },
    "Car–Parrinello molecular dynamics": {
        "title": "Car–Parrinello molecular dynamics",
        "extract": "Car–Parrinello molecular dynamics or CPMD refers to either a method used in molecular dynamics (also known as the Car–Parrinello method) or the computational chemistry software package used to implement this method.\nThe CPMD method is one of the major methods for calculating ab-initio molecular dynamics (ab-initio MD or AIMD).\nAb initio molecular dynamics (ab initio MD) is a computational method that uses first principles, or fundamental laws of nature, to simulate the motion of atoms in a system. It is a type of molecular dynamics (MD) simulation that does not rely on empirical potentials or force fields to describe the interactions between atoms, but rather calculates these interactions directly from the electronic structure of the system using quantum mechanics.\nIn an ab initio MD simulation, the total energy of the system is calculated at each time step using density functional theory (DFT) or another method of quantum chemistry. The forces acting on each atom are then determined from the gradient of the energy with respect to the atomic coordinates, and the equations of motion are solved to predict the trajectory of the atoms.\nAIMD permits chemical bond breaking and forming events to occur and accounts for electronic polarization effect. Therefore, Ab initio MD simulations can be used to study a wide range of phenomena, including the structural, thermodynamic, and dynamic properties of materials and chemical reactions. They are particularly useful for systems that are not well described by empirical potentials or force fields, such as systems with strong electronic correlation or systems with many degrees of freedom. However, ab initio MD simulations are computationally demanding and require significant computational resources.\nThe CPMD method is related to the more common Born–Oppenheimer molecular dynamics (BOMD) method in that the quantum mechanical effect of the electrons is included in the calculation of energy and forces for the classical motion of the nuclei. CPMD and BOMD are different types of AIMD. However, whereas BOMD treats the electronic structure problem within the time-independent Schrödinger equation, CPMD explicitly includes the electrons as active degrees of freedom, via (fictitious) dynamical variables.\nThe software is a parallelized plane wave / pseudopotential implementation of density functional theory, particularly designed for ab initio molecular dynamics.",
        "pageid": 6770335
    },
    "Catalytic computing": {
        "title": "Catalytic computing",
        "extract": "Catalytic computing is a technique in computer science, relevant to complexity theory, that uses full memory, as well as empty memory space, to perform computations. Full memory is memory that begins in an arbitrary state and must be returned to that state at the end of the computation, for example important data. It can sometimes be used to reduce the memory needs of certain algorithms, for example the tree evaluation problem. It was defined by Buhrman, Cleve, Koucký, Loff, and Speelman in 2014 and was named after catalysts in chemistry, based on the metaphorically viewing the full memory as a \"catalyst\", a non-consumed factor critical for the computational \"reaction\" to succeed.\nThe complexity class CSPACE(s(n)) is the class of sets computable by catalytic Turing machines whose work tape is bounded by s(n) tape cells and whose auxiliary full memory space is bounded by \n  \n    \n      \n        \n          2\n          \n            s\n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle 2^{s(n)}}\n  \n tape cells. It has been shown that CSPACE(log(n)), or catalytic logspace, is contained within ZPP and, importantly, contains TC1.\n\n",
        "pageid": 79297557
    },
    "Certifying algorithm": {
        "title": "Certifying algorithm",
        "extract": "In theoretical computer science, a certifying algorithm is an algorithm that outputs, together with a solution to the problem it solves, a proof that the solution is correct. A certifying algorithm is said to be efficient if the combined runtime of the algorithm and a proof checker is slower by at most a constant factor than the best known non-certifying algorithm for the same problem.\nThe proof produced by a certifying algorithm should be in some sense simpler than the algorithm itself, for otherwise any algorithm could be considered certifying (with its output verified by running the same algorithm again). Sometimes this is formalized by requiring that a verification of the proof take less time than the original algorithm, while for other problems (in particular those for which the solution can be found in linear time) simplicity of the output proof is considered in a less formal sense. For instance, the validity of the output proof may be more apparent to human users than the correctness of the algorithm, or a checker for the proof may be more amenable to formal verification.\nImplementations of certifying algorithms that also include a checker for the proof generated by the algorithm may be considered to be more reliable than non-certifying algorithms. For, whenever the algorithm is run, one of three things happens: it produces a correct output (the desired case), it detects a bug in the algorithm or its implication (undesired, but generally preferable to continuing without detecting the bug), or both the algorithm and the checker are faulty in a way that masks the bug and prevents it from being detected (undesired, but unlikely as it depends on the existence of two independent bugs).\n\n",
        "pageid": 51386092
    },
    "Chandy–Misra–Haas algorithm resource model": {
        "title": "Chandy–Misra–Haas algorithm resource model",
        "extract": "The Chandy–Misra–Haas algorithm resource model checks for deadlock in a distributed system. It was developed by K. Mani Chandy, Jayadev Misra and Laura M Haas.",
        "pageid": 39093307
    },
    "Chinese whispers (clustering method)": {
        "title": "Chinese whispers (clustering method)",
        "extract": "Chinese whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.\nChinese whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can belong to only one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese whispers runs in linear time, which means that it is extremely fast even if there are very many nodes and links in the network.\n\n",
        "pageid": 46877898
    },
    "Collaborative diffusion": {
        "title": "Collaborative diffusion",
        "extract": "Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of antiobjects, objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.\nCollaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.\n\n",
        "pageid": 47341174
    },
    "Collective operation": {
        "title": "Collective operation",
        "extract": "Collective operations are building blocks for interaction patterns, that are often used in SPMD algorithms in the parallel programming context. Hence, there is an interest in efficient realizations of these operations.\nA realization of the collective operations is provided by the Message Passing Interface (MPI).\n\n",
        "pageid": 23515853
    },
    "Collision problem": {
        "title": "Collision problem",
        "extract": "The r-to-1 collision problem is an important theoretical problem in complexity theory, quantum computing, and computational mathematics. The collision problem most often refers to the 2-to-1 version: given \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n even and a function \n  \n    \n      \n        f\n        :\n        \n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n        →\n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle f:\\,\\{1,\\ldots ,n\\}\\rightarrow \\{1,\\ldots ,n\\}}\n  \n, we are promised that f is either 1-to-1 or 2-to-1. We are only allowed to make queries about the value of \n  \n    \n      \n        f\n        (\n        i\n        )\n      \n    \n    {\\displaystyle f(i)}\n  \n for any \n  \n    \n      \n        i\n        ∈\n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,\\ldots ,n\\}}\n  \n. The problem then asks how many such queries we need to make to determine with certainty whether f is 1-to-1 or 2-to-1.",
        "pageid": 11857532
    },
    "Communication-avoiding algorithm": {
        "title": "Communication-avoiding algorithm",
        "extract": "Communication-avoiding algorithms minimize movement of data within a memory hierarchy for improving its running-time and energy consumption. These minimize the total of two costs (in terms of time and energy): arithmetic and communication. Communication, in this context refers to moving data, either between levels of memory or between multiple processors over a network. It is much more expensive than arithmetic.\n\n",
        "pageid": 48786651
    },
    "Decrease-and-conquer": {
        "title": "Divide-and-conquer algorithm",
        "extract": "In computer science, divide and conquer is an algorithm design paradigm. A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\nThe divide-and-conquer technique is the basis of efficient algorithms for many problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g., the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).\nDesigning efficient divide-and-conquer algorithms can be difficult. As in mathematical induction, it is often necessary to generalize the problem to make it amenable to a recursive solution. The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.\n\n",
        "pageid": 201154
    },
    "Dependency network (graphical model)": {
        "title": "Dependency network (graphical model)",
        "extract": "Dependency networks (DNs) are graphical models, similar to Markov networks, wherein each vertex (node) corresponds to a random variable and each edge captures dependencies among variables.  \nUnlike Bayesian networks, DNs may contain cycles.   \nEach node is associated to a conditional probability table, which determines the realization of the random variable given its parents.",
        "pageid": 62838132
    },
    "Devex algorithm": {
        "title": "Devex algorithm",
        "extract": "In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Paula M. J. Harris. It identifies the steepest-edge approximately in its search for the optimal solution.",
        "pageid": 40129720
    },
    "Distributed tree search": {
        "title": "Distributed tree search",
        "extract": "Distributed tree search (DTS) algorithm is a class of algorithms for searching values in an efficient and distributed manner. Their purpose is to iterate through a tree by working along multiple branches in parallel and merging the results of each branch into one common solution, in order to minimize time spent searching for a value in a tree-like data structure.\nThe original paper was written in 1988 by Chris Ferguson and Richard E. Korf, from the University of California, Los Angeles Computer Science Department. They used multiple other chess AIs to develop this wider range algorithm.\n\n",
        "pageid": 50546680
    },
    "Divide-and-conquer algorithm": {
        "title": "Divide-and-conquer algorithm",
        "extract": "In computer science, divide and conquer is an algorithm design paradigm. A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\nThe divide-and-conquer technique is the basis of efficient algorithms for many problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g., the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).\nDesigning efficient divide-and-conquer algorithms can be difficult. As in mathematical induction, it is often necessary to generalize the problem to make it amenable to a recursive solution. The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.\n\n",
        "pageid": 201154
    },
    "Domain reduction algorithm": {
        "title": "Domain reduction algorithm",
        "extract": "Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.",
        "pageid": 45194398
    },
    "DONE": {
        "title": "DONE",
        "extract": "The Data-based Online Nonlinear Extremumseeker (DONE) algorithm is a black-box optimization algorithm.\nDONE models the unknown cost function and attempts to find an optimum of the underlying function.\nThe DONE algorithm is suitable for optimizing costly and noisy functions and does not require derivatives.\nAn advantage of DONE over similar algorithms, such as Bayesian optimization, is that the computational cost per iteration is independent of the number of function evaluations.\n\n",
        "pageid": 51017812
    },
    "Driver scheduling problem": {
        "title": "Driver scheduling problem",
        "extract": "The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.\nThe DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods, within the constraints of various legislative and logistical criteria.",
        "pageid": 39456471
    },
    "EdgeRank": {
        "title": "EdgeRank",
        "extract": "EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the EdgeRank system and uses a machine learning algorithm that, as of 2013, takes more than 100,000 factors into account.\nEdgeRank was developed and implemented by Serkan Piantino.\n\n",
        "pageid": 38090349
    },
    "Emergent algorithm": {
        "title": "Emergent algorithm",
        "extract": "An emergent algorithm is an algorithm that exhibits emergent behavior.  In essence an emergent algorithm implements a set of simple building block behaviors that when combined exhibit more complex behaviors.  One example of this is the implementation of fuzzy motion controllers used to adapt robot movement in response to environmental obstacles.\nAn emergent algorithm has the following characteristics: \n\nit achieves predictable global effects\nit does not require global visibility\nit does not assume any kind of centralized control\nit is self-stabilizing\nOther examples of emergent algorithms and models include cellular automata, artificial neural networks and swarm intelligence systems (ant colony optimization, bees algorithm, etc.).",
        "pageid": 214269
    },
    "Enumeration algorithm": {
        "title": "Enumeration algorithm",
        "extract": "In computer science, an enumeration algorithm is an algorithm that enumerates the answers to a computational problem. Formally, such an algorithm applies to problems that take an input and produce a list of solutions, similarly to function problems. For each input, the enumeration algorithm must produce the list of all solutions, without duplicates, and then halt. The performance of an enumeration algorithm is measured in terms of the time required to produce the solutions, either in terms of the total time required to produce all solutions, or in terms of the maximal delay between two consecutive solutions and in terms of a preprocessing time, counted as the time before outputting the first solution. This complexity can be expressed in terms of the size of the input, the size of each individual output, or the total size of the set of all outputs, similarly to what is done with output-sensitive algorithms.",
        "pageid": 60842845
    },
    "External memory algorithm": {
        "title": "External memory algorithm",
        "extract": "In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a computer's main memory at once. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives, or when memory is on a computer network. External memory algorithms are analyzed in the external memory model.",
        "pageid": 1881722
    },
    "Flajolet–Martin algorithm": {
        "title": "Flajolet–Martin algorithm",
        "extract": "The Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption logarithmic in the maximal number of possible distinct elements in the stream (the count-distinct problem). The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 article \"Probabilistic Counting Algorithms for Data Base Applications\". Later it has been refined in \"LogLog counting of large cardinalities\" by Marianne Durand and Philippe Flajolet, and \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\" by Philippe Flajolet et al.\nIn their 2010 article \"An optimal algorithm for the distinct elements problem\", Daniel M. Kane, Jelani Nelson and David P. Woodruff give an improved algorithm, which uses nearly optimal space and has optimal O(1) update and reporting times.\n\n",
        "pageid": 44308703
    },
    "Free Spaced Repetition Scheduler": {
        "title": "Spaced repetition",
        "extract": "Spaced repetition is an evidence-based learning technique that is usually performed with flashcards. Newly introduced and more difficult flashcards are shown more frequently, while older and less difficult flashcards are shown less frequently in order to exploit the psychological spacing effect. The use of spaced repetition has been proven to increase the rate of learning.\n\nAlthough the principle is useful in many contexts, spaced repetition is commonly applied in contexts in which a learner must acquire many items and retain them indefinitely in memory. It is, therefore, well suited for the problem of vocabulary acquisition in the course of second-language learning. A number of spaced repetition software programs have been developed to aid the learning process. It is also possible to perform spaced repetition with physical flashcards using the Leitner system. The testing effect and spaced repetition can be combined to improve long-term memory. Therefore, memorization can be easier to do.\n\n",
        "pageid": 27805
    },
    "Generalized distributive law": {
        "title": "Generalized distributive law",
        "extract": "The generalized distributive law (GDL) is a generalization of the distributive property which gives rise to a general message passing algorithm. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. The law and algorithm were introduced in a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.",
        "pageid": 35685954
    },
    "Gutmann method": {
        "title": "Gutmann method",
        "extract": "The Gutmann method is an algorithm for securely erasing the contents of computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper Secure Deletion of Data from Magnetic and Solid-State Memory in July 1996, it involved writing a series of 35 patterns over the region to be erased.\nThe selection of patterns assumes that the user does not know the encoding mechanism used by the drive, so it includes patterns designed specifically for three types of drives. A user who knows which type of encoding the drive uses can choose only those patterns intended for their drive. A drive with a different encoding mechanism would need different patterns.\nMost of the patterns in the Gutmann method were designed for older MFM/RLL encoded disks. Gutmann himself has noted that more modern drives no longer use these older encoding techniques, making parts of the method irrelevant. He said \"In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques\".\nSince about 2001, some ATA IDE and SATA hard drive manufacturer designs include support for the ATA Secure Erase standard, obviating the need to apply the Gutmann method when erasing an entire drive. The Gutmann method does not apply to USB sticks: a 2011 study reports that 71.7% of data remained available. On solid state drives it resulted in 0.8–4.3% recovery.",
        "pageid": 1773852
    },
    "HAKMEM": {
        "title": "HAKMEM",
        "extract": "HAKMEM, alternatively known as AI Memo 239, is a February 1972 \"memo\" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware – in Guy L. Steele's words, \"a bizarre and eclectic potpourri of technical trivia\".\nContributors included about two dozen members and associates of the AI Lab. The title of the report is short for \"hacks memo\", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).",
        "pageid": 505526
    },
    "Hall circles": {
        "title": "Hall circles",
        "extract": "Hall circles (also known as M-circles and N-circles) are a graphical tool in control theory used to obtain values of a closed-loop transfer function from the Nyquist plot (or the Nichols plot) of the associated open-loop transfer function. Hall circles have been introduced in control theory by Albert C. Hall in his thesis.",
        "pageid": 57506816
    },
    "Higuchi dimension": {
        "title": "Higuchi dimension",
        "extract": "In fractal geometry, the Higuchi dimension (or Higuchi fractal dimension (HFD)) is an approximate value for the box-counting dimension of the graph of a real-valued function or time series. This value is obtained via an algorithmic approximation so one also talks about the Higuchi method. It has many applications in science and engineering and has been applied to subjects like characterizing primary waves in seismograms, clinical neurophysiology and analyzing changes in the electroencephalogram in Alzheimer's disease.\n\n",
        "pageid": 64202283
    },
    "Hindley–Milner type system": {
        "title": "Hindley–Milner type system",
        "extract": "A Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.\nAmong HM's more notable properties are its completeness and its ability to infer the most general type of a given program without programmer-supplied type annotations or other hints. Algorithm W is an efficient type inference method in practice and has been successfully applied on large code bases, although it has a high theoretical complexity. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell.",
        "pageid": 32612385
    },
    "Holographic algorithm": {
        "title": "Holographic algorithm",
        "extract": "In computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them holographic because \"their effect can be viewed as that of producing interference patterns among the solution fragments\". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.\nHolographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory.  Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.\nHolographic algorithms have some similarities with quantum computation, but are completely classical.",
        "pageid": 14609233
    },
    "How to Solve it by Computer": {
        "title": "How to Solve it by Computer",
        "extract": "How to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.\nIt is occasionally used as a textbook, especially in India.\nIt is an introduction to the whys of algorithms and data structures.\nFeatures of the book:\n\nThe design factors associated with problems\nThe creative process behind coming up with innovative solutions for algorithms and data structures\nThe line of reasoning behind the constraints, factors and the design choices made.\nThe very fundamental algorithms portrayed by this book are mostly presented in pseudocode and/or Pascal notation.",
        "pageid": 4104986
    },
    "Hybrid algorithm": {
        "title": "Hybrid algorithm",
        "extract": "A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one based on some characteristic of the data, or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.\n\"Hybrid algorithm\" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.",
        "pageid": 40338559
    },
    "Hyphenation algorithm": {
        "title": "Syllabification",
        "extract": "Syllabification () or syllabication (), also known as hyphenation, is the separation of a word into syllables, whether spoken, written or signed.\n\n",
        "pageid": 1393831
    },
    "In-place algorithm": {
        "title": "In-place algorithm",
        "extract": "In computer science, an in-place algorithm is an algorithm that operates directly on the input data structure without requiring extra space proportional to the input size. In other words, it modifies the input in place, without creating a separate copy of the data structure. An algorithm which is not in-place is sometimes called not-in-place or out-of-place.\nIn-place can have slightly different meanings. In its strictest form, the algorithm can only have a constant amount of extra space, counting everything including function calls and pointers. However, this form is very limited as simply having an index to a length n array requires O(log n) bits. More broadly, in-place means that the algorithm does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log n), though sometimes anything in o(n) is allowed. Note that space complexity also has varied choices in whether or not to count the index lengths as part of the space used. Often, the space complexity is given in terms of the number of indices or pointers needed, ignoring their length. In this article, we refer to total space complexity (DSPACE), counting pointer lengths. Therefore, the space requirements here have an extra log n factor compared to an analysis that ignores the lengths of indices and pointers.  \nAn algorithm may or may not count the output as part of its space usage. Since in-place algorithms usually overwrite their input with output, no additional space is needed. When writing the output to write-only memory or a stream, it may be more appropriate to only consider the working space of the algorithm. In theoretical applications such as log-space reductions, it is more typical to always ignore output space (in these cases it is more essential that the output is write-only).\n\n",
        "pageid": 219861
    },
    "Irish logarithm": {
        "title": "Irish logarithm",
        "extract": "The Irish logarithm was a system of number manipulation invented by Percy Ludgate for machine multiplication. The system used a combination of mechanical cams as lookup tables and mechanical addition to sum pseudo-logarithmic indices to produce partial products, which were then added to produce results.\nThe technique is similar to Zech logarithms (also known as Jacobi logarithms), but uses a system of indices original to Ludgate.",
        "pageid": 31818344
    },
    "Iteration": {
        "title": "Iteration",
        "extract": "Iteration is the repetition of a process in order to generate a (possibly unbounded) sequence of outcomes. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration. \nIn mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.\n\n",
        "pageid": 68833
    },
    "Jumble algorithm": {
        "title": "Jumble",
        "extract": "Jumble is a word puzzle with a clue, a drawing illustrating the clue, and a set of words, each of which is “jumbled” by scrambling its letters. A solver reconstructs the words, and then arranges letters at marked positions in the words to spell the answer phrase to the clue.  The clue, and sometimes the illustration, provide hints about the answer phrase, which frequently uses a homophone or pun.\nJumble was created in 1954 by Martin Naydel, who was better known for his work on comic books. It originally appeared under the title \"Scramble.\" Henri Arnold and Bob Lee took over the feature in 1962 and continued it for at least 30 years. As of 2013, Jumble was being maintained by David L. Hoyt and Jeff Knurek. Jumble is one of the most valuable properties of its distributor, US company Tribune Content Agency, which owns the JUMBLE trademarks and copyrights. Daily and Sunday Jumble puzzles appear in over 600 newspapers in the United States and internationally.\nThe current syndicated version found in most daily newspapers (under the official title Jumble--That Scrambled Word Game) has four base anagrams, two of five letters and two of six, followed by a clue and a series of blank spaces into which the answer to the clue fits. The answer to the clue is generally a pun of some sort. A weekly \"kids version\" of the puzzle features a three-letter word plus three four-letter words. In order to find the letters that are in the answer to the given clue, the player must unscramble all four of the scrambled words; the letters that are in the clue will be circled. The contestant then unscrambles the circled letters to form the answer to the clue. An alternate workaround is to solve some of the scrambled words, figure out the answer to the clue without all the letters, then use the \"extra\" letters as aids to solve the remaining scrambled words.\nThere are many variations of puzzles from the Jumble brand including Jumble, Jumble for Kids, Jumble Crosswords, TV Jumble, Jumble BrainBusters, Jumble BrainBusters Junior, Hollywood Jumble, Jumble Jong, Jumble Word Vault, Jumpin' Jumble, Jumble Solitaire, and Jumble Word Web.",
        "pageid": 1173921
    },
    "Jump-and-Walk algorithm": {
        "title": "Jump-and-Walk algorithm",
        "extract": "Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.\nJump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).\nJump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.",
        "pageid": 13830115
    },
    "Kinodynamic planning": {
        "title": "Kinodynamic planning",
        "extract": "In robotics and motion planning, kinodynamic planning is a class of problems for which velocity,  acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles.  The term was coined by Bruce Donald, Pat Xavier, John Canny, and John Reif. Donald et al. developed the first polynomial-time approximation schemes (PTAS) for the problem. By providing a  provably polynomial-time ε-approximation algorithm, they resolved a long-standing open problem in optimal control. Their first paper considered time-optimal control  (\"fastest path\") of a point mass under Newtonian dynamics, amidst polygonal (2D) or polyhedral  (3D) obstacles, subject to  state  bounds on position, velocity, and acceleration. Later they extended the technique to many other cases, for example, to 3D open-chain kinematic robots under full Lagrangian dynamics.\n More recently, many  practical heuristic algorithms  based on stochastic optimization and iterative sampling were developed, by a wide range of authors, to address the kinodynamic planning problem. These techniques for kinodynamic planning have been shown to work well in practice. However,  none of these heuristic techniques can guarantee the optimality of the computed solution (i.e., they have no performance guarantees), and  none can be mathematically proven to be faster than the original PTAS algorithms (i.e., none have a provably lower computational complexity).\n\n",
        "pageid": 19759995
    },
    "KiSAO": {
        "title": "KiSAO",
        "extract": "The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.\n\n",
        "pageid": 42563034
    },
    "Kleene's algorithm": {
        "title": "Kleene's algorithm",
        "extract": "In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given nondeterministic finite automaton (NFA) into a regular expression. \nTogether with other conversion algorithms, it establishes the equivalence of several description formats for regular languages. Alternative presentations of the same method include the \"elimination method\" attributed to Brzozowski and McCluskey, the algorithm of McNaughton and Yamada, and the use of Arden's lemma.",
        "pageid": 42923391
    },
    "Knuth–Plass line-breaking algorithm": {
        "title": "Knuth–Plass line-breaking algorithm",
        "extract": "The Knuth–Plass algorithm is a line-breaking algorithm designed for use in Donald Knuth's typesetting program TeX. It integrates the problems of text justification and hyphenation into a single algorithm by using a discrete dynamic programming method to minimize a loss function that attempts to quantify the aesthetic qualities desired in the finished output.\nThe algorithm works by dividing the text into a stream of three kinds of objects: boxes, which are non-resizable chunks of content, glue, which are flexible, resizeable elements, and penalties, which represent places where breaking is undesirable (or, if negative, desirable). The loss function, known as \"badness\", is defined in terms of the deformation of the glue elements, and any extra penalties incurred through line breaking.\nMaking hyphenation decisions follows naturally from the algorithm, but the choice of possible hyphenation points within words, and optionally their preference weighting, must be performed first, and that information inserted into the text stream in advance. Knuth and Plass' original algorithm does not include page breaking, but may be modified to interface with a pagination algorithm, such as the algorithm designed by Plass in his PhD thesis.\nTypically, the cost function for this technique should be modified so that it does not count the space left on the final line of a paragraph; this modification allows a paragraph to end in the middle of a line without penalty. The same technique can also be extended to take into account other factors such as the number of lines or costs for hyphenating long words.",
        "pageid": 76478062
    },
    "Krauss wildcard-matching algorithm": {
        "title": "Krauss wildcard-matching algorithm",
        "extract": "In computer science, the Krauss wildcard-matching algorithm is a pattern matching algorithm. Based on the wildcard syntax in common use, e.g. in the Microsoft Windows command-line interface, the algorithm provides a non-recursive mechanism for matching patterns in software applications, based on syntax simpler than that typically offered by regular expressions.",
        "pageid": 57373227
    },
    "Kunerth's algorithm": {
        "title": "Kunerth's algorithm",
        "extract": "Kunerth's algorithm is an algorithm for computing the modular square root of a given number.\nThe algorithm does not require the factorization of the modulus, and uses modular operations that are often easy when the given number is prime.",
        "pageid": 69299670
    },
    "Kunstweg": {
        "title": "Kunstweg",
        "extract": "Bürgi's Kunstweg is a set of algorithms invented by Jost Bürgi at the end of the 16th century. They can be used for the calculation of sines to an arbitrary precision. Bürgi used these algorithms to calculate a Canon Sinuum, a table of sines in steps of 2 arc seconds. It is thought that this table had 8 sexagesimal places. Some authors have speculated that this table only covered the range from 0 to 45 degrees, but nothing seems to support this claim. Such tables were extremely important for navigation at sea. Johannes Kepler called the Canon Sinuum the most precise known table of sines. Bürgi explained his algorithms in his work Fundamentum Astronomiae which he presented to Emperor Rudolf II. in 1592.\nThe principles of iterative sine table calculation through the Kunstweg are as follows: cells in a column sum up the values of the two previous cells in the same column. The final cell's value is divided by two, and the next iteration starts. Finally, the values of the last column get normalized. Rather accurate approximations of sines are obtained after few iterations.\nAs recently as 2015, Folkerts et al. showed that this simple process converges indeed towards the true sines. According to Folkerts et al., this was the first step towards difference calculus.\n\n",
        "pageid": 49589765
    },
    "Lamé's theorem": {
        "title": "Lamé's theorem",
        "extract": "Lamé's Theorem is the result of Gabriel Lamé's analysis of the complexity of the Euclidean algorithm. Using Fibonacci numbers, he proved in 1844 that when looking for the greatest common divisor (GCD) of two integers a and b, the algorithm finishes in at most 5k steps, where k is the number of digits (decimal) of b.",
        "pageid": 73761875
    },
    "Lancichinetti–Fortunato–Radicchi benchmark": {
        "title": "Lancichinetti–Fortunato–Radicchi benchmark",
        "extract": "Lancichinetti–Fortunato–Radicchi benchmark is an algorithm that generates benchmark networks (artificial networks that resemble real-world networks). They have a priori known communities and are used to compare different community detection methods.  The advantage of the benchmark over other methods is that it accounts for the heterogeneity in the distributions of node degrees and of community sizes.\n\n",
        "pageid": 46902242
    },
    "Learning augmented algorithm": {
        "title": "Learning augmented algorithm",
        "extract": "A learning augmented algorithm is an algorithm that can make use of a prediction to improve its performance.\nWhereas in regular algorithms just the problem instance is inputted, learning augmented algorithms accept an extra parameter.\nThis extra parameter often is a prediction of some property of the solution.\nThis prediction is then used by the algorithm to improve its running time or the quality of its output.",
        "pageid": 70856028
    },
    "Least-squares spectral analysis": {
        "title": "Least-squares spectral analysis",
        "extract": "Least-squares spectral analysis (LSSA) is a method of estimating a frequency spectrum based on a least-squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in the long and gapped records; LSSA mitigates such problems. Unlike in Fourier analysis, data need not be equally spaced to use LSSA.\nDeveloped in 1969 and 1971, LSSA is also known as the Vaníček method and the Gauss-Vaniček method after Petr Vaníček, and as the Lomb method or the Lomb–Scargle periodogram, based on the simplifications first by Nicholas R. Lomb and then by Jeffrey D. Scargle.",
        "pageid": 13609399
    },
    "Leiden algorithm": {
        "title": "Leiden algorithm",
        "extract": "The Leiden algorithm is a community detection algorithm developed by Traag et al \n at Leiden University. It was developed as a modification of the \nLouvain method. Like the Louvain method, the Leiden algorithm attempts to optimize modularity in extracting communities from networks; however, it addresses key issues present in the Louvain method, namely poorly connected communities and the resolution limit of modularity.",
        "pageid": 76155381
    },
    "Lion algorithm": {
        "title": "Lion algorithm",
        "extract": "Lion algorithm (LA) is one among the bio-inspired (or) nature-inspired optimization algorithms (or) that are mainly based on meta-heuristic principles. It was first introduced by B. R. Rajakumar in 2012 in the name, Lion’s Algorithm.. It was further extended in 2014 to solve the system identification problem. This version was referred as LA, which has been applied by many researchers for their optimization problems.",
        "pageid": 65467021
    },
    "List of cryptosystems": {
        "title": "List of cryptosystems",
        "extract": "A cryptosystem is a set of cryptographic algorithms that map ciphertexts and plaintexts to each other.\n\n",
        "pageid": 69167071
    },
    "Long division": {
        "title": "Long division",
        "extract": "In arithmetic, long division is a standard division algorithm suitable for dividing multi-digit Hindu-Arabic numerals (positional notation) that is simple enough to perform by hand. It breaks down a division problem into a series of easier steps.\nAs in all division problems, one number, called the dividend, is divided by another, called the divisor,  producing a result called the quotient. It enables computations involving arbitrarily large numbers to be performed by following a series of simple steps. The abbreviated form of long division is called short division, which is almost always used instead of long division when the divisor has only one digit.\n\n",
        "pageid": 313384
    },
    "Magic state distillation": {
        "title": "Magic state distillation",
        "extract": "Magic state distillation is a method for creating more accurate quantum states from multiple noisy ones, which is important for building fault tolerant quantum computers. It has also been linked to quantum contextuality, a concept thought to contribute to quantum computers' power.\nThe technique was first proposed by Emanuel Knill in 2004,\nand further analyzed by Sergey Bravyi and Alexei Kitaev the same year.\nThanks to the Gottesman–Knill theorem, it is known that some quantum operations (operations in the Clifford group) can be perfectly simulated in polynomial time on a classical computer. In order to achieve universal quantum computation, a quantum computer must be able to perform operations outside this set. Magic state distillation achieves this, in principle, by concentrating the usefulness of imperfect resources, represented by mixed states, into states that are conducive for performing operations that are difficult to simulate classically.\nA variety of qubit magic state distillation routines and distillation routines for qubits with various advantages have been proposed.",
        "pageid": 61982153
    },
    "Plotting algorithms for the Mandelbrot set": {
        "title": "Plotting algorithms for the Mandelbrot set",
        "extract": "There are many programs and algorithms used to plot the Mandelbrot set and other fractals, some of which are described in fractal-generating software. These programs use a variety of algorithms to determine the color of individual pixels efficiently.",
        "pageid": 63087276
    },
    "Manhattan address algorithm": {
        "title": "Manhattan address algorithm",
        "extract": "The Manhattan address algorithm is a series of formulas used to estimate the closest east–west cross street for building numbers on north–south avenues in the New York City borough of Manhattan.",
        "pageid": 26784161
    },
    "The Master Algorithm": {
        "title": "The Master Algorithm",
        "extract": "The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\n\n",
        "pageid": 47937215
    },
    "Maze generation algorithm": {
        "title": "Maze generation algorithm",
        "extract": "Maze generation algorithms are automated methods for the creation of mazes.\n\n",
        "pageid": 200877
    },
    "Maze-solving algorithm": {
        "title": "Maze-solving algorithm",
        "extract": "A maze-solving algorithm is an automated method for solving a maze. The random mouse, wall follower, Pledge, and Trémaux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.\nMazes containing no loops are known as \"simply connected\", or \"perfect\" mazes, and are equivalent to a tree in graph theory.  Maze-solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.\n\n",
        "pageid": 22074859
    },
    "Medical algorithm": {
        "title": "Medical algorithm",
        "extract": "A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare.  Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty. A medical prescription is also a type of medical algorithm.",
        "pageid": 1551981
    },
    "Miller's recurrence algorithm": {
        "title": "Miller's recurrence algorithm",
        "extract": "Miller's recurrence algorithm is a procedure for the backward calculation of a rapidly decreasing solution of a three-term recurrence relation developed by J. C. P. Miller. It was originally developed to compute tables of the modified Bessel function but also applies to Bessel functions of the first kind and has other applications such as computation of the coefficients of  Chebyshev expansions of other special functions.\nMany families of special functions satisfy a recurrence relation that relates the values of the functions of different orders with common argument \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\nThe modified Bessel functions of the first kind \n  \n    \n      \n        \n          I\n          \n            n\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle I_{n}(x)}\n  \n satisfy the recurrence relation\n\n  \n    \n      \n        \n          I\n          \n            n\n            −\n            1\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              2\n              n\n            \n            x\n          \n        \n        \n          I\n          \n            n\n          \n        \n        (\n        x\n        )\n        +\n        \n          I\n          \n            n\n            +\n            1\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle I_{n-1}(x)={\\frac {2n}{x}}I_{n}(x)+I_{n+1}(x)}\n  \n.\nHowever, the modified Bessel functions of the second kind \n  \n    \n      \n        \n          K\n          \n            n\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle K_{n}(x)}\n  \n also satisfy the same recurrence relation\n\n  \n    \n      \n        \n          K\n          \n            n\n            −\n            1\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              2\n              n\n            \n            x\n          \n        \n        \n          K\n          \n            n\n          \n        \n        (\n        x\n        )\n        +\n        \n          K\n          \n            n\n            +\n            1\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle K_{n-1}(x)={\\frac {2n}{x}}K_{n}(x)+K_{n+1}(x)}\n  \n.\nThe first solution decreases rapidly with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n.  The second solution increases rapidly with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n.  Miller's algorithm provides a numerically stable procedure to obtain the decreasing solution.\nTo compute the terms of a recurrence \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle a_{0}}\n  \n through \n  \n    \n      \n        \n          a\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle a_{N}}\n  \n according to Miller's algorithm, one first chooses a value \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n much larger than \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and computes a trial solution taking initial condition\n  \n    \n      \n        \n          a\n          \n            M\n          \n        \n      \n    \n    {\\displaystyle a_{M}}\n  \n to an arbitrary non-zero value (such as 1) and taking \n  \n    \n      \n        \n          a\n          \n            M\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{M+1}}\n  \n and later terms to be zero.  Then the recurrence relation is used to successively compute trial values for \n  \n    \n      \n        \n          a\n          \n            M\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{M-1}}\n  \n, \n  \n    \n      \n        \n          a\n          \n            M\n            −\n            2\n          \n        \n      \n    \n    {\\displaystyle a_{M-2}}\n  \n down to \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle a_{0}}\n  \n. Noting that a second sequence obtained from the trial sequence by multiplication by a constant normalizing factor will still satisfy the same recurrence relation, one can then apply a separate normalizing relationship to determine the normalizing factor that yields the actual solution.\nIn the example of the modified Bessel functions, a suitable normalizing relation is a summation involving the even terms of the recurrence: \n\n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        (\n        x\n        )\n        +\n        2\n        \n          ∑\n          \n            m\n            =\n            1\n          \n          \n            ∞\n          \n        \n        (\n        −\n        1\n        \n          )\n          \n            m\n          \n        \n        \n          I\n          \n            2\n            m\n          \n        \n        (\n        x\n        )\n        =\n        1\n      \n    \n    {\\displaystyle I_{0}(x)+2\\sum _{m=1}^{\\infty }(-1)^{m}I_{2m}(x)=1}\n  \n\nwhere the infinite summation becomes finite due to the approximation that \n  \n    \n      \n        \n          a\n          \n            M\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{M+1}}\n  \n and later terms are zero.\nFinally, it is confirmed that the approximation error of the procedure is acceptable by repeating the procedure with a second choice of \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n larger than the initial choice and confirming that the second set of results for \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle a_{0}}\n  \n through \n  \n    \n      \n        \n          a\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle a_{N}}\n  \n agree within the first set within the desired tolerance. Note that to obtain this agreement, the value of \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n must be large enough such that the term \n  \n    \n      \n        \n          a\n          \n            M\n          \n        \n      \n    \n    {\\displaystyle a_{M}}\n  \n is small compared to the desired tolerance.\nIn contrast to Miller's algorithm, attempts to apply the recurrence relation in the forward direction starting from known values of \n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle I_{0}(x)}\n  \n and \n  \n    \n      \n        \n          I\n          \n            1\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle I_{1}(x)}\n  \n obtained by other methods will fail as rounding errors introduce components of the rapidly increasing solution.\nOlver and Gautschi analyses the error propagation of the algorithm in detail.\nFor Bessel functions of the first kind, the equivalent recurrence relation and normalizing relationship are:\n\n  \n    \n      \n        \n          J\n          \n            n\n            −\n            1\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              2\n              n\n            \n            x\n          \n        \n        \n          J\n          \n            n\n          \n        \n        (\n        x\n        )\n        −\n        \n          J\n          \n            n\n            +\n            1\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle J_{n-1}(x)={\\frac {2n}{x}}J_{n}(x)-J_{n+1}(x)}\n  \n\n  \n    \n      \n        \n          J\n          \n            0\n          \n        \n        (\n        x\n        )\n        +\n        2\n        \n          ∑\n          \n            m\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          J\n          \n            2\n            m\n          \n        \n        (\n        x\n        )\n        =\n        1\n      \n    \n    {\\displaystyle J_{0}(x)+2\\sum _{m=1}^{\\infty }J_{2m}(x)=1}\n  \n.\nThe algorithm is particularly efficient in applications that require the values of the Bessel functions for all orders \n  \n    \n      \n        0\n        ⋯\n        N\n      \n    \n    {\\displaystyle 0\\cdots N}\n  \n for each value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n compared to direct independent computations of \n  \n    \n      \n        N\n        +\n        1\n      \n    \n    {\\displaystyle N+1}\n  \n separate functions.",
        "pageid": 60034541
    },
    "Multiplicative weight update method": {
        "title": "Multiplicative weight update method",
        "extract": "The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.",
        "pageid": 52242050
    },
    "Neural style transfer": {
        "title": "Neural style transfer",
        "extract": "Neural style transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, in order to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks for the sake of image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user-supplied photographs.  Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma. This method has been used by artists and designers around the globe to develop new artwork based on existent style(s).\n\n",
        "pageid": 59892172
    },
    "Newest vertex bisection": {
        "title": "Newest vertex bisection",
        "extract": "Newest Vertex Bisection is an algorithmic method to locally refine triangulations. It is widely used in computational science, numerical simulation, and computer graphics. The advantage of newest vertex bisection is that it allows local refinement of triangulations without degenerating the shape of the triangles after repeated usage.\nIn newest vertex bisection, whenever a triangle is to be split into smaller triangles, it will be bisected by drawing a line from the newest vertex to the midpoint of the edge opposite to that vertex. That midpoint becomes the newest vertex of the two newer triangles. One can show that repeating this procedure for a given triangulation leads to triangles that belong to only a finite number of similarity classes.\nGeneralizations of newest vertex bisection to dimension three and higher are known. Newest vertex bisection is used in local mesh refinement for adaptive finite element methods, where it is an alternative to red-green refinement and uniform mesh refinement.",
        "pageid": 59538271
    },
    "Newman–Janis algorithm": {
        "title": "Newman–Janis algorithm",
        "extract": "In general relativity, the Newman–Janis algorithm (NJA) is a complexification technique for finding exact solutions to the Einstein field equations.  In 1964, Newman and Janis showed that the Kerr metric could be obtained from the Schwarzschild metric by means of a coordinate transformation and allowing the radial coordinate to take on complex values.  Originally, no clear reason for why the algorithm works was known.\nIn 1998, Drake and Szekeres gave a detailed explanation of the success of the algorithm and proved the uniqueness of certain solutions.  In particular, the only perfect fluid solution generated by NJA is the Kerr metric and the only Petrov type D solution is the Kerr–Newman metric.\nThe algorithm works well on ƒ(R) and Einstein–Maxwell–Dilaton theories, but doesn't return expected results on Braneworld and Born–Infield theories.",
        "pageid": 63151790
    },
    "Non-malleable code": {
        "title": "Non-malleable code",
        "extract": "The notion of non-malleable codes was introduced in 2009 by Dziembowski, Pietrzak, and Wichs, for relaxing the notion of error-correction and error-detection. Informally, a code is non-malleable if the message contained in a modified code-word is either the original message, or a completely unrelated value. Non-malleable codes provide a useful and meaningful security guarantee in situations where traditional error-correction and error-detection is impossible; for example, when the attacker can completely overwrite the encoded message. Although such codes do not exist if the family of \"tampering functions\" F is completely unrestricted, they are known to exist for many broad tampering families F.",
        "pageid": 48768665
    },
    "Online optimization": {
        "title": "Online optimization",
        "extract": "Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). These kind of problems are denoted as online problems and are seen as opposed to the classical optimization problems where complete information is assumed (offline). The research on online optimization can be distinguished into online problems where multiple decisions are made sequentially based on a piece-by-piece input and those where a decision is made only once. A famous online problem where a decision is made only once is the Ski rental problem. In general, the output of an online algorithm is compared to the solution of a corresponding offline algorithm which is necessarily always optimal and knows the entire input in advance (competitive analysis).\nIn many situations, present decisions (for example, resources allocation) must be made with incomplete knowledge of the future or distributional assumptions on the future are not reliable. In such cases, online optimization  can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes.",
        "pageid": 49914674
    },
    "Pan–Tompkins algorithm": {
        "title": "Pan–Tompkins algorithm",
        "extract": "The Pan–Tompkins algorithm is commonly used to detect QRS complexes in electrocardiographic signals (ECG). The QRS complex represents the ventricular depolarization and the main spike visible in an ECG signal (see figure). This feature makes it particularly suitable for measuring heart rate, the first way to assess the heart health state. In the first derivation of Einthoven of a physiological heart, the QRS complex is composed by a downward deflection (Q wave), a high upward deflection (R wave) and a final downward deflection (S wave).\nThe Pan–Tompkins algorithm applies a series of filters to highlight the frequency content of this rapid heart depolarization and removes the background noise. Then, it squares the signal to amplify the QRS contribution, which makes identifying the QRS complex more straightforward. Finally, it applies adaptive thresholds to detect the peaks of the filtered signal. The algorithm was proposed by Jiapu Pan and Willis J. Tompkins in 1985, in the journal IEEE Transactions on Biomedical Engineering. The performance of the method was tested on an annotated arrhythmia database (MIT/BIH) and evaluated also in presence of noise. Pan and Tompkins reported that the 99.3 percent of QRS complexes was correctly detected.",
        "pageid": 61186810
    },
    "Parallel external memory": {
        "title": "Parallel external memory",
        "extract": "In computer science, a parallel external memory (PEM) model is a cache-aware, external-memory abstract machine. It is the parallel-computing analogy to the single-processor external memory (EM) model. In a similar way, it is the cache-aware analogy to the parallel random-access machine (PRAM). The PEM model consists of a number of processors, together with their respective private caches and a shared main memory.\n\n",
        "pageid": 59730114
    },
    "Parameterized approximation algorithm": {
        "title": "Parameterized approximation algorithm",
        "extract": "A parameterized approximation algorithm is a type of algorithm that aims to find approximate solutions to NP-hard optimization problems in polynomial time in the input size and a function of a specific parameter. These algorithms are designed to combine the best aspects of both traditional approximation algorithms and fixed-parameter tractability.\nIn traditional approximation algorithms, the goal is to find solutions that are at most a certain factor α away from the optimal solution, known as an α-approximation, in polynomial time. On the other hand, parameterized algorithms are designed to find exact solutions to problems, but with the constraint that the running time of the algorithm is polynomial in the input size and a function of a specific parameter k. The parameter describes some property of the input and is small in typical applications. The problem is said to be fixed-parameter tractable (FPT) if there is an algorithm that can find the optimum solution in \n  \n    \n      \n        f\n        (\n        k\n        )\n        \n          n\n          \n            O\n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle f(k)n^{O(1)}}\n  \n time, where \n  \n    \n      \n        f\n        (\n        k\n        )\n      \n    \n    {\\displaystyle f(k)}\n  \n is a function independent of the input size n.\nA parameterized approximation algorithm aims to find a balance between these two approaches by finding approximate solutions in FPT time: the algorithm computes an α-approximation in \n  \n    \n      \n        f\n        (\n        k\n        )\n        \n          n\n          \n            O\n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle f(k)n^{O(1)}}\n  \n time, where \n  \n    \n      \n        f\n        (\n        k\n        )\n      \n    \n    {\\displaystyle f(k)}\n  \n is a function independent of the input size n. This approach aims to overcome the limitations of both traditional approaches by having stronger guarantees on the solution quality compared to traditional approximations while still having efficient running times as in FPT algorithms. An overview of the research area studying parameterized approximation algorithms can be found in the survey of Marx and the more recent survey by Feldmann et al.",
        "pageid": 72808068
    },
    "PHY-Level Collision Avoidance": {
        "title": "PHY-Level Collision Avoidance",
        "extract": "PHY-Level Collision Avoidance (PLCA) is a component of the Ethernet reconciliation sublayer (between the PHY and the MAC) defined within IEEE 802.3 clause 148. The purpose of PLCA is to avoid the shared medium collisions and associated retransmission overhead. PLCA is used in 802.3cg (10BASE-T1), which focuses on bringing Ethernet connectivity to short-haul embedded internet of things and low throughput, noise-tolerant, industrial deployment use cases.\nIn order for a multidrop 10BASE-T1S standard to successfully compete with CAN XL, some kind of arbitration was necessary. The linear arbitration scheme of PLCA somewhat resembles the one of the Byteflight, but PLCA was designed from scratch to accommodate the existing shared medium Ethernet MACs with their busy sensing mechanisms.",
        "pageid": 71578738
    },
    "Ping-pong scheme": {
        "title": "Ping-pong scheme",
        "extract": "Algorithms said to employ a ping-pong scheme exist in different fields of software engineering. They are characterized by an alternation between two entities. In the examples described below, these entities are communication partners, network paths or file blocks.",
        "pageid": 12242679
    },
    "Pointer jumping": {
        "title": "Pointer jumping",
        "extract": "Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. Pointer jumping allows an algorithm to follow paths with a time complexity that is logarithmic with respect to the length of the longest path. It does this by \"jumping\" to the end of the path computed by neighbors.\nThe basic operation of pointer jumping is to replace each neighbor in a pointer structure with its neighbor's neighbor. In each step of the algorithm, this replacement is done for all nodes in the data structure, which can be done independently in parallel. In the next step when a neighbor's neighbor is followed, the neighbor's path already followed in the previous step is added to the node's followed path in a single step. Thus, each step effectively doubles the distance traversed by the explored paths.\nPointer jumping is best understood by looking at simple examples such as list ranking and root finding.",
        "pageid": 41026219
    },
    "Pol.is": {
        "title": "Pol.is",
        "extract": "Polis (or Pol.is) is wiki survey software designed for large group collaborations. An example of a civic technology, Polis allows people to share their opinions and ideas, and its algorithm is intended to elevate ideas that can facilitate better decision-making, especially when there are lots of participants. \nPolis has been credited for assisting the passage of legislation in Taiwan. Pol.is has also been used in America, Canada, Singapore, Philippines, Spain and other governments around the world.\nPol.is was founded by Colin Megill, Christopher Small, and Michael Bjorkegren after the Occupy Wall Street and Arab Spring movements.\nIn Taiwan, pol.is has been \"one of the key parts\" of vTaiwan's suite of open-source tools for its citizen engagement efforts arising out of the Sunflower Student Movement. vTaiwan claims that of the 26 national issues related to technology were discussed on the platform and 80% led to government action. Pol.is is also utilized by \"Join,\" a national platform for online deliberation run by the Taiwanese government. Megill credits Audrey Tang and CL Kao, a cofounder of g0v, with convincing him to open-source pol.is.\nIn 2023, Megill advised OpenAI on how to facilitate deliberation at scale in a way that was more efficient that Polis, which still required significant human labor and analysis at the time. He helped to award $1 million in grants to teams working on solving the problem of deliberation at scale.",
        "pageid": 70699900
    },
    "Predictor–corrector method": {
        "title": "Predictor–corrector method",
        "extract": "In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equations – to find an unknown function that satisfies a given differential equation.  All such algorithms proceed in two steps: \n\nThe initial, \"prediction\" step, starts from a function fitted to the function-values and derivative-values at a preceding set of points to extrapolate (\"anticipate\") this function's value at a subsequent, new point.\nThe next, \"corrector\" step refines the initial approximation by using the predicted value of the function and another method to interpolate that unknown function's value at the same subsequent point.",
        "pageid": 17876651
    },
    "Proof of authority": {
        "title": "Proof of authority",
        "extract": "Proof of authority (PoA) is an algorithm used with blockchains that delivers comparatively fast transactions through a consensus mechanism based on identity as a stake. The most notable platforms using PoA are VeChain, Bitgert, Palm Network and Xodex.",
        "pageid": 56036557
    },
    "Randomized rounding": {
        "title": "Randomized rounding",
        "extract": "In computer science and operations research, randomized rounding\nis a widely used approach for designing and analyzing approximation algorithms.  \nMany combinatorial optimization problems are computationally intractable to solve exactly (to optimality). For such problems, randomized rounding can be used to design fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.\nThe basic idea of randomized rounding is to convert an optimal solution of a relaxation of the problem into an approximately-optimal solution to the original problem. The resulting algorithm is usually analyzed using the probabilistic method.\n\n",
        "pageid": 26754386
    },
    "Regulation of algorithms": {
        "title": "Regulation of algorithms",
        "extract": "Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning. For the subset of AI algorithms, the term regulation of artificial intelligence is used. The regulatory and  policy landscape for artificial intelligence (AI) is an emerging issue in jurisdictions globally, including in the European Union. Regulation of AI is considered necessary to both encourage AI and manage associated risks, but challenging. Another emerging topic is the regulation of blockchain algorithms (Use of the smart contracts must be regulated) and is mentioned along with regulation of AI algorithms. Many countries have enacted regulations of high frequency trades, which is shifting due to technological progress into the realm of AI algorithms.\nThe motivation for regulation of algorithms is the apprehension of losing control over the algorithms, whose impact on human life increases. Multiple countries have already introduced regulations in case of automated credit score calculation—right to explanation is mandatory for those algorithms.  For example, The IEEE has begun developing a new standard to explicitly address ethical issues and the values of potential future users. Bias, transparency, and ethics concerns have emerged with respect to the use of algorithms in diverse domains ranging from criminal justice to healthcare—many fear that artificial intelligence could replicate existing social inequalities along race, class, gender, and sexuality lines.",
        "pageid": 63442371
    },
    "Rendezvous hashing": {
        "title": "Rendezvous hashing",
        "extract": "Rendezvous or highest random weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n options out of a possible set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to.\nConsistent hashing addresses the special case  \n  \n    \n      \n        k\n        =\n        1\n      \n    \n    {\\displaystyle k=1}\n  \n using a different method. Rendezvous hashing is both much simpler and more general than consistent hashing (see below).\n\n",
        "pageid": 40543215
    },
    "Reservoir sampling": {
        "title": "Reservoir sampling",
        "extract": "Reservoir sampling is a family of randomized algorithms for choosing a simple random sample, without replacement, of k items from a population of unknown size n in a single pass over the items.  The size of the population n is not known to the algorithm and is typically too large for all n items to fit into main memory.  The population is revealed to the algorithm over time, and the algorithm cannot look back at previous items. At any point, the current state of the algorithm must permit extraction of a simple random sample without replacement of size k over the part of the population seen so far.\n\n",
        "pageid": 25190127
    },
    "Right to explanation": {
        "title": "Right to explanation",
        "extract": "In the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to an explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.\"\nSome such legal rights already exist, while the scope of a general \"right to explanation\" is a matter of ongoing debate. There have been arguments made that a \"social right to explanation\" is a crucial foundation for an information society, particularly as the institutions of that society will need to use digital technologies, artificial intelligence, machine learning. In other words, that the related automated decision making systems that use explainability would be more trustworthy and transparent. Without this right, which could be constituted both legally and through professional standards, the public will be left without much recourse to challenge the decisions of automated systems.",
        "pageid": 54625345
    },
    "Run-time algorithm specialization": {
        "title": "Run-time algorithm specialization",
        "extract": "In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.\nThe idea is inspired by the use of partial evaluation in optimising program translation. \nMany core operations in theorem provers exhibit the following pattern.\nSuppose that we need to execute some algorithm \n  \n    \n      \n        \n          \n            a\n            l\n            g\n          \n        \n        (\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle {\\mathit {alg}}(A,B)}\n  \n in a situation where a value of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is fixed for potentially many different values of \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n. In order to do this efficiently, we can try to find a specialization of \n  \n    \n      \n        \n          \n            a\n            l\n            g\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {alg}}}\n  \n for every fixed \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, i.e., such an algorithm \n  \n    \n      \n        \n          \n            \n              a\n              l\n              g\n            \n          \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {alg}}_{A}}\n  \n, that executing \n  \n    \n      \n        \n          \n            \n              a\n              l\n              g\n            \n          \n          \n            A\n          \n        \n        (\n        B\n        )\n      \n    \n    {\\displaystyle {\\mathit {alg}}_{A}(B)}\n  \n is equivalent to executing \n  \n    \n      \n        \n          \n            a\n            l\n            g\n          \n        \n        (\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle {\\mathit {alg}}(A,B)}\n  \n.\nThe specialized algorithm may be more efficient than the generic one, since it can exploit some particular properties of the fixed value \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. Typically, \n  \n    \n      \n        \n          \n            \n              a\n              l\n              g\n            \n          \n          \n            A\n          \n        \n        (\n        B\n        )\n      \n    \n    {\\displaystyle {\\mathit {alg}}_{A}(B)}\n  \n can avoid some operations that \n  \n    \n      \n        \n          \n            a\n            l\n            g\n          \n        \n        (\n        A\n        ,\n        B\n        )\n      \n    \n    {\\displaystyle {\\mathit {alg}}(A,B)}\n  \n would have to perform, if they are known to be redundant for this particular parameter \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. \nIn particular, we can often identify some tests that are true or false for \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, unroll loops and recursion, etc.",
        "pageid": 2935699
    },
    "Run-to-completion scheduling": {
        "title": "Run-to-completion scheduling",
        "extract": "Run-to-completion scheduling or nonpreemptive scheduling is a scheduling model in which each task runs until it either finishes, or explicitly yields control back to the scheduler. Run-to-completion systems typically have an event queue which is serviced either in strict order of admission by an event loop, or by an admission scheduler which is capable of scheduling events out of order, based on other constraints such as deadlines.\nSome preemptive multitasking scheduling systems behave as run-to-completion schedulers in regard to scheduling tasks at one particular process priority level, at the same time as those processes still preempt other lower priority tasks and are themselves preempted by higher priority tasks.",
        "pageid": 37606787
    },
    "Sardinas–Patterson algorithm": {
        "title": "Sardinas–Patterson algorithm",
        "extract": "In coding theory, the Sardinas–Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 1953. The algorithm carries out a systematic search for a string which admits two different decompositions into codewords. As Knuth reports, the algorithm was rediscovered about ten years later in 1963 by Floyd, despite the fact that it was at the time already well known in coding theory.",
        "pageid": 23632960
    },
    "Sequential algorithm": {
        "title": "Sequential algorithm",
        "extract": "In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.",
        "pageid": 23868049
    },
    "Serial algorithm": {
        "title": "Sequential algorithm",
        "extract": "In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.",
        "pageid": 23868049
    },
    "Shapiro–Senapathy algorithm": {
        "title": "Shapiro–Senapathy algorithm",
        "extract": "The Shapiro—Senapathy algorithm (S&S) is an algorithm for predicting splice junctions in genes of animals and plants. This algorithm has been used to discover disease-causing splice site mutations and cryptic splice sites.",
        "pageid": 52300160
    },
    "Shuffling algorithm": {
        "title": "Shuffling",
        "extract": "Shuffling is a technique used to randomize a deck of playing cards, introducing an element of chance into card games. Various shuffling methods exist, each with its own characteristics and potential for manipulation.\nOne of the simplest shuffling techniques is the overhand shuffle, where small packets of cards are transferred from one hand to the other. This method is easy to perform but can be manipulated to control the order of cards. Another common technique is the riffle shuffle, where the deck is split into two halves and interleaved. This method is more complex but minimizes the risk of exposing cards. The Gilbert–Shannon–Reeds model suggests that seven riffle shuffles are sufficient to thoroughly randomize a deck, although some studies indicate that six shuffles may be enough.\nOther shuffling methods include the Hindu shuffle, commonly used in Asia, and the pile shuffle, where cards are dealt into piles and then stacked. The Mongean shuffle involves a specific sequence of transferring cards between hands, resulting in a predictable order. The faro shuffle, a controlled shuffle used by magicians, involves interweaving two halves of the deck and can restore the original order after several shuffles.\nShuffling can be simulated using algorithms like the Fisher–Yates shuffle, which generates a random permutation of cards. In online gambling, the randomness of shuffling is crucial, and many sites provide descriptions of their shuffling algorithms. Shuffling machines are also used in casinos to increase complexity and prevent predictions. Despite these advances, the mathematics of shuffling continue to be a subject of research, with ongoing debates about the number of shuffles required for true randomization.",
        "pageid": 23189
    },
    "Sieve of Eratosthenes": {
        "title": "Sieve of Eratosthenes",
        "extract": "In mathematics, the sieve of Eratosthenes is an ancient algorithm for finding all prime numbers up to any given limit.\nIt does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, 2. The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime. Once all the multiples of each discovered prime have been marked as composites, the remaining unmarked numbers are primes.\nThe earliest known reference to the sieve (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous) is in Nicomachus of Gerasa's Introduction to Arithmetic, an early 2nd century CE book which attributes it to Eratosthenes of Cyrene, a 3rd century BCE Greek mathematician, though describing the sieving by odd numbers instead of by primes.\nOne of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in arithmetic progressions.",
        "pageid": 73415
    },
    "Sieve of Pritchard": {
        "title": "Sieve of Pritchard",
        "extract": "In mathematics, the sieve of Pritchard is an algorithm for finding all prime numbers up to a specified bound.\nLike the ancient sieve of Eratosthenes, it has a simple conceptual basis in number theory.\nIt is especially suited to quick hand computation for small bounds.\nWhereas the sieve of Eratosthenes marks off each non-prime for each of its prime factors, the sieve of Pritchard avoids considering almost all non-prime numbers by building progressively larger wheels, which represent the pattern of numbers not divisible by any of the primes processed thus far.\nIt thereby achieves a better asymptotic complexity, and was the first sieve with a running time sublinear in the specified bound.\nIts asymptotic running-time has not been improved on, and it deletes fewer composites than any other known sieve.\nIt was created in 1979 by Paul Pritchard.\nSince Pritchard has created a number of other sieve algorithms for finding prime numbers, the sieve of Pritchard is sometimes singled out by being called the wheel sieve (by Pritchard himself) or the dynamic wheel sieve.",
        "pageid": 70873538
    },
    "Sikidy": {
        "title": "Sikidy",
        "extract": "Sikidy is a form of algebraic geomancy practiced by Malagasy peoples in Madagascar. It involves algorithmic operations performed on random data generated from tree seeds, which are ritually arranged in a tableau called a toetry and divinely interpreted after being mathematically operated on. Columns of seeds, designated \"slaves\" or \"princes\" belonging to respective \"lands\" for each, interact symbolically to express vintana ('fate') in the interpretation of the diviner. The diviner also prescribes solutions to problems and ways to avoid fated misfortune, often involving a sacrifice. \nThe centuries-old practice derives from Islamic influence brought to the island by medieval Arab traders. The sikidy is consulted for a range of divinatory questions pertaining to fate and the future, including identifying sources of and rectifying misfortune, reading the fate of newborns, and planning annual migrations. The mathematics of sikidy include the concepts of Boolean algebra, symbolic logic and parity.",
        "pageid": 76866914
    },
    "Simulation algorithms for atomic DEVS": {
        "title": "Simulation algorithms for atomic DEVS",
        "extract": "Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan \n  \n    \n      \n        \n          t\n          \n            s\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle t_{s}\\in [0,\\infty ]}\n  \n and elapsed time \n  \n    \n      \n        \n          t\n          \n            e\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle t_{e}\\in [0,\\infty )}\n  \n by introducing two other time variables, last event time, \n  \n    \n      \n        \n          t\n          \n            l\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle t_{l}\\in [0,\\infty )}\n  \n, and next event time \n  \n    \n      \n        \n          t\n          \n            n\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle t_{n}\\in [0,\\infty ]}\n  \n with the following relations:\n\nand\n\nwhere \n  \n    \n      \n        t\n        ∈\n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle t\\in [0,\\infty )}\n  \n denotes the current time. And the remaining time,\n\n  is equivalently computed as\n, apparently \n  \n    \n      \n        \n          t\n          \n            r\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle t_{r}\\in [0,\\infty ]}\n  \n.\nSince the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.",
        "pageid": 22509875
    },
    "Simulation algorithms for coupled DEVS": {
        "title": "Simulation algorithms for coupled DEVS",
        "extract": "Given a coupled DEVS model, simulation algorithms are methods to generate the model's legal behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.) [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan \n  \n    \n      \n        \n          t\n          \n            s\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle t_{s}\\in [0,\\infty ]}\n  \n and elapsed time \n  \n    \n      \n        \n          t\n          \n            e\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle t_{e}\\in [0,\\infty )}\n  \n by introducing two other time variables, last event time, \n  \n    \n      \n        \n          t\n          \n            l\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle t_{l}\\in [0,\\infty )}\n  \n, and next event time \n  \n    \n      \n        \n          t\n          \n            n\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle t_{n}\\in [0,\\infty ]}\n  \n with the following relations: \nand\n\nwhere \n  \n    \n      \n        t\n        ∈\n        [\n        0\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle t\\in [0,\\infty )}\n  \n denotes the current time. And the remaining time,\n\n  is equivalently computed as\n apparently \n  \n    \n      \n        \n          t\n          \n            r\n          \n        \n        ∈\n        [\n        0\n        ,\n        ∞\n        ]\n      \n    \n    {\\displaystyle t_{r}\\in [0,\\infty ]}\n  \n.\nBased on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.",
        "pageid": 22513037
    },
    "Snap rounding": {
        "title": "Snap rounding",
        "extract": "Snap rounding is a method of approximating line segment locations by creating a grid and placing each point in the centre of a cell (pixel) of the grid.  The method preserves certain topological properties of the arrangement of line segments.\nDrawbacks include the potential interpolation of additional vertices in  line segments (lines become polylines), the arbitrary closeness of a point to a non-incident edge, and arbitrary numbers of intersections between input line-segments.   The 3 dimensional case is worse, with a polyhedral subdivision of complexity n becoming complexity O(n4).\nThere are more refined algorithms to cope with some of these issues, for example  iterated snap rounding guarantees a \"large\" separation between points and non-incident edges.",
        "pageid": 61070746
    },
    "Sparse identification of non-linear dynamics": {
        "title": "Sparse identification of non-linear dynamics",
        "extract": "Sparse identification of nonlinear dynamics (SINDy) is a data-driven algorithm for obtaining dynamical systems from data. Given a series of snapshots of a dynamical system and its corresponding time derivatives, SINDy performs a sparsity-promoting regression (such as LASSO and spare Bayesian inference) on a library of nonlinear candidate functions of the snapshots against the derivatives to find the governing equations. This procedure relies on the assumption that most physical systems only have a few dominant terms which dictate the dynamics, given an appropriately selected coordinate system and quality training data. It has been applied to identify the dynamics of fluids, based on proper orthogonal decomposition, as well as other complex dynamical systems, such as biological networks.",
        "pageid": 72100476
    },
    "Spreading activation": {
        "title": "Spreading activation",
        "extract": "Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or \"activation\" and then iteratively propagating or \"spreading\" that activation out to other nodes linked to the source nodes.  Most often these \"weights\" are real values that decay as activation propagates through the network.  When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.\nSpreading activation in semantic networks as a model were invented in cognitive psychology to model the fan out effect.\nSpreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.\n\n",
        "pageid": 7347241
    },
    "Tarjan's algorithm": {
        "title": "Tarjan's algorithm",
        "extract": "Tarjan's algorithm may refer to one of several algorithms attributed to Robert Tarjan, including:\n\nTarjan's strongly connected components algorithm\nTarjan's off-line lowest common ancestors algorithm\nTarjan's algorithm for finding bridges in an undirected graph\nTarjan's algorithm for finding simple circuits in a directed graph",
        "pageid": 8157026
    },
    "Text-to-video model": {
        "title": "Text-to-video model",
        "extract": "A text-to-video model is a machine learning model that uses a natural language description as input to produce a video relevant to the input text. Advancements during the 2020s in the generation of high-quality, text-conditioned videos have largely been driven by the development of video diffusion models.\n\n",
        "pageid": 71986552
    },
    "Time Warp Edit Distance": {
        "title": "Time Warp Edit Distance",
        "extract": "In the data analysis of time series, Time Warp Edit Distance (TWED) is a measure of similarity (or dissimilarity) between pairs of discrete time series, controlling the relative distortion of the time units of the two series using the physical notion of elasticity. In comparison to other distance measures, (e.g. DTW (dynamic time warping) or  LCS (longest common subsequence problem)), TWED is a metric. Its computational time complexity is \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  \n, but can be drastically reduced in some specific situations by using a corridor to reduce the search space. Its memory space complexity can be reduced to \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n. It was first proposed in 2009 by P.-F. Marteau.",
        "pageid": 51574459
    },
    "Timeline of algorithms": {
        "title": "Timeline of algorithms",
        "extract": "The following timeline of algorithms outlines the development of algorithms (mainly \"mathematical recipes\") since their inception.",
        "pageid": 416776
    },
    "Token-based replay": {
        "title": "Token-based replay",
        "extract": "Token-based replay technique is a conformance checking algorithm  that checks how well a process conforms with its model by replaying each trace on the model (in Petri net notation ). Using the four counters produced tokens, consumed tokens, missing tokens, and remaining tokens, it records the situations where a transition is forced to fire and the remaining tokens after the replay ends. Based on the count at each counter, we can compute the fitness value between the trace and the model.",
        "pageid": 69072129
    },
    "Tomasulo's algorithm": {
        "title": "Tomasulo's algorithm",
        "extract": "Tomasulo's algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. It was developed by Robert Tomasulo at IBM in 1967 and was first implemented in the IBM System/360 Model 91’s floating point unit.\nThe major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.\nRobert Tomasulo received the Eckert–Mauchly Award in 1997 for his work on the algorithm.\n\n",
        "pageid": 390562
    },
    "Weak stability boundary": {
        "title": "Weak stability boundary",
        "extract": "Weak stability boundary (WSB), including low-energy transfer, is a concept introduced by Edward Belbruno in 1987. The concept explained how a spacecraft could change orbits using very little fuel.\nWeak stability boundary is defined for the three-body problem. This problem considers the motion of a particle P of negligible mass moving with respect to two larger bodies, P1, P2, modeled as point masses, where these bodies move in circular or elliptical orbits with respect to each other, and P2 is smaller than P1.\nThe force between the three bodies is the classical Newtonian gravitational force. For example, P1 is the Earth, P2 is the Moon and P is a spacecraft; or P1 is the Sun, P2 is Jupiter and P is a comet, etc. This model is called the restricted three-body problem. The weak stability boundary defines a region about P2 where P is temporarily captured. This region is in position-velocity space.  Capture means that the Kepler energy between P and P2 is negative. This is also called weak capture.",
        "pageid": 71652129
    },
    "Whitehead's algorithm": {
        "title": "Whitehead's algorithm",
        "extract": "Whitehead's algorithm is a mathematical algorithm in group theory for solving the automorphic equivalence problem in the finite rank free group Fn. The algorithm is based on a classic 1936 paper of J. H. C. Whitehead. It is still unknown (except for the case n = 2) if Whitehead's algorithm has polynomial time complexity.",
        "pageid": 60327286
    },
    "XOR swap algorithm": {
        "title": "XOR swap algorithm",
        "extract": "In computer programming, the exclusive or swap (sometimes shortened to XOR swap) is an algorithm that uses the exclusive or bitwise operation to swap the values of two variables without using the temporary variable which is normally required.\nThe algorithm is primarily a novelty and a way of demonstrating properties of the exclusive or operation. It is sometimes discussed as a program optimization, but there are almost no cases where swapping via exclusive or provides benefit over the standard, obvious technique.\n\n",
        "pageid": 145555
    },
    "Xulvi-Brunet–Sokolov algorithm": {
        "title": "Xulvi-Brunet–Sokolov algorithm",
        "extract": "Xulvi-Brunet and Sokolov's algorithm generates networks with chosen degree correlations. This method is based on link rewiring, in which the desired degree is governed by parameter ρ. By varying this single parameter it is possible to generate networks from random (when ρ = 0) to perfectly assortative or disassortative (when ρ = 1).  This algorithm allows to keep network's degree distribution unchanged when changing the value of ρ.",
        "pageid": 46900918
    },
    "Zassenhaus algorithm": {
        "title": "Zassenhaus algorithm",
        "extract": "In mathematics, the Zassenhaus algorithm\nis a method to calculate a basis for the intersection and sum of two subspaces of a vector space.\nIt is named after Hans Zassenhaus, but no publication of this algorithm by him is known. It is used in computer algebra systems.\n\n",
        "pageid": 46953393
    },
    "Data structure": {
        "title": "Data structure",
        "extract": "In computer science, a data structure is a data organization and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.\n\n",
        "pageid": 8519
    },
    "Region (model checking)": {
        "title": "Region (model checking)",
        "extract": "In model checking, a field of computer science, a region is a convex polytope in \n  \n    \n      \n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d}}\n  \n for some dimension \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n, and more precisely a zone, satisfying some minimality property. The regions partition \n  \n    \n      \n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d}}\n  \n.\nThe set of zones depends on a set \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n of constraints of the form \n  \n    \n      \n        x\n        ≤\n        c\n      \n    \n    {\\displaystyle x\\leq c}\n  \n, \n  \n    \n      \n        x\n        ≥\n        c\n      \n    \n    {\\displaystyle x\\geq c}\n  \n, \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ≤\n        \n          x\n          \n            2\n          \n        \n        +\n        c\n      \n    \n    {\\displaystyle x_{1}\\leq x_{2}+c}\n  \n and \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ≥\n        \n          x\n          \n            2\n          \n        \n        +\n        c\n      \n    \n    {\\displaystyle x_{1}\\geq x_{2}+c}\n  \n, with \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n and \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  \n some variables, and \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n a constant. The regions are defined such that if two vectors \n  \n    \n      \n        \n          \n            \n              x\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {x}}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                →\n              \n            \n          \n          ′\n        \n      \n    \n    {\\displaystyle {\\vec {x}}'}\n  \n belong to the same region, then they satisfy the same constraints of \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n. Furthermore, when those vectors are considered as a tuple of clocks, both vectors have the same  set of possible futures. Intuitively, it means that any timed propositional temporal logic-formula, or timed automaton or signal automaton using only the constraints of \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n can not distinguish both vectors.\nThe set of region allows to create the region automaton, which is a directed graph in which each node is a region, and each edge \n  \n    \n      \n        r\n        →\n        \n          r\n          ′\n        \n      \n    \n    {\\displaystyle r\\to r'}\n  \n ensure that \n  \n    \n      \n        \n          r\n          ′\n        \n      \n    \n    {\\displaystyle r'}\n  \n is a possible future of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  \n. Taking a product of this region automaton and of a timed automaton \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n which accepts a language \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n creates a finite automaton or a Büchi automaton which accepts untimed \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n. In particular, it allows to reduce the emptiness problem for \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n to the emptiness problem for a finite or Büchi automaton. This technique is used for example by the software UPPAAL.",
        "pageid": 60442605
    },
    "List of data structures": {
        "title": "List of data structures",
        "extract": "This is a list of well-known data structures. For a wider list of terms, see list of terms relating to algorithms and data structures. For a comparison of running times for a subset of this list see comparison of data structures.\n\n",
        "pageid": 177318
    },
    "Active data structure": {
        "title": "Active data structure",
        "extract": "An active data structure is a data structure with an associated thread or process that performs internal operations. More specifically, an active data structure is associated with a computing resource, which contains one or more concurrently executing processes, and data associated with those processes. Communication is modeled using remote procedure calls, as opposed to shared memory or message passing. The active data structure's internals are hidden behind its RPC interface, and may be accessed concurrently. Common examples include databases and file systems. Active data structures can perform maintenance when resources would otherwise be idle, and present multiple views of the data.",
        "pageid": 4436168
    },
    "Block availability map": {
        "title": "Block availability map",
        "extract": "In computer file systems, a block availability map (BAM)   is a data structure used to track disk blocks that are considered free (available for new data). It is used along with a directory to manage files on a disk (originally only a floppy disk, and later also a hard disk).\nIn terms of Commodore DOS (CBM DOS) compatible disk drives, the BAM was a data structure stored in a reserved area of the disk (its size and location varied based on the physical characteristics of the disk).  For each track, the BAM consisted of a bitmap of available blocks and (usually) a count of the available blocks.  The count was held in a single byte, as all formats had 256 or fewer blocks per track. The count byte was simply the sum of all 1-bits in the bitmap of bytes for the current track.\nThe following table illustrates the layout of Commodore 1541 BAM.  The table would be larger for higher-capacity disks (described below).\n\nThe bitmap was contained in 3 bytes for Commodore 1541 format (single-sided) disks because it had 17 to 20 sectors per track (note 3 bytes can hold at least 20 bits).  Similarly, the Commodore 1571 used 3 bytes for the bitmap of each track, but the BAM was twice the size because there were twice as many tracks when formatted as double-sided.  In contrast, the Commodore 1581 disk drive used 5 bytes for the bitmap because the disk format had 40 blocks per track (note 5 bytes can hold 40 bits).\nIn the bitmap of any format, a 1 bit indicated the block was available (free), while a 0 bit indicated the block was not available (used), and the bitmap data was stored low-byte first.  So the first byte held a map for blocks 0 to 7, the second byte held a map for blocks 8 to 15, and so on.  Within a byte, the bitmap was ordered low-bit first.  For example, the first byte would represent block 0 with the least significant bit and block 7 with the most significant bit.\nStorage devices by Creative Micro Designs, intended for use with CBM computers, also used a Block Availability Map which served the same purpose.  However, these devices (FD-2000, FD-4000, and CMD-HD) did not include a count byte, and the bits in each byte were reversed (high-bit first).  Although the bits were reversed (compared to CBM formats), the bytes were still stored in the same order (low-byte first).",
        "pageid": 38426261
    },
    "Comparison of data structures": {
        "title": "Comparison of data structures",
        "extract": "This is a comparison of the performance of notable data structures, as measured by the complexity of their logical operations. For a more comprehensive listing of data structures, see List of data structures.\nThe comparisons in this article are organized by abstract data type. As a single concrete data structure may be used to implement many abstract data types, some data structures may appear in multiple comparisons (for example, a hash map can be used to implement an associative array or a set).",
        "pageid": 73255051
    },
    "Compressed data structure": {
        "title": "Compressed data structure",
        "extract": "The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science.  It refers to a data structure whose operations are roughly as fast as those of a conventional data structure for the problem, but whose size can be substantially smaller.  The size of the compressed data structure is typically highly dependent upon the information entropy of the data being represented.\nImportant examples of compressed data structures include the compressed suffix array and the FM-index, both of which can represent an arbitrary text of characters T for pattern matching.  Given any input pattern P, they support the operation of finding if and where P appears in T.  The search time is proportional to the sum of the length of pattern P, a very slow-growing function of the length of the text T, and the number of reported matches.  The space they occupy is roughly equal to the size of the text T in entropy-compressed form, such as that obtained by Prediction by Partial Matching or gzip.  Moreover, both data structures are self-indexing, in that they can reconstruct the text T in a random access manner, and thus the underlying text T can be discarded.  In other words, they simultaneously provide a compressed and quickly searchable representation of the text T.  They represent a substantial space improvement over the conventional suffix tree and suffix array, which occupy many times more space than the size of T.  They also support searching for arbitrary patterns, as opposed to the inverted index, which can support only word-based searches.  In addition, inverted indexes do not have the self-indexing feature.\nAn important related notion is that of a succinct data structure, which uses space roughly equal to the information-theoretic minimum, which is a worst-case notion of the space needed to represent the data.  In contrast, the size of a compressed data structure depends upon the particular data being represented.  When the data are compressible, as is often the case in practice for natural language text, the compressed data structure can occupy space very close to the information-theoretic minimum, and significantly less space than most compression schemes.\n\n",
        "pageid": 24757213
    },
    "Dynamization": {
        "title": "Dynamization",
        "extract": "In computer science, dynamization is the process of transforming a static data structure into a dynamic one. Although static data structures may provide very good functionality and fast queries, their utility is limited because of their inability to grow/shrink quickly, thus making them inapplicable for the solution of dynamic problems, where the input data changes. Dynamization techniques provide uniform ways of creating dynamic data structures.",
        "pageid": 3641207
    },
    "Implicit data structure": {
        "title": "Implicit data structure",
        "extract": "In computer science, an implicit data structure or space-efficient data structure is a data structure that stores very little information other than the main or required data: a data structure that requires low overhead. They are called \"implicit\" because the position of the elements carries meaning and relationship between elements; this is contrasted with the use of pointers to give an explicit relationship between elements. Definitions of \"low overhead\" vary, but generally means constant overhead; in big O notation, O(1) overhead. A less restrictive definition is a succinct data structure, which allows greater overhead.",
        "pageid": 3669635
    },
    "Monoque": {
        "title": "Monoque",
        "extract": "A monoque is a linear data structure which provides dynamic array semantics. A monoque is similar in structure to a deque but is limited to operations on one end. Hence the name, mono-que. A monoque offers O(1) random access and O(1) push_back/pop_back. Unlike a C++ vector, the push_back/pop_back functions are not amortized and are strictly O(1) in time complexity.  Because the block list is never reallocated or resized, it maintains strictly O(1) non-amortized worst case performance. Unlike C++'s deque, the O(1) performance guarantee includes the time complexity of working with the block list, whereas the C++ standard only guarantees the deque to be O(1) in terms of operations on the underlying value type.\nThe monoque consists of a size variable and a fixed-size block list of blocks with exponentially increasing sizes. Thus, the size of the monoque in bits is roughly proportional to the square of the system pointer size. Though arguably O(lg N) in size, because lg(pointer_size) is constant on any particular machine the block list is O(1) in size and is an upper bound to O(lg(N)), it bounds the space complexity of the structure by a constant.",
        "pageid": 54593071
    },
    "Oblivious data structure": {
        "title": "Oblivious data structure",
        "extract": "In computer science, an oblivious data structure is a data structure that gives no information about the sequence or pattern of the operations that have been applied except for the final result of the operations.\nIn most conditions, even if the data is encrypted, the access pattern can be achieved, and this pattern can leak some important information such as encryption keys. And in the outsourcing of cloud data, this leakage of access pattern is still very serious.  An access pattern is a specification of an access mode for every attribute of a relation schema. For example, the sequences of user read or write the data in the cloud are access patterns.\nWe say a machine is oblivious if the sequence in which it accesses is equivalent for any two inputs with the same running time. So the data access pattern is independent from the input.\nApplications:\n\nCloud data outsourcing: When writing or reading data from a cloud server, oblivious data structures are useful. And modern databases rely on data structures heavily, so oblivious data structures come in handy.\nSecure processor: Tamper-resilient secure processors are used for defense against physical attacks or the malicious intruders access the users’ computer platforms. The existing secure processors designed in academia and industry include AEGIS and Intel SGX. But the memory addresses are still transferred in the clear on the memory bus. So the research finds that this memory buses can give out the information about encryption keys. With the Oblivious data structure comes in practical, the secure processor can obfuscate memory access pattern in a provably secure manner.\nSecure computation: Traditionally people used circuit-model to do the secure computation, but the model is not enough for the security when the amount of data is getting big. RAM-model secure computation was proposed as an alternative to the traditional circuit model, and oblivious data structure is used to prevent information access behavioral being stolen.\n\n",
        "pageid": 48730466
    },
    "Partition refinement": {
        "title": "Partition refinement",
        "extract": "In the design of algorithms, partition refinement is a technique for representing a partition of a set as a data structure that allows the partition to be refined by splitting its sets into a larger number of smaller sets. In that sense it is dual to the union-find data structure, which also maintains a partition into disjoint sets but in which the operations merge pairs of sets. In some applications of partition refinement, such as lexicographic breadth-first search, the data structure maintains as well an ordering on the sets in the partition.\nPartition refinement forms a key component of several efficient algorithms on graphs and finite automata, including DFA minimization, the Coffman–Graham algorithm for parallel scheduling, and lexicographic breadth-first search of graphs.",
        "pageid": 31595644
    },
    "Persistent data structure": {
        "title": "Persistent data structure",
        "extract": "In computing, a persistent data structure or not ephemeral data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. The term was introduced in Driscoll, Sarnak, Sleator, and Tarjan's 1986 article.\nA data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent. Structures that are not persistent are called ephemeral.\nThese types of data structures are particularly common in logical and functional programming, as languages in those paradigms discourage (or fully forbid) the use of mutable data.\n\n",
        "pageid": 662889
    },
    "Postings list": {
        "title": "Postings list",
        "extract": "The postings list is a data structure commonly used in information retrieval (IR) systems to store indexing information about a corpus. It is central to the design and efficiency of search engines and database management systems that need to retrieve information rapidly.\nAt the bare minimum, a postings list is associated with a term from a document and records the places where that term appears. Each term found in documents within a corpus is mapped to a corresponding postings list containing information such as the documents the term appears in and often the positions within those documents.\n\n",
        "pageid": 75521167
    },
    "Predecessor problem": {
        "title": "Predecessor problem",
        "extract": "In computer science, the predecessor problem involves maintaining a set of items to, given an element, efficiently query which element precedes or succeeds that element in an order. Data structures used to solve the problem include balanced binary search trees, van Emde Boas trees, and fusion trees. In the static predecessor problem, the set of elements does not change, but in the dynamic predecessor problem, insertions into and deletions from the set are allowed.\nThe predecessor problem is a simple case of the nearest neighbor problem, and data structures that solve it have applications in problems like integer sorting.\n\n",
        "pageid": 56037139
    },
    "Retroactive data structure": {
        "title": "Retroactive data structure",
        "extract": "In computer science a retroactive data structure is a data structure which supports efficient modifications to a sequence of operations that have been performed on the structure. These modifications can take the form of retroactive insertion, deletion or updating an operation that was performed at some time in the past.",
        "pageid": 35579271
    },
    "Routing table": {
        "title": "Routing table",
        "extract": "In computer networking, a routing table, or routing information base (RIB), is a data table stored in a router or a network host that lists the routes to particular network destinations, and in some cases, metrics (distances) associated with those routes. The routing table contains information about the topology of the network immediately around it.\nThe construction of routing tables is the primary goal of routing protocols. Static routes are entries that are fixed, rather than resulting from routing protocols and network topology discovery procedures.",
        "pageid": 48043
    },
    "Search data structure": {
        "title": "Search data structure",
        "extract": "In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database.\nThe simplest, most general, and least efficient search structure is merely an unordered sequential list of all the items. Locating the desired item in such a list, by the linear search method, inevitably requires a number of operations proportional to the number n of items, in the worst case as well as in the average case.  Useful search data structures allow faster retrieval; however, they are limited to queries of some specific kind.  Moreover, since the cost of building such structures is at least proportional to n, they only pay off if several queries are to be performed on the same database (or on a database that changes little between queries).\nStatic search structures are designed for answering many queries on a fixed database; dynamic structures also allow insertion, deletion, or modification of items between successive queries. In the dynamic case, one must also consider the cost of fixing the search structure to account for the changes in the database.",
        "pageid": 24019691
    },
    "Set intersection oracle": {
        "title": "Set intersection oracle",
        "extract": "A set intersection oracle (SIO) is a data structure which represents a collection of sets and can quickly answer queries about whether the set intersection of two given sets is non-empty.\nThe input to the problem is n finite sets. The sum of the sizes of all sets is N (which also means that there are at most N distinct elements). The SIO should quickly answer any query of the form:\n\n\"Does the set Si intersect the set Sk\"?",
        "pageid": 45378348
    },
    "Term indexing": {
        "title": "Term indexing",
        "extract": "In computer science, a term index is a data structure to facilitate fast lookup of terms and clauses in a logic program, deductive database, or automated theorem prover.\n\n",
        "pageid": 2930391
    },
    "Platform engineering": {
        "title": "Platform engineering",
        "extract": "Platform engineering is a software engineering discipline focused on the development of self-service toolchains, services, and processes to create an internal developer platform (IDP). The shared IDP can be utilized by software development teams, enabling them to innovate.\nPlatform engineering uses components like configuration management, infrastructure orchestration, and role-based access control to improve reliability. The discipline is associated with DevOps and platform as a service practices.\n\n",
        "pageid": 33040530
    },
    "Software engineering": {
        "title": "Software engineering",
        "extract": "Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.\nThe terms programmer and coder overlap software engineer, but they imply only the construction aspect of a typical software engineer workload.\nA software engineer applies a software development process, which involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.\n\n",
        "pageid": 27010
    },
    "Outline of software engineering": {
        "title": "Outline of software engineering",
        "extract": "The following outline is provided as an overview of and topical guide to software engineering:\nSoftware engineering – application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is the application of engineering to software.\nThe ACM Computing Classification system is a poly-hierarchical ontology that organizes the topics of the field and can be used in semantic web applications and as a de facto standard classification system for the field.   The major section \"Software and its Engineering\" provides an outline and ontology for software engineering.",
        "pageid": 357616
    },
    "Index of software engineering articles": {
        "title": "Index of software engineering articles",
        "extract": "This is an alphabetical list of articles pertaining specifically to software engineering.",
        "pageid": 2702229
    },
    "Abstraction (computer science)": {
        "title": "Abstraction (computer science)",
        "extract": "In software engineering and computer science, abstraction is the process of generalizing concrete details, such as attributes, away from the study of objects and systems to focus attention on details of greater importance. Abstraction is a fundamental concept in computer science and software engineering, especially within the object-oriented programming paradigm. Examples of this include:\n\nthe usage of abstract data types to separate usage from working representations of data within programs;\nthe concept of functions or subroutines which represent a specific way of implementing control flow;\nthe process of reorganizing common behavior from groups of non-abstract classes into abstract classes using inheritance and sub-classes, as seen in object-oriented programming languages.",
        "pageid": 60491
    },
    "Agile software development": {
        "title": "Agile software development",
        "extract": "Agile software development is an umbrella term for approaches to developing software that reflect the values and principles agreed upon by The Agile Alliance, a group of 17 software practitioners, in 2001. As documented in their Manifesto for Agile Software Development the practitioners value: \n\nIndividuals and interactions over processes and tools\nWorking software over comprehensive documentation\nCustomer collaboration over contract negotiation\nResponding to change over following a plan\nThe practitioners cite inspiration from new practices at the time including extreme programming, scrum, dynamic systems development method, adaptive software development and being sympathetic to the need for an alternative to documentation driven, heavyweight software development processes.\nMany software development practices emerged from the agile mindset. These agile-based practices, sometimes called Agile (with a capital A) include requirements, discovery and solutions improvement through the collaborative effort of self-organizing and cross-functional teams with their customer(s)/end user(s).\nWhile there is much anecdotal evidence that the agile mindset and agile-based practices improve the software development process, the empirical evidence is limited and less than conclusive.",
        "pageid": 639009
    },
    "Bookmark manager": {
        "title": "Bookmark manager",
        "extract": "A bookmark manager is any software program or feature designed to store, organize, and display web bookmarks. The bookmarks feature included in each major web browser is a rudimentary bookmark manager. More capable bookmark managers are available online as web apps, mobile apps, or browser extensions, and may display bookmarks as text links or graphical tiles (often depicting icons). Social bookmarking websites are bookmark managers.  Start page browser extensions, new tab page browser extensions, and some browser start pages, also have bookmark presentation and organization features, which are typically tile-based. Some more general programs, such as certain note taking apps, have bookmark management functionality built-in.",
        "pageid": 4268565
    },
    "Brownout (software engineering)": {
        "title": "Brownout (software engineering)",
        "extract": "Brownout in software engineering is a technique that involves disabling certain features of an application.",
        "pageid": 53231843
    },
    "Certified software development professional": {
        "title": "Certified software development professional",
        "extract": "Certified Software Development Professional (CSDP) is a vendor-neutral professional certification in software engineering developed by the IEEE Computer Society for experienced software engineering professionals. This certification was offered globally since 2001 through Dec. 2014.\nThe certification program constituted an element of the Computer Society's major efforts in the area of Software engineering professionalism, along with the IEEE-CS and ACM Software Engineering 2004 (SE2004) Undergraduate Curricula Recommendations, and The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide 2004), completed two years later.\nAs a further development of these elements, to facilitate the global portability of the software engineering certification, since 2005 through 2008 the International Standard ISO/IEC 24773:2008 \"Software engineering -- Certification of software engineering professionals -- Comparison framework\"\n\nhas been developed. (Please, see an overview of this ISO/IEC JTC 1 and IEEE standardization effort in the article published by Stephen B. Seidman, CSDP.\n\n) The standard was formulated in such a way, that it allowed to recognize the CSDP certification scheme as basically aligned with it, soon after the standard's release date, 2008-09-01. Several later revisions of the CSDP certification were undertaken with the aim of making the alignment more complete. In 2019, ISO/IEC 24773:2008 has been withdrawn and revised (by ISO/IEC 24773-1:2019 ).\nThe certification was initially offered by the IEEE Computer Society to experienced software engineering and software development practitioners globally in 2001 in the course of the certification examination beta-testing. The CSDP certification program has been officially approved in 2002.\n\nAfter December 2014 this certification program has been discontinued, all issued certificates are recognized as valid forever.\n \n \nA number of new similar certifications were introduced by the IEEE Computer Society, including the Professional Software Engineering Master (PSEM) and Professional Software Engineering Process Master (PSEPM) Certifications (the later soon discontinued).\nTo become a Certified Software Development Professional (CSDP) candidates had to have four years (initially six years) of professional software engineering experience, pass a three-and-half-hour, 180-question examination on various knowledge areas of software engineering, and possess at least a bachelor's degree in Computer Science or Software Engineering. The CSDP examination tested candidates' proficiency in internationally accepted, industry-standard software engineering principles and practices. CSDP credential holders are also obligated to adhere to the IEEE/ACM's Software Engineering Code of Ethics and Professional Practice. \nAs of 2021, the IEEE-CS offer which is a successor to CSDP is the Professional Software Engineering Master (PSEM) certification. The exam is three hours, is proctored remotely, and consists of 160 questions over the 11 SWEBOK knowledge areas: Software Requirements, Software Design, Software Construction, Software Testing, Software Maintenance, Software Configuration Management, Software Engineering Management, Software Engineering Process, Software Engineering Models and Methods, Software Quality, Software Engineering Economics.\n(There is also the Professional Software Developer (PSD) certification, which covers only 4 knowledge areas: software requirements, software design, software construction, and software testing. The similarity of the name of this certification to the CSDP is confusing, it is a reputable credential but NOT an equivalent of CSDP.)\n\n",
        "pageid": 3292102
    },
    "Component-based software engineering": {
        "title": "Component-based software engineering",
        "extract": "Component-based software engineering (CBSE), also called component-based development (CBD), is a style of software engineering that aims to construct a software system from components that are loosely-coupled and reusable. This emphasizes the separation of concerns among components.\nTo find the right level of component granularity, software architects have to continuously iterate their component designs with developers. Architects need to take into account user requirements, responsibilities and architectural characteristics.",
        "pageid": 2816674
    },
    "Configuration management": {
        "title": "Configuration management",
        "extract": "Configuration management (CM) is a management process for establishing and maintaining consistency of a product's performance, functional, and physical attributes with its requirements, design, and operational information throughout its life. The CM process is widely used by military engineering organizations to manage changes throughout the system lifecycle of complex systems, such as weapon systems, military vehicles, and information systems. Outside the military, the CM process is also used with IT service management as defined by ITIL, and with other domain models in the civil engineering and other industrial engineering segments such as roads, bridges, canals, dams, and buildings.",
        "pageid": 40948
    },
    "Data engineering": {
        "title": "Data engineering",
        "extract": "Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science, which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing.\n\n",
        "pageid": 455220
    },
    "Developer relations": {
        "title": "Developer relations",
        "extract": "Developer relations, abbreviated as DevRel, is an umbrella term for practices employed by an organization that builds developer-facing software to connect with the developers that use that software. Developer relations is a form of platform evangelism and the activities involved are sometimes referred to as a developer program or a DevRel program. DevRel programs often include the following:\n\nDeveloper marketing: Outreach and engagement activities to create awareness and encourage developers to use a product.\nDeveloper education: Product documentation and resources such as videos to aid learning a product.\nDeveloper experience: Often referred to as \"zeroth customer\" and \"friction logging\", devrel programs include using the product directly, finding problems, and improving the developer experience.\nDeveloper success: Activities to nurture and retain developers as they build and scale with a product.\nCommunity: Events, forums, and social groups around the product.",
        "pageid": 50468959
    },
    "Empirical software engineering": {
        "title": "Empirical software engineering",
        "extract": "Empirical software engineering (ESE) is a subfield of software engineering (SE) research that uses empirical research methods to study and evaluate an SE phenomenon of interest. The phenomenon may refer to software development tools/technology, practices, processes, policies, or other human and organizational aspects.\nESE has roots in experimental software engineering, but as the field has matured the need and acceptance for both quantitative and qualitative research has grown. Today, common research methods used in ESE for primary and secondary research are the following:\n\nPrimary research (experimentation, case study research, survey research, simulations in particular software Process simulation)\nSecondary research methods (Systematic reviews, Systematic mapping studies, rapid reviews, tertiary review)\n\n",
        "pageid": 13307469
    },
    "Kim Guldstrand Larsen": {
        "title": "Kim Guldstrand Larsen",
        "extract": "Kim Guldstrand Larsen R (born 1957) is a Danish scientist and professor of computer science at Aalborg University, Denmark. His field of research includes modeling, validation and verification, performance analysis, and synthesing of real-time, embedded, and cyber-physical systems utilizing and contributing to concurrency theory and model checking. Within this domain, he has been instrumental in the invention and continuous development of one of the most widely used verification tools, and has received several awards and honors for his work.\n\n",
        "pageid": 74033110
    },
    "History of software engineering": {
        "title": "History of software engineering",
        "extract": "The history of software engineering begins around the 1960s. Writing software has evolved into a profession concerned with how best to maximize the quality of software and of how to create it. Quality can refer to how maintainable software is, to its stability, speed, usability, testability, readability, size, cost, security, and number of flaws or \"bugs\", as well as to less measurable qualities like elegance, conciseness, and customer satisfaction, among many other attributes. How best to create high quality software is a separate and controversial problem covering software design principles, so-called \"best practices\" for writing code, as well as broader management issues such as optimal team size, process, how best to deliver software on time and as quickly as possible, work-place \"culture\", hiring practices, and so forth. All this falls under the broad rubric of software engineering.\n\n",
        "pageid": 758895
    },
    "Integrated development environment": {
        "title": "Integrated development environment",
        "extract": "An integrated development environment (IDE) is a software application that provides comprehensive facilities for software development. An IDE normally consists of at least a source-code editor, build automation tools, and a debugger. Some IDEs, such as IntelliJ IDEA, Eclipse and Lazarus contain the necessary compiler, interpreter or both; others, such as SharpDevelop and NetBeans, do not.\nThe boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development.",
        "pageid": 15305
    },
    "Mining software repositories": {
        "title": "Mining software repositories",
        "extract": "Within software engineering, the mining software repositories (MSR) field  analyzes the rich data available in software repositories, such as version control repositories, mailing list archives, bug tracking systems, issue tracking systems, etc.  to uncover interesting and actionable information about software systems, projects and software engineering.",
        "pageid": 36810432
    },
    "Mixed criticality": {
        "title": "Mixed criticality",
        "extract": "A mixed criticality system is a system containing computer hardware and software that can execute several applications of different criticality, such as safety-critical and non-safety critical, or of different safety integrity level (SIL). Different criticality applications are engineered to different levels of assurance, with high criticality applications being the most costly to design and verify. These kinds of systems are typically embedded in a machine such as an aircraft whose safety must be ensured.",
        "pageid": 38571997
    },
    "Observability (software)": {
        "title": "Observability (software)",
        "extract": "In software engineering, more specifically in distributed computing, observability is the ability to collect data about programs' execution, modules' internal states, and the communication among components. To improve observability, software engineers use a wide range of logging and tracing techniques to gather telemetry information, and tools to analyze and use it. Observability is foundational to site reliability engineering, as it is the first step in triaging a service outage.\nOne of the goals of observability is to minimize the amount of prior knowledge needed to debug an issue.\n\n",
        "pageid": 72023462
    },
    "Process map": {
        "title": "Process map",
        "extract": "Process map is a global-system process model that is used to outline the processes that make up the business system and how they interact with each other. Process map shows the processes as objects, which means it is a static and non-algorithmic view of the processes. It should be differentiated from a detailed process model, which shows a dynamic and algorithmic view of the processes, usually known as a process flow diagram. There are different notation standards that can be used for modelling process maps, but the most notable ones are TOGAF Event Diagram, Eriksson-Penker notation, and ARIS Value Added Chain.",
        "pageid": 20052365
    },
    "Product-family engineering": {
        "title": "Product-family engineering",
        "extract": "Product-family engineering (PFE), also known as product-line engineering, is based on the ideas of \"domain engineering\" created by the Software Engineering Institute, a term coined by James Neighbors in his 1980 dissertation at University of California, Irvine. Software product lines are quite common in our daily lives, but before a product family can be successfully established, an extensive process has to be followed. This process is known as product-family engineering.\nProduct-family engineering can be defined as a method that creates an underlying architecture of an organization's product platform. It provides an architecture that is based on commonality as well as planned variabilities. The various product variants can be derived from the basic product family, which creates the opportunity to reuse and differentiate on products in the family.  Product-family engineering is conceptually similar to the widespread use of vehicle platforms in the automotive industry.\nProduct-family engineering is a relatively new approach to the creation of new products. It focuses on the process of engineering new products in such a way that it is possible to reuse product components and apply variability with decreased costs and time. Product-family engineering is all about reusing components and structures as much as possible.\nSeveral studies have proven that using a product-family engineering approach for product development can have several benefits. Here is a list of some of them:\n\nHigher productivity\nHigher quality\nFaster time-to-market\nLower labor needs\nThe Nokia case mentioned below also illustrates these benefits.",
        "pageid": 4774788
    },
    "Protocol engineering": {
        "title": "Protocol engineering",
        "extract": "Protocol engineering is the application of systematic methods to the development of communication protocols. It uses many of the principles of software engineering, but it is specific to the development of distributed systems.\n\n",
        "pageid": 58454314
    },
    "Research software engineering": {
        "title": "Research software engineering",
        "extract": "Research software engineering is the use of software engineering practices, methods and techniques for research software, i.e. software that was made for and is mainly used within research projects. The term was proposed in a research paper in 2010 in response to an empirical survey on tools used for software development in research projects. It started to be used in United Kingdom in 2012, when it was needed to define the type of software development needed in research. This focuses on reproducibility, reusability, and accuracy of data analysis and applications created for research.",
        "pageid": 56458866
    },
    "SEMAT": {
        "title": "SEMAT",
        "extract": "SEMAT (Software Engineering Method and Theory) is an initiative to reshape software engineering such that software engineering qualifies as a rigorous discipline. The initiative was launched in December 2009  by Ivar Jacobson, Bertrand Meyer, and Richard Soley with a call for action statement and a vision statement. The initiative was envisioned as a multi-year effort for bridging the gap between the developer community and the academic community and for creating a community giving value to the whole software community.\nThe work is now structured in four different but strongly related areas: Practice, Education, Theory, and Community. The Practice area primarily addresses practices. The Education area is concerned with all issues related to training for both the developers and the academics including students. The Theory area is primarily addressing the search for a General Theory in Software Engineering. Finally, the Community area works with setting up legal entities, creating websites and community growth.  It was expected that the Practice area, the Education area and the Theory area would at some point in time integrate in a way of value to all of them: the Practice area would be a \"customer\" of the Theory area, and direct the research to useful results for the developer community.  The Theory area would give a solid and practical platform for the Practice area.  And, the Education area would communicate the results in proper ways.",
        "pageid": 25451462
    },
    "Service-oriented software engineering": {
        "title": "Service-oriented software engineering",
        "extract": "Service-oriented Software Engineering (SOSE), also referred to as service engineering, is a software engineering methodology focused on the development of software systems by composition of reusable services (service-orientation) often provided by other service providers. Since it involves composition, it shares many characteristics of component-based software engineering, the composition of software systems from reusable components, but it adds the ability to dynamically locate necessary services at run-time. These services may be provided by others as web services, but the essential element is the dynamic nature of the connection between the service users and the service providers.",
        "pageid": 22947099
    },
    "Site reliability engineering": {
        "title": "Site reliability engineering",
        "extract": "Site Reliability Engineering (SRE) is a discipline in the field of Software Engineering and IT infrastructure support that monitors and improves the availability and performance of deployed software systems and large software services (which are expected to deliver reliable response times across events such as new software deployments, hardware failures, and cybersecurity attacks). There is typically a focus on automation and an Infrastructure as Code methodology. SRE uses elements of software engineering, IT infrastructure, web development, and operations to assist with reliability. It is similar to DevOps as they both aim to improve the reliability and availability of deployed software systems.",
        "pageid": 49946142
    },
    "Social software engineering": {
        "title": "Social software engineering",
        "extract": "Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.\nSSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.\nSSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.\nThe participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:\n\nCommunity-centered: Software is produced and consumed by and/or for a community rather than focusing on individuals\nCollaboration/collectiveness: Exploiting the collaborative and collective capacity of human beings\nCompanionship/relationship: Making explicit the various associations among people\nHuman/social activities: Software is designed consciously to support human activities and to address social problems\nSocial inclusion: Software should enable social inclusion enforcing links and trust in communities\nThus, SSE can be defined as \"the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments\".\nOne of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity. SSE is not limited to specific activities of software development. Accordingly, tools have been proposed supporting different parts of SSE, for instance, social system design or social requirements engineering. \nConsequently vertical market software, such as software development tools, engineering tools, marketing tools or software that helps users in a decision making process can profit from social components. Such vertical social software differentiates strongly in its user-base from traditional social software such as Yammer.\n\n",
        "pageid": 19886721
    },
    "Software bot": {
        "title": "Software bot",
        "extract": "A software bot is a type of software agent in the service of software project management and software engineering. A software bot has an identity and potentially personified aspects in order to serve their stakeholders. Software bots often compose software services and provide an alternative user interface, which is sometimes, but not necessarily conversational.\nSoftware bots are typically used to execute tasks, suggest actions, engage in dialogue, and promote social and cultural aspects of a software project.\nThe term bot is derived from robot. However, robots act in the physical world and software bots act only in digital spaces. Some software bots are designed and behave as chatbots, but not all chatbots are software bots. Discussions about the past and future of software bots show that software bots have been adopted for many years.\n\n",
        "pageid": 62381917
    },
    "Software component": {
        "title": "Software component",
        "extract": "A software component is a modular unit    of software that encapsulates specific functionality. The desired characteristics of a component are reusability and maintainability.",
        "pageid": 919625
    },
    "Software configuration management": {
        "title": "Software configuration management",
        "extract": "Software configuration management (SCM), a.k.a. \nsoftware change and configuration management (SCCM), is the software engineering practice of tracking and controlling changes to a software system; part of the larger cross-disciplinary field of configuration management (CM).  SCM includes version control and the establishment of baselines.",
        "pageid": 165180
    },
    "Software construction": {
        "title": "Software construction",
        "extract": "Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.\n\n",
        "pageid": 33408418
    },
    "Software design": {
        "title": "Software design",
        "extract": "Software design is the process of conceptualizing how a software system will work before it is implemented or modified.\nSoftware design also refers to the direct result of the design process – the concepts of how the software will work which consists of both design documentation and undocumented concepts.\nSoftware design usually is directed by goals for the resulting system and involves problem-solving and planning – including both \nhigh-level software architecture and low-level component and algorithm design.\nIn terms of the waterfall development process, software design is the activity of following requirements specification and before coding.\n\n",
        "pageid": 223325
    },
    "Software development process": {
        "title": "Software development process",
        "extract": "In software engineering, a software development process or software development life cycle (SDLC) is a process of planning and managing software development. It typically involves dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design and/or product management. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.\nMost modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.\nA life-cycle \"model\" is sometimes considered a more general term for a category of methodologies and a software development \"process\" is a particular instance as adopted by a specific organization. For example, many specific software development processes fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.",
        "pageid": 23407868
    },
    "Software diagnosis": {
        "title": "Software diagnosis",
        "extract": "Software diagnosis (also: software diagnostics) refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behaviour, and evolution. It serves as means to monitor, steer, observe and optimize software development, software maintenance, and software re-engineering in the sense of a business intelligence approach specific to software systems. It is generally based on the automatic extraction, analysis, and visualization of corresponding information sources of the software system. It can also be manually done and not automatic.\n\n",
        "pageid": 42505492
    },
    "Software diversity": {
        "title": "Software diversity",
        "extract": "Software diversity is a research field about the comprehension and engineering of diversity in the context of software.",
        "pageid": 58834822
    },
    "Software durability": {
        "title": "Software durability",
        "extract": "In software engineering, software durability means the solution ability of serviceability of software and to meet user's needs for a relatively long time. Software durability is important for user's satisfaction. For a software security to be durable, it must allow an organization to adjust the software to business needs that are constantly evolving, often in impulsive ways.\nDurability of software depends on four characteristics mainly; i.e. software trustworthiness, Human Trust for Serviceability, software dependability and software usability.",
        "pageid": 49250015
    },
    "Software engine": {
        "title": "Software engine",
        "extract": "A software engine is a core component of a complex software system. The word \"engine\" is a metaphor of a car's engine. Thus a software engine is a complex subsystem.\nThere is no formal guideline for what should be called an engine, but the term has become widespread in the software industry.\n\n",
        "pageid": 7357562
    },
    "Software engineering demographics": {
        "title": "Software engineering demographics",
        "extract": "Software engineers make up a significant portion of the global workforce. As of 2022, there are an estimated 26.9 million professional software engineers worldwide, up from 21 million in 2016.\n\n",
        "pageid": 535034
    },
    "Software engineering professionalism": {
        "title": "Software engineering professionalism",
        "extract": "Software engineering professionalism is a movement to make software engineering a profession, with aspects such as degree and certification programs, professional associations, professional ethics, and government licensing.  The field is a licensed discipline in Texas in the United States (Texas Board of Professional Engineers, since 2013), Engineers Australia(Course Accreditation since 2001, not Licensing), and many provinces in Davao.",
        "pageid": 473948
    },
    "Software requirements": {
        "title": "Software requirements",
        "extract": "Software requirements for a system are the description of what the system should do, the service or services that it provides and the constraints on its operation.  The IEEE Standard Glossary of Software Engineering Terminology defines a requirement as:\n\nA condition or capability needed by a user to solve a problem or achieve an objective\nA condition or capability that must be met or possessed by a system or system component to satisfy a contract, standard, specification, or other formally imposed document\nA documented representation of a condition or capability as in 1 or 2\nThe activities related to working with software requirements can broadly be broken down into elicitation, analysis, specification, and management.\nNote that the wording Software requirements is additionally used in software release notes to explain, which depending on software packages are required for a certain software to be built/installed/used.",
        "pageid": 21168501
    },
    "Static program analysis": {
        "title": "Static program analysis",
        "extract": "In computer science, static program analysis (also known as static analysis or static simulation) is the analysis of computer programs performed without executing them, in contrast with dynamic program analysis, which is performed on programs during their execution in the integrated environment. \nThe term is usually applied to analysis performed by an automated tool, with human analysis typically being called \"program understanding\", program comprehension, or code review. In the last of these, software inspection and software walkthroughs are also used. In most cases the analysis is performed on some version of a program's source code, and, in other cases, on some form of its object code.\n\n",
        "pageid": 28811
    },
    "Structural synthesis of programs": {
        "title": "Structural synthesis of programs",
        "extract": "Structural synthesis of programs (SSP) is a special form of (automatic) program synthesis that is based on propositional calculus. More precisely, it uses intuitionistic logic for describing the structure of a program in such a detail that the program can be automatically composed from pieces like subroutines or even computer commands. It is assumed that these pieces have been implemented correctly, hence no correctness verification of these pieces is needed. SSP is well suited for automatic composition of services for service-oriented architectures and for synthesis of large simulation programs.",
        "pageid": 34226901
    },
    "System appreciation": {
        "title": "System appreciation",
        "extract": "System appreciation is an activity often included in the maintenance phase of software engineering projects.  Key deliverables from this phase include documentation that describes what the system does in terms of its functional features, and how it achieves those features in terms of its architecture and design.  Software architecture recovery is often the first step within System appreciation.\n\n",
        "pageid": 17389142
    },
    "System context diagram": {
        "title": "System context diagram",
        "extract": "A system context diagram in engineering is a diagram that defines the boundary between the system, or part of a system, and its environment, showing the entities that interact with it. This diagram is a high level view of a system. It is similar to a block diagram.",
        "pageid": 243791
    },
    "System requirements specification": {
        "title": "System requirements specification",
        "extract": "A System Requirements Specification (SysRS) (abbreviated SysRS to be distinct from a software requirements specification (SRS)) is a structured collection of information that embodies the requirements of a system.\nA business analyst (BA), sometimes titled system analyst, is responsible for analyzing the business needs of their clients and stakeholders to help identify business problems and propose solutions. Within the systems development life cycle domain, the BA typically performs a liaison function between the business side of an enterprise and the information technology department or external service providers.\n\n",
        "pageid": 2705889
    },
    "Systems development life cycle": {
        "title": "Systems development life cycle",
        "extract": "In systems engineering, information systems and software engineering, the systems development life cycle (SDLC), also referred to as the application development life cycle, is a process for planning, creating, testing, and deploying an information system. The SDLC concept applies to a range of hardware and software configurations, as a system can be composed of hardware only, software only, or a combination of both. There are usually six stages in this cycle: requirement analysis, design, development and testing, implementation, documentation, and evaluation. \n\n",
        "pageid": 573528
    },
    "Tertiary review": {
        "title": "Tertiary review",
        "extract": "In software engineering, a tertiary review is a systematic review of systematic reviews. It is also referred to as a tertiary study in the software engineering literature. However, Umbrella review is the term more commonly used in medicine.\nKitchenham et al.  suggest that methodologically there is no difference between a systematic review and a tertiary review. However, as the software engineering community has started performing tertiary reviews new concerns unique to tertiary reviews have surfaced. These include the challenge of quality assessment of systematic reviews, search validation and the additional risk of double counting.\n\n",
        "pageid": 70572808
    },
    "Test data": {
        "title": "Test data",
        "extract": "Test data are sets of inputs or information used to verify the correctness, performance, and reliability of software systems. Test data encompass various types, such as positive and negative scenarios, edge cases, and realistic user scenarios, and aims to exercise different aspects of the software to uncover bugs and validate its behavior. Test data is also used in regression testing to verify that new code changes or enhancements do not introduce unintended side effects or break existing functionalities.",
        "pageid": 6476885
    },
    "Traceability": {
        "title": "Traceability",
        "extract": "Traceability is the capability to trace something. In some cases, it is interpreted as the ability to verify the history, location, or application of an item by means of documented recorded identification.\nOther common definitions include the capability (and implementation) of keeping track of a given set or type of information to a given degree, or the ability to chronologically interrelate uniquely identifiable entities in a way that is verifiable.\nTraceability is applicable to measurement, supply chain, software development, healthcare and security.\n\n",
        "pageid": 174247
    },
    "Unit of work": {
        "title": "Unit of work",
        "extract": "A unit of work is a behavioral pattern in software development. Martin Fowler has defined it as everything one does during a business transaction which can affect the database. When the unit of work is finished, it will provide everything that needs to be done to change the database as a result of the work.\nA unit of work encapsulates one or more code repositories[de] and a list of actions to be performed which are necessary for the successful implementation of self-contained and consistent data change. A unit of work is also responsible for handling concurrency issues, and can be used for transactions and stability patterns.[de]\n\n",
        "pageid": 36203874
    },
    "View model": {
        "title": "View model",
        "extract": "A view model or viewpoints framework in systems engineering, software engineering, and enterprise engineering is a framework which defines a coherent set of views to be used in the construction of a system architecture, software architecture, or  enterprise architecture. A view is a representation of the whole system from the perspective of a related set of concerns.\nSince the early 1990s there have been a number of efforts to prescribe approaches for describing and analyzing system architectures. A result of these efforts have been to define a set of views (or viewpoints). They are sometimes referred to as architecture frameworks or enterprise architecture frameworks, but are usually called \"view models\".\nUsually a view is a work product that presents specific architecture data for a given system. However, the same term is sometimes used to refer to a view definition, including the particular viewpoint and the corresponding guidance that defines each concrete view. The term view model is related to view definitions.\n\n",
        "pageid": 20061545
    },
    "Computer programming": {
        "title": "Computer programming",
        "extract": "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\nAuxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.",
        "pageid": 5311
    },
    "Outline of computer programming": {
        "title": "Outline of computer programming",
        "extract": "The following outline is provided as an overview of and topical guide to computer programming:\nComputer programming – process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem.",
        "pageid": 6301802
    },
    "List of programming languages": {
        "title": "List of programming languages",
        "extract": "This is an index to notable programming languages, in current or historical use. Dialects of BASIC, esoteric programming languages, and markup languages are not included. A programming language does not need to be imperative or Turing-complete, but must be executable and so does not include markup languages such as HTML or XML, but does include domain-specific languages such as SQL and its dialects.",
        "pageid": 144146
    },
    "Algorave": {
        "title": "Algorave",
        "extract": "An algorave (from an algorithm and rave) is an event where people dance to music generated from algorithms, often using live coding techniques. Alex McLean of Slub and Nick Collins coined the word \"algorave\" in 2011, and the first event under such a name was organised in London, England. It has since become a movement, with algoraves taking place around the world.",
        "pageid": 40428751
    },
    "Asynchronous procedure call": {
        "title": "Asynchronous procedure call",
        "extract": "An asynchronous procedure call (APC) is a unit of work in a computer.",
        "pageid": 53345935
    },
    "Asynchrony (computer programming)": {
        "title": "Asynchrony (computer programming)",
        "extract": "Asynchrony, in computer programming, refers to the occurrence of events independent of the main program flow and ways to deal with such events. These may be \"outside\" events such as the arrival of signals, or actions instigated by a program that take place concurrently with program execution, without the program hanging to wait for results. Asynchronous input/output is an example of the latter case of asynchrony, and lets programs issue commands to storage or network devices that service these requests while the processor continues executing the program. Doing so provides a degree of concurrency.\nA common way for dealing with asynchrony in a programming interface is to provide subroutines that return a future or promise that represents the ongoing operation, and a synchronizing operation that blocks until the future or promise is completed. Some programming languages, such as Cilk, have special syntax for expressing an asynchronous procedure call.\nExamples of asynchrony include the following:\n\nAsynchronous procedure call, a method to run a procedure concurrently, a lightweight alternative to threads.\nAjax is a set of client-side web technologies used by the client to create asynchronous I/O web applications.\nAsynchronous method dispatch (AMD), a data communication method used when there is a need for the server side to handle a large number of long lasting client requests. Using synchronous method dispatch (SMD), this scenario may turn the server into an unavailable busy state resulting in a connection failure response caused by a network connection request timeout. The servicing of a client request is immediately dispatched to an available thread from a pool of threads and the client is put in a blocking state. Upon the completion of the task, the server is notified by a callback. The server unblocks the client and transmits the response back to the client. In case of thread starvation, clients are blocked waiting for threads to become available.\n\n",
        "pageid": 49031794
    },
    "Bayesian program synthesis": {
        "title": "Bayesian program synthesis",
        "extract": "In programming languages and machine learning, Bayesian program synthesis (BPS) is a program synthesis technique where Bayesian probabilistic programs automatically construct new Bayesian probabilistic programs. This approach stands in contrast to routine practice in probabilistic programming where human developers manually write new probabilistic programs.",
        "pageid": 53394437
    },
    "Boolean flag": {
        "title": "Boolean flag",
        "extract": "A Boolean flag, truth bit or truth flag in computer science is a Boolean value represented as one or more bits, which encodes a state variable with two possible values.",
        "pageid": 24999643
    },
    "Characteristic based product configurator": {
        "title": "Characteristic based product configurator",
        "extract": "A characteristic-based product configurator is a product configurator extension which uses a set of discrete variables, called characteristics (or features), to define all possible product variations.\n\n",
        "pageid": 32537475
    },
    "Cheat sheet": {
        "title": "Cheat sheet",
        "extract": "A cheat sheet (also cheatsheet) or crib sheet is a concise set of notes used for quick reference. Cheat sheets were historically used by students without an instructor or teacher's knowledge to cheat on a test or exam. In the context of higher education or vocational training, where rote memorization is not as important, students may be permitted (or even encouraged) to develop and consult their cheat sheets during exams. The act of preparing such reference notes can be an educational exercise in itself, in which case students may be restricted to using only those reference notes they have developed themselves. Some universities publish guidelines for the creation of cheat sheets.\n\n",
        "pageid": 5733800
    },
    "Codecademy": {
        "title": "Codecademy",
        "extract": "Codecademy is an American online interactive platform that offers free coding classes in 13 different programming languages including Python, Java, Go, JavaScript, Ruby, SQL, C++, C#, Lua, and Swift, as well as markup languages HTML and CSS.  The site also offers a paid \"Pro\" option that gives users access to personalized learning plans, quizzes, and realistic projects.\n\n",
        "pageid": 36548336
    },
    "CodeCombat": {
        "title": "CodeCombat",
        "extract": "CodeCombat is an educational video game for learning software programming concepts and languages. This game is recommended for students ages 9–16. Students learn to type coding languages like JavaScript, Python, HTML and CoffeeScript, as well as learning the fundamentals of computer science. CodeCombat has 11 units - three game development units, two web development units, and six computer science units. The first unit, Computer Science 1, is free to all students and teachers. In 2019, CodeCombat was recognized by the College Board as an endorsed provider of curriculum and professional development for AP Computer Science Principles (AP CSP).\nCodeCombat works directly with schools and districts, as well as offering self-paced learners a monthly paid subscription that gives access to additional game content. In order to advance through the game's levels, players must prove their knowledge by writing code. It includes both single-player and multi-player components, and is ideally suited for 4th-12th graders. The game was positively reviewed by PC Magazine, won the 2017 SIIA CODiE award for Best Creativity Tool for Students, and has been named a top pick for learning by Common Sense Education.\nIn January 2014, CodeCombat made their software open-source, and released a level editor so that users could create their own game content. In August 2019, CodeCombat released its newest game, Ozaria.",
        "pageid": 50221460
    },
    "CodeHS": {
        "title": "CodeHS",
        "extract": "CodeHS is an interactive online learning platform offering computer science and programming instruction for schools and individual learners. CodeHS is focused on spreading access to and knowledge of computer science by offering online instructional materials supported by remote tutors. In the introductory learning module, students on the site practice computer science concepts and programming skills by giving commands to a dog named Karel. In the most popular course offered, which is similar to the original Karel programming language developed by Richard E. Pattis, Karel the dog must complete various tasks by moving around a grid world, and putting down and picking up tennis balls using only simple commands. Later learning modules teach more advanced concepts using languages like JavaScript, Java, and HTML.",
        "pageid": 43154124
    },
    "CoderDojo": {
        "title": "CoderDojo",
        "extract": "CoderDojo is a global volunteer-led community of free programming workshops for young people. The movement began in 2011 as a grassroots organisation with each individual clubs (called a dojo [1], after the Japanese name for a hall or place for immersive learning, experiential learning) acting independently, with one founding principle: One Rule, Be Cool. Supporters of CoderDojo believe it is part of the solution to address the global shortage of programmers by getting young people more involved with ICT learning. The movement has seen significant growth since its founding. The CoderDojo Foundation estimates 1,250 Dojos spread across 69 countries, with a growth rate of several new Dojos every week.",
        "pageid": 47211280
    },
    "Codewars": {
        "title": "Codewars",
        "extract": "Codewars is an educational community for computer programming. On the platform, software developers train on programming challenges known as kata. These discrete programming exercises train a range of skills in a variety of programming languages, and are completed within an online integrated development environment. On Codewars the community and challenge progression is gamified, with users earning ranks and honor for completing kata, contributing kata, and quality solutions. \nThe platform is owned and operated by Qualified, a technology company that provides a platform for assessing and training software engineering skills.",
        "pageid": 55532583
    },
    "Coding best practices": {
        "title": "Coding best practices",
        "extract": "Coding best practices or programming best practices are a set of informal, sometimes personal, rules (best practices) that many software developers, in computer programming follow to improve software quality. Many computer programs require being robust and reliable for long periods of time, so any rules need to facilitate both initial development and subsequent maintenance of source code by people other than the original authors.\nIn the ninety–ninety rule, Tom Cargill explains why programming projects often run late: \"The first 90% of the code takes the first 90% of the development time. The last 10% takes another 90% of the time.\" Any guidance which can redress this lack of foresight is worth considering.\nThe size of a project or program has a significant effect on error rates, programmer productivity, and the amount of management needed.\n\n",
        "pageid": 5548053
    },
    "Compile and go system": {
        "title": "Compile and go system",
        "extract": "In computer programming, a compile and go system; compile, load, and go system; assemble and go system; or load and go system\nis a programming language processor in which the compilation, assembly, or link steps are not separated from program execution.  The intermediate forms of the program are generally kept in primary memory, and not saved to the file system.\nExamples of compile-and-go systems are WATFOR, PL/C, and Dartmouth BASIC. An example of load-and-go systems is the loader Anthony J. Barr wrote for the University Computing Corporation in 1968 that was replaced in the market by the IBM OS/360 loader in 1972. These OS/360 loaders performed many of the functions of the Linkage Editor but placed the linked program in memory rather than creating an executable on disk. Compile and go systems differ from interpreters, which either directly execute source code or execute an intermediate representation.",
        "pageid": 35117434
    },
    "Computer network programming": {
        "title": "Computer network programming",
        "extract": "Computer network programming involves writing computer programs that enable processes to communicate with each other across a computer network.\n\n",
        "pageid": 3224769
    },
    "Computer program": {
        "title": "Computer program",
        "extract": "A computer program is a sequence or set of instructions in a programming language for a computer to execute. It is one component of software, which also includes documentation and other intangible components.\nA computer program in its human-readable form is called source code. Source code needs another computer program to execute because computers can only execute their native machine instructions. Therefore, source code may be translated to machine instructions using a compiler written for the language. (Assembly language programs are translated using an assembler.) The resulting file is called an executable. Alternatively, source code may execute within an interpreter written for the language.\nIf the executable is requested for execution, then the operating system loads it into memory and starts a process. The central processing unit will soon switch to this process so it can fetch, decode, and then execute each machine instruction.\nIf the source code is requested for execution, then the operating system loads the corresponding interpreter into memory and starts a process. The interpreter then loads the source code into memory to translate and execute each statement. Running the source code is slower than running an executable. Moreover, the interpreter must be installed on the computer.\n\n",
        "pageid": 5783
    },
    "Conditional operator": {
        "title": "Conditional operator",
        "extract": "The conditional operator is supported in many programming languages. This term usually refers to ?: as in C, C++, C#, and JavaScript. However, in Java, this term can also refer to && and ||.",
        "pageid": 5647807
    },
    "Copy-and-paste programming": {
        "title": "Copy-and-paste programming",
        "extract": "Copy-and-paste programming, sometimes referred to as just pasting,  is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence and ability to create abstractions. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), languages with limited metaprogramming facilities, or certain programming idioms, and it is supported by some source code editors in the form of snippets.\n\n",
        "pageid": 234071
    },
    "Creative coding": {
        "title": "Creative coding",
        "extract": "Creative coding is a type of computer programming in which the goal is to create something expressive instead of something functional. It is used to create live visuals and for VJing, as well as creating visual art and design, entertainment (e.g. video games), art installations, projections and projection mapping, sound art, advertising, product prototypes, and much more.",
        "pageid": 42177488
    },
    "Daily build": {
        "title": "Daily build",
        "extract": "A daily build or nightly build is a software build of the latest version of a software system, run automatically on a daily/nightly basis. \nThis is so it can first be compiled to ensure that all required dependencies are present, and possibly tested to show no bugs have been introduced. The daily build is also often publicly available allowing access to the latest features for feedback.\nIn this context, a build is the result of compiling and linking all the files that make up a program.  The use of such disciplined procedures as daily builds is particularly necessary in large organizations where many programmers are working on a single piece of software. Performing daily builds helps ensure that developers can work knowing with reasonable certainty that any new bugs that show up are a result of their own work done within the last day.\nDaily builds typically include a set of tests, sometimes called a \"smoke test.\"  These tests are included to assist in determining what may have been broken by the changes included in the latest build.  The critical piece of this process is to include new and revised tests as the project progresses.",
        "pageid": 1126769
    },
    "Dangling else": {
        "title": "Dangling else",
        "extract": "The dangling else is a problem in programming of parser generators in which an optional else clause in an if–then(–else) statement can make nested conditional statements ambiguous. Formally, the reference context-free grammar of the language is ambiguous, meaning there is more than one correct parse tree.\nIn many programming languages, one may write conditionally executed code in two forms: the if-then form, or the if-then-else form. (The else clause is optional.):\n\nif a then s\nif b then s1 else s2\n\nAmbiguous interpretation becomes possible when there are nested statements; specifically when an if-then-else form replaces the statement s inside the above if-then construct:\n\nif a then if b then s1 else s2\n\nIn this example, s1 gets executed if and only if a is true and b is true. But what about s2? One person might be sure that s2 gets executed whenever a is false (by attaching the else to the first if), while another person might be sure that s2 gets executed only when a is true and b is false (by attaching the else to the second if). In other words, someone could interpret the previous statement as being equivalent to either of the following unambiguous statements:\n\nif a then { if b then s1 } else s2\nif a then { if b then s1 else s2 }\n\nThe dangling-else problem dates back to ALGOL 60, and subsequent languages have resolved it in various ways. In LR parsers, the dangling else is the archetypal example of a shift-reduce conflict.",
        "pageid": 648096
    },
    "DatalogZ": {
        "title": "DatalogZ",
        "extract": "DatalogZ (stylized as Datalogℤ) is an extension of Datalog with integer arithmetic and comparisons. The decision problem of whether or not a given ground atom (fact) is entailed by a DatalogZ program is RE-complete (hence, undecidable), which can be shown by a reduction to diophantine equations.",
        "pageid": 73214867
    },
    "Derivative code": {
        "title": "Derivative code",
        "extract": "Derivative code or Chameleon code is source code which has been derived entirely from one or more other machine readable file formats.  If recursive transcompiling is used in the development process, some code will survive all the way through the pipeline from beginning to end, and then back to the beginning again.\nThis code is, by definition, derivative code.  The following procedure can be used to easily test if any source code is derivative code or not.  \n\nDelete the code in question\nBuild (or compile) the project\nIf the build process simply replaces the source code which has been deleted, it is (obviously) code which has been derived from something else and is therefore, by definition, derivative code.\nIf the build process fails, and a human needs to re-create the deleted code by hand, this is again, by definition, hand code.\nThe transcompilers and other tools which create derivative code, are usually themselves either in part, or entirely hand code.\n\n",
        "pageid": 51717770
    },
    "Deutsch limit": {
        "title": "Deutsch limit",
        "extract": "The Deutsch limit is an aphorism about the information density of visual programming languages originated by L. Peter Deutsch that states:\n\nThe problem with visual programming is that you can't have more than 50 visual primitives on the screen at the same time.\nThe term was coined by Fred Lakin, after Deutsch made the following comment at a talk on visual programming by Scott Kim and Warren Robinett: \"Well, this is all fine and well, but the problem with visual programming languages is that you can't have more than 50 visual primitives on the screen at the same time. How are you going to write an operating system?\"\nThe primitives in a visual language are the separate graphical elements used to build a program, and having more of them available at the same time enables the programmer to read more information. This limit is sometimes cited as an example of the advantage of textual over visual languages, pointing out the greater information density of text, and posing a difficulty in scaling the language.\nHowever, criticisms of the limit include that it is not clear whether a similar limit also exists in textual programming languages; and that the limit could be overcome by applying modularity to visual programming as is commonly done in textual programming.",
        "pageid": 3919573
    },
    "Directive (programming)": {
        "title": "Directive (programming)",
        "extract": "In computer programming, a directive or pragma (from \"pragmatic\") is a language construct that specifies how a compiler (or other translator) should process its input. Depending on the programming language, directives may or may not be part of the grammar of the language and may vary from compiler to compiler. They can be processed by a preprocessor to specify compiler behavior, or function as a form of in-band parameterization.\nIn some cases directives specify global behavior, while in other cases they only affect a local section, such as a block of programming code. In some cases, such as some C programs, directives are optional compiler hints and may be ignored, but normally they are prescriptive and must be followed. However, a directive does not perform any action in the language itself, but rather only a change in the behavior of the compiler.\nThis term could be used to refer to proprietary third-party tags and commands (or markup) embedded in code that result in additional executable processing that extend the existing compiler, assembler and language constructs present in the development environment. The term \"directive\" is also applied in a variety of ways that are similar to the term command.\n\n",
        "pageid": 930128
    },
    "End-user development": {
        "title": "End-user development",
        "extract": "End-user development (EUD) or end-user programming (EUP) refers to activities and tools that allow end-users – people who are not professional software developers – to program computers. People who are not professional developers can use EUD tools to create or modify software artifacts (descriptions of automated behavior) and complex data objects without significant knowledge of a programming language. In 2005 it was estimated (using statistics from the U.S. Bureau of Labor Statistics) that by 2012 there would be more than 55 million end-user developers in the United States, compared with fewer than 3 million professional programmers. Various EUD approaches exist, and it is an active research topic within the field of computer science and human-computer interaction. Examples include natural language programming, spreadsheets, scripting languages (particularly in an office suite or art application), visual programming, trigger-action programming and programming by example.\nThe most popular EUD tool is the spreadsheet. Due to their unrestricted nature, spreadsheets allow relatively un-sophisticated computer users to write programs that represent complex data models, while shielding them from the need to learn lower-level programming languages.  Because of their common use in business, spreadsheet skills are among the most beneficial skills for a graduate employee to have, and are therefore the most commonly sought after In the United States of America alone, there are an estimated 13 million end-user developers programming with spreadsheets\nThe programming by example (PbE) approach reduces the need for the user to learn the abstractions of a classic programming language. The user instead introduces some examples of the desired results or operations that should be performed on the data, and the PbE system infers some abstractions corresponding to a program that produces this output, which the user can refine. New data may then be introduced to the automatically created program, and the user can correct any mistakes made by the program in order to improve its definition. Low-code development platforms are also an approach to EUD.\nOne evolution in this area has considered the use of mobile devices to support end-user development activities. In this case previous approaches for desktop applications cannot be simply reproposed, given the specific characteristics of mobile devices. Desktop EUD environments lack the advantages of enabling end users to create applications opportunistically while on the move.\nMore recently, interest in how to exploit EUD to support development of Internet of Things applications has increased. In this area trigger-action programming seems a promising approach.\nLessons learned from EUD solutions can significantly influence the software life cycles for commercial software products, in-house intranet/extranet developments and enterprise application deployments.\n\n",
        "pageid": 6795600
    },
    "Energy modeling": {
        "title": "Energy modeling",
        "extract": "Energy modeling or energy system modeling is the process of building computer models of energy systems in order to analyze them.  Such models often employ scenario analysis to investigate different assumptions about the technical and economic conditions at play. Outputs may include the system feasibility, greenhouse gas emissions, cumulative financial costs, natural resource use, and energy efficiency of the system under investigation.  A wide range of techniques are employed, ranging from broadly economic to broadly engineering.  Mathematical optimization is often used to determine the least-cost in some sense.  Models can be international, regional, national, municipal, or stand-alone in scope.  Governments maintain national energy models for energy policy development.\nEnergy models are usually intended to contribute variously to system operations, engineering design, or energy policy development.  This page concentrates on policy models.  Individual building energy simulations are explicitly excluded, although they too are sometimes called energy models.  IPCC-style integrated assessment models, which also contain a representation of the world energy system and are used to examine global transformation pathways through to 2050 or 2100 are not considered here in detail.\nEnergy modeling has increased in importance as the need for climate change mitigation has grown in importance.  The energy supply sector is the largest contributor to global greenhouse gas emissions.  The IPCC reports that climate change mitigation will require a fundamental transformation of the energy supply system, including the substitution of unabated (not captured by CCS) fossil fuel conversion technologies by low-GHG alternatives.",
        "pageid": 50413747
    },
    "Entry point": {
        "title": "Entry point",
        "extract": "In computer programming, an entry point is the place in a program where the execution of a program begins, and where the program has access to command line arguments.\nTo start a program's execution, the loader or operating system passes control to its entry point. (During booting, the operating system itself is the program). This marks the transition from load time (and dynamic link time, if present) to run time.\nFor some operating systems and programming languages, the entry point is in a runtime library, a set of support functions for the language. The library code initializes the program and then passes control to the program proper. In other cases, the program may initialize the runtime library itself. \nIn simple systems, execution begins at the first statement, which is common in interpreted languages, simple executable formats, and boot loaders. In other cases, the entry point is at some other known memory address which can be an absolute address or relative address (offset).\nAlternatively, execution of a program can begin at a named point, either with a conventional name defined by the programming language or operating system or at a caller-specified name. In many C-family languages, this is a function called main; as a result, the entry point is often known as the main function.\nIn JVM languages, such as Java, the entry point is a static method called main; in CLI languages such as C# the entry point is a static method named Main.\n\n",
        "pageid": 1569732
    },
    "EPANET": {
        "title": "EPANET",
        "extract": "EPANET (Environmental Protection Agency Network Evaluation Tool) is a public domain, water distribution system modeling software package developed by the United States Environmental Protection Agency's (EPA) Water Supply and Water Resources Division. It performs extended-period simulation of hydraulic and water-quality behavior within pressurized pipe networks and is designed to be \"a research tool that improves our understanding of the movement and fate of drinking-water constituents within distribution systems\". EPANET first appeared in 1993.\nEPANET 2 is available both as a standalone program and as an open-source toolkit (API in C). Its computational engine is used by many software companies that developed more powerful, proprietary packages, often GIS-centric. The EPANET \".inp\" input file format, which represents network topology, water consumption, and control rules, is supported by many free and commercial modeling packages. Therefore, it is arguably considered to be the industry standard.\n\n",
        "pageid": 27838362
    },
    "Error guessing": {
        "title": "Error guessing",
        "extract": "In software testing, error guessing is a test method in which test cases used to find bugs in programs are established based on experience in prior testing. The scope of test cases usually rely on the software tester involved, who uses experience and intuition to determine what situations commonly cause software failure, or may cause errors to appear. Typical errors include divide by zero, null pointers, or invalid parameters.\nError guessing has no explicit rules for testing; test cases can be designed depending on the situation, either drawing from functional documents or when an unexpected/undocumented error is found while testing operations.",
        "pageid": 13284409
    },
    "Event (computing)": {
        "title": "Event (computing)",
        "extract": "In computing, an event is a detectable occurrence or change in the system's state, such as user input, hardware interrupts, system notifications, or changes in data or conditions, that the system is designed to monitor. Events trigger responses or actions and are fundamental to event-driven systems. These events can be handled synchronously, where the execution thread is blocked until the event handler completes its processing, or asynchronously, where the event is processed independently, often through an event loop. Even when synchronous handling appears to block execution, the underlying mechanism in many systems is still asynchronous, managed by the event loop. \nEvents can be implemented through various mechanisms such as callbacks, message objects, signals, or interrupts, and events themselves are distinct from the implementation mechanisms used. Event propagation models, such as bubbling, capturing, and pub/sub, define how events are distributed and handled within a system. Other key aspects include event loops, event queueing and prioritization, event sourcing, and complex event processing patterns. These mechanisms contribute to the flexibility and scalability of event-driven systems. \n\n",
        "pageid": 7962417
    },
    "Example-centric programming": {
        "title": "Example-centric programming",
        "extract": "Example-centric programming is an approach to software development that helps the user to create software by locating and modifying small examples into a larger whole. That approach can be helped by tools that allow an integrated development environment (IDE) to show code examples or API documentation related to coding behaviors occurring in the IDE. “Borrow” tactics are often employed from online sources, by programmers leaving the IDE to troubleshoot.\nThe purpose of example-centric programming is to reduce the time spent by developers searching online. Ideally, in example-centric programming, the user interface integrates with help module examples for assistance without programmers leaving the IDE. The idea for this type of “instant documentation” is to reduce programming interruptions. The usage of this feature is not limited to experts, as some novices reap the benefits of an integrated knowledge base, without resorting to frequent web searches or browsing.\n\n",
        "pageid": 44382509
    },
    "Existence detection": {
        "title": "Existence detection",
        "extract": "Existence checking or existence detection is an important aspect of many computer programs. An existence check before reading a file can catch and/or prevent a fatal error, for instance. For that reason, most programming language libraries contain a means of checking whether a file exists.\nAn existence check can sometimes involve a \"brute force\" approach of checking all records for a given identifier, as in this Microsoft Excel Visual Basic for Applications code for detecting whether a worksheet exists:",
        "pageid": 26320219
    },
    "Extempore (software)": {
        "title": "Extempore (software)",
        "extract": "Extempore is a live coding environment focused on real-time audiovisual software development. It is designed to accommodate the demands of cyber-physical computing. Extempore consists of two integrated languages, Scheme (with extensions) and Extempore Language. It uses the LLVM cross-language compiler to achieve performant digital signal processing and related low-level features, on-the-fly.\n\n",
        "pageid": 46600256
    },
    "Feature toggle": {
        "title": "Feature toggle",
        "extract": "A feature toggle in software development provides an alternative to maintaining multiple  feature branches in source code. A condition within the code enables or disables a feature during runtime. In agile settings the toggle is used in production, to switch on the feature on demand, for some or all the users. Thus, feature toggles do make it easier to release often. Advanced roll out strategies such as canary roll out and A/B testing are easier to handle. \nContinuous delivery is supported by feature toggles, even if new releases are not deployed to production continuously. The feature is integrated into the main branch even before it is completed. The version is deployed into a test environment once, the toggle allows to turn the feature on, and test it. Software integration cycles get shorter, and a version ready to go to production can be provided.\nThe third use of the technique is to allow developers to release a version of a product that has unfinished features. These unfinished features are hidden (toggled) so that they do not appear in the user interface. There is less effort to merge features into and out of the productive branch, and hence allows many small incremental versions of software.\nA feature toggle is also called feature switch, feature flag, feature gate, feature flipper, or conditional feature.",
        "pageid": 33193592
    },
    "Flowchart": {
        "title": "Flowchart",
        "extract": "A flowchart is a type of diagram that represents a workflow or process. A flowchart can also be defined as a diagrammatic representation of an algorithm, a step-by-step approach to solving a task.\nThe flowchart shows the steps as boxes of various kinds, and their order by connecting the boxes with arrows. This diagrammatic representation illustrates a solution model to a given problem. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields.\n\n",
        "pageid": 527453
    },
    "Floyd's triangle": {
        "title": "Floyd's triangle",
        "extract": "Floyd's triangle is a triangular array of natural numbers used in computer science education. It is named after Robert Floyd. It is defined by filling the rows of the triangle with consecutive numbers, starting with a 1 in the top left corner:\n\nThe problem of writing a computer program to produce this triangle has been frequently used as an exercise or example for beginning computer programmers, covering the concepts of text formatting and simple loop constructs.",
        "pageid": 14705292
    },
    "Fluxus (programming environment)": {
        "title": "Fluxus (programming environment)",
        "extract": "Fluxus is a live coding environment for 3D graphics, music and games. It uses the programming language Racket (a dialect of Scheme/Lisp) to work with a games engine with built-in 3D graphics, physics simulation and sound synthesis. All programming is done on-the-fly, where the code editor appears on top of the graphics that the code is generating.  Fluxus has found use in research and practice in exploratory programming, pedagogy, live performance and games programming.",
        "pageid": 36787028
    },
    "Free variables and bound variables": {
        "title": "Free variables and bound variables",
        "extract": "In mathematics, and in other disciplines involving formal languages, including mathematical logic and computer science, a variable may be said to be either free or bound. Some older books use the terms real variable and apparent variable for free variable and bound variable, respectively. A free variable is a notation (symbol) that specifies places in an expression where substitution may take place and is not a parameter of this or any container expression. The idea is related to a placeholder (a symbol that will later be replaced by some value), or a wildcard character that stands for an unspecified symbol.\nIn computer programming, the term free variable refers to variables used in a function that are neither local variables nor parameters of that function. The term non-local variable is often a synonym in this context.\nAn instance of a variable symbol is bound, in contrast, if the value of that variable symbol has been bound to a specific value or range of values in the domain of discourse or universe. This may be achieved through the use of logical quantifiers, variable-binding operators, or an explicit statement of allowed values for the variable (such as, \"...where \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is a positive integer\".) A variable symbol overall is bound if at least one occurrence of it is bound.pp.142--143 Since the same variable symbol may appear in multiple places in an expression, some occurrences of the variable symbol may be free while others are bound,p.78 hence \"free\" and \"bound\" are at first defined for occurrences and then generalized over all occurrences of said variable symbol in the expression. However it is done, the variable ceases to be an independent variable on which the value of the expression depends, whether that value be a truth value or the numerical result of a calculation, or, more generally, an element of an image set of a function.\nWhile the domain of discourse in many contexts is understood, when an explicit range of values for the bound variable has not been given, it may be necessary to specify the domain in order to properly evaluate the expression. For example, consider the following expression in which both variables are bound by logical quantifiers:\n\n  \n    \n      \n        ∀\n        y\n        \n        ∃\n        x\n        \n        \n          (\n          \n            x\n            =\n            \n              \n                y\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\forall y\\,\\exists x\\,\\left(x={\\sqrt {y}}\\right).}\n  \n\nThis expression evaluates to false if the domain of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is the real numbers, but true if the domain is the complex numbers.\nThe term \"dummy variable\" is also sometimes used for a bound variable (more commonly in general mathematics than in computer science), but this should not be confused with the identically named but unrelated concept of dummy variable as used in statistics, most commonly in regression analysis.p.17\n\n",
        "pageid": 147460
    }
}